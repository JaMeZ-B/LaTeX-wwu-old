\input{../!config/Tazdr/extra_files/Praeambel.tex}

\newcommand{\vorlesung}{Numerische Analysis}
\newcommand{\Prof}{Prof. Dr. Ohlberger}
\newcommand{\subt}{Mitschrift der Tafelnotizen}

\input{../!config/Tazdr/extra_files/headings.tex}



\begin{document}
\maketitle
\thispagestyle{empty}
\cleardoubleoddemptypage

\thispagestyle{empty}
\vspace*{\fill}
\begin{center}
	Hierbei handelt es sich um eine \subt von \textbf{\Prof}, WWU Münster, aus der Vorlesung \textbf{\vorlesung} im Wintersemester 2014/15. 
	Dies ist kein Skript der Vorlesung und keine eigene Arbeit des Autors.\\
	\vspace{2cm}
	Für Fehler in der Mitschrift wird keine Haftung übernommen. 
	Hinweise auf Fehler sind gerne gesehen, hierfür kann man mich in der Uni ansprechen oder alternativ eine e-Mail an: \textit{tobias.wedemeier@gmx.de}\\
	Auch ist eine Mitarbeit über Github möglich.\\
	\vspace{2cm}
	Wenn Teile aus der Vorlesung selber fehlen, können diese gerne an meine e-Mail versandt werden. 
	Ich werde diese dann einarbeiten.\\
\end{center}
\vspace*{\fill}
\cleardoubleoddemptypage

\pagenumbering{Roman}

\tableofcontents
\cleardoubleoddemptypage %sorgt dafür, dass alles folgende erst auf der nächsten freien "rechten" Seite steht

\thispagestyle{empty}

\pagenumbering{arabic}

\setcounter{section}{-1}

\section{Einleitung}
\label{sec:einleitung}

\subsection{Variationsprinzip und Galerkinapproximation}
\label{sub:vartiation_galerkin}
\bet{Beispiel:} Elastizitätstheorie in der Physik:\\
Gesucht: $u:\R^d\to \R,~ d=1,2,3$, Gegeben: Energiefunktional $E:\R\to \R$\\
Aufgabe: Finde $\argmin\ablim{u\in X}E(u)$\\
$u$ entspricht der Auslenkung/Verschiebevektor, $\nabla u$ der Gradient (Jacobimatrix); der Symmetrische Gradient $\frac{1}{2}(\nabla u+ \nabla u^T)=:\epsilon(u)$, dann ist die elastische Gesamtenergie:
\[
E(u):= \frac{1}{2}\int\limits_{\Omega} \Theta:\epsilon(u)\dint x-\int\limits_{\Omega}f(x)u(x)\dint x
\]\marginnote{: ist das Skalarprodukt}
mit symmetrischem \Index{Spannungstensor} $\Theta$ und äußerer Kraft $f:\R^d\to \R^d$.\\
Materialgesetz: Der Spannungstensor ist proportional zum \Index{Verzerrungstensor}:
\begin{equation*}
\begin{aligned}
	\Theta(u) &= A \epsilon(u)\\
	\Theta(u)_{i,j} &= A_{ijkl}\epsilon(u)_{kl}~\forall i,j,k,l=1\dt{,}d
\end{aligned}
\end{equation*}
\[
\Rightarrow E(u)=\frac{1}{2}\int\limits_{\Omega} A\epsilon(u):\epsilon(u)\dint x-\int\limits_{\Omega}f(x)u(x)\dint x
\]
%sub end

\subsection{Definition 1 (Energieminimierung/Variationsprinzip)}
\label{sub:def_1}\index{Variationsprinzip}
\begin{enumerate}[(a)]
	\item Physikalisches Prinzip: Ein physikalisches System strebt immer in einen Zustand minimaler Energie.
	\item Mathematisches Prinzip: Sei $\bar{u}(x,t)$ eine Zustandsvariable und $E(u)$ die Energie eines Systems, das durch $\bar{u}$ repräsentiert wird.
	Dann strebt $\bar{u}$ gegen ein $u=u(x)$, der die Energie minimiert, d.h. falls $E$ genügend glatt ist gilt:
	\[
	\difff{ }{\epsilon}E(u+\epsilon\varphi)|_{\epsilon=0}=0\qquad \forall \text{zulässigen Variationen von }\varphi
	\]
	Elastizität:
	\begin{equation*}
	\begin{aligned}
		\difff{ }{\epsilon}E(u+\epsilon\varphi)|_{\epsilon=0} &= \difff{ }{\epsilon}\cenbrace{\frac{1}{2}\int\limits_{\Omega} A\epsilon(u+\epsilon\varphi):\epsilon(u+\epsilon\varphi)\dint x-\int\limits_{\Omega}f(x)u(x)\dint x}_{\epsilon=0}\\
		&=\int\limits_{\Omega}A\epsilon(u):\epsilon(\varphi)\dint x-\int\limits_{\Omega}f\varphi \bgl{!} 0\\
		&\Rightarrow -\nabla(A\epsilon(u))=f \text{ Dgl.}
	\end{aligned}
	\end{equation*}
	Diese Dgl. gliedert sich auf in 
	\[
	-\sum_{i=1}^{d}\sum_{k,l=1}^{d}\partial_{x_i}A_{ijkl}\epsilon(u)_{kl}=f_i\qquad \forall j=1\dt{,}d
	\]
	Im 1D ergibt sich $-\difff{ }{x}\enbrace{A \difff{ }{x}u}=f$, mit $A\in \R$.
	Für $A=1:~ -u''(x)=f$.\\
	Im 2D ergibt sich:
	\[
	-\nabla(A\nabla u)=f;~A=\id \Rightarrow -\Delta u=f
	\]
\end{enumerate}
%sub end

\subsection{Galerkinverfahren}
\label{sub:galerkin}
\uline{Idee:} Energieminimierung in endlich-dimensionalen Teilräumen.
Sei $X$ Funktionenraum und $E:V\to \R$ ein Energiefunktional.
Gesucht ist $u=\argmin\ablim{v\in X}E(v)$.\\
Sei $X_h\subseteq X$ endlich-dimensionaler Teilraum von $X$.
Wir erhalten die \Index{Galerkin-Approximation}
\[
u_h\in X_h:~ u_h=\argmin\ablim{v_h\in X_h} E(v_h)
\]
\[
\Rightarrow \difff{ }{\epsilon}E(u_h+\epsilon v_h)|_{\epsilon=0}=0~\forall v_h\in X_h
\]
$X_h$ endl.-dim. $\Rightarrow \exists$ Basis $\Phi:=\penbrace{\varphi_i|i=1\dt{,}N:=\dim(X_h)}$, mit der Basisdarstellung $u_h(x)=\sum_{i=1}^{N}u_i\varphi_i(x)$, $u_i\in \R,~i=1\dt{,}N$.
\[
\Rightarrow \difff{ }{\epsilon}E\enbrace{\sum_{i=1}^{N}u_i\varphi_i+\epsilon \varphi_j}_{\epsilon=0}=0~\forall j=1\dt{,}N
\]
Dies ist ein Gleichungssystem mit $N$ Unbekannten und $N$ Gleichungen. 
Allgemein ist das System nicht linear.
%sub end

\subsection{Beispiel Elastizität in 1D}
\label{sub:bsp_elastizitaet}
\[
E(u)= \frac{1}{2}\int\limits_{0}^{1}(u'(x))^2+ fu;~A=1
\]
Betrachte
\begin{equation*}
\begin{aligned}
	\difff{ }{\epsilon}E(u+\epsilon\varphi)|_{\epsilon=0} &= \difff{ }{\epsilon}\frac{1}{2}\int_{0}^{1}(u'+\epsilon\varphi')^2-fu|_{\epsilon=0}\\
	&= \int\limits_{0}^{1}u'\varphi'-f\varphi
\end{aligned}
\end{equation*}
Sei $(u,v):=\int_{0}^{1}uv$ das $L^2$-Skalarprodukt, so folgt
\[
(u',\varphi')=(f,\varphi)~\forall \varphi\in X
\]
Analog folgt für $u_h\in X_h$:
\[
(u_h',\varphi_h')=(f,\varphi_h)~\forall \varphi_h\in X_h
\]
Sei $\varphi_1\dt{,}\varphi_N$ Basis von $X_h$, $u_h=\sum_{i=1}^{N}u_i\varphi_i$.
Dann folgt
\begin{equation*}
\begin{aligned}
	\enbrace{\sum_{i=1}^{N}u_i\varphi_i',\varphi_j'}&=(f,\varphi_j),~j=1\dt{,}N\\
	\Rightarrow \sum_{i=1}^{N}u_i(\varphi_i',\varphi_j')&=(f,\varphi_j),~j=1\dt{,}N\\
	U_i=u_i,~i=1\dt{,}N;~U&\in \R^N,~S_{ij}=(\varphi_i',\varphi_j'),~S\in \R^{N\times N}\\
	F_j=(f,\varphi_i&),~F\in \R^N\\
	\Rightarrow SU=F &\text{ lin. Gleichungssystem}
\end{aligned}
\end{equation*}
%sub end
%sec end
\section{Interpolation}
\label{sec:interpolation}
Sei $\penbrace{\Phi(x,a_0\dt{,}a_n|a_0\dt{,}a_n\in \R)}$ eine Familie von Funktionen mit $x\in \R$.
Ein Element aus dieser Familie ist durch $(n+1)$ Parameter $a_0\dt{,}a_n\in \R$ charakterisiert.\\
\uline{Aufgabe:} Zu $(x_k,f_k)\in \R^2,~k=0\dt{,}n$ mit $x_i\neq x_k$ für $i\neq k$, finde Parameter $a_0\dt{,}a_n\in \R$, so dass
\[
\Phi(x_k,a_0\dt{,}a_n)=f_k,~k=0\dt{,}n.
\]
Dies ist ein Gleichungssystem mit $(n+1)$ Gleichungen und Unbekannten.\\
Familie von linearen prarmeterabh. Funktionen:
Sei $f\in C^0(\R)$ und $V\subseteq C^0(\R)$ sei ein Teilraum mit $\dim(V)=n+1$.
Sei $\varphi_0\dt{,}\varphi_n$ eine Basis von $V$, so setze
\[
\Phi(x,a_0\dt{,}a_n)=\sum_{i=0}^{n}a_i\varphi_i(x)
\]

\subsection{Beispiel: Polynominterpolation}
\label{sub:polynom}
Hier wählt man $V=\Pw_n$ und z.B. $\varphi_i(x)=x^i;~i=0\dt{,}n$.
\[
\Rightarrow \Phi(x,a_0\dt{,}a_n)=\sum_{i=0}^{n}a_ix^i=:p(x)
\]
Aufgabe: Finde $p\in \Pw$ mit $p(x_i)=f_i,~i=0\dt{,}n$.

\subsection{Beispiel: Trigonometrische Interpolation}
\label{sub:trigonometrische}
$\Phi(x,a_0\dt{,}a_n)=a_0+a_1e^{ix}\dt{+}a_ne^{nix}=\sum_{j=0}^{n}a_je^{jix}=a_0=\sum_{k=1}^{n}a_k(\cos(kx)+i\sin(kx))$

\subsection{Beispiel: Nicht lineare Interpolation}
\label{sub:nicht_linear}
Exponentielle Interpolation:
\[
\Phi(x,a_0\dt{,}a_n)=a_0e^{\lambda_0x}\dt{+}a_ne^{\lambda_nx}
\]
mit $\lambda_0\dt{,}\lambda_n\in \R$ fest gewählt oder
\[
\Phi(x,a_0\dt{,}a_n)= \sum_{i=0}^{n}a_ie^{\lambda_ix}\text{ und }(m+1)\cdot 2=n+1
\]

\subsection{Beispiel: Rationale Interpolation}
\label{sub:rational}
\begin{equation*}
\begin{aligned}
\Phi(x,a_0\dt{,}a_n,b_0\dt{,}b_m) &= \frac{a_0\dt{+}a_nx^n}{b_0\dt{+}b_mx^m}
\end{aligned}
\end{equation*}
mit $(m+1)\cdot 2=n+1$.

\subsection{Erweitertes Problem: Hermite-Interpolation}
\label{sub:hermite}\index{Interpolation!Hermite-}
Aufgabe: zu Stützstellen $x_0\dt{,}x_n$ seien die Funktionswerte $f_0\dt{,}f_n$ und Ableitungen $f_0^{(p)}\dt{,}f_n^{(p)},~p=1\dt{,}p_{\max}$ gegeben.
Ist $p_{\max}=1$, so suchen wir ein Interpolationsproblem
\[
p(x)=\sum_{i=0}^N a_ix^i
\]
mit $(n+1)\cdot 2=N+1$ mit $p(x_k)=f_k,~p'(x_k)=f'(x_k),~k=0\dt{,}n$.

\subsection{Beispiel: Spline-Interpolation}
\label{sub:spline}\index{Interpolation!Spline-}
Gesucht: $\Phi\in C^q(\R)$ mit $q$ fest gewählt mit 
\[
\Phi(x_k)=f_k \text{ und } \Phi|_{[x_k,x_{k+1}]}\in \Pw_r.
\] 
Das heißt $(q,r)$ bestimmen die Klasse von \Index{Splines}.

\subsection{Polynominterpolation}
\label{sub:polynominterpolation}
Gegeben: $(x_0,f_0)\dt{,}(x_n,f_n)\in \R^2$ mit $x_i\neq x_k,~i\neq k$.\\
Gesucht: $p\in Pw_N$ mit $p(x_i)=f_i,~i=0\dt{,}n$ und $N$ minimal gewählt.\\
Beispiel: $(x_0,f_0)=(0,0),~(x_1,f_1)=(1,1)$ dann folgt $p\in\Pw_1,~p=x$ ist eindeutiges Interpolationspolynom, aber jedes Monom $x^k$ erfüllt die Interpolationsaufgabe.

\subsection{Satz 1}
\label{sub:satz_1}
Es existiert genau ein $p\in \Pw_n$ mit 
\[
p(x_i)=f_i,~i=0\dt{,}n.
\]

\bet{Beweis:}\\
Sei $\varphi_0\dt{,}\varphi_n$ eine Basis von $\Pw_n$.
Dann ist das Interpolationsproblem äquivalent zu einem linearem Gleichungssystem:
\[
A\cdot a=f \text{ mit }A\in \R^{(n+1)\times(n+1)},~a\in \R^{(n+1)},~f\in \R^{(n+1)}
\]
so dass $p(x)=\sum_{i=0}^{n}a_i\varphi_i(x)$ und $A_{ik}=\varphi_k(x_i)~\forall k,i=0\dt{,}n$, dann folgt
\[
(A\cdot a)_j=\enbrace{\sum_{k=0}^{n}A_{ik}a_k}_j=p(x_j)=f_j
\]
Zeige: $A$ ist regulär.
Sei $a=(a_0\dt{,}a_n)^T$ Lösung der Gleichung $Aa=0$, das heißt
\[
\sum_{k=0}^{n}a_k\varphi_k(x_i)=0~\forall i=0\dt{,}n
\]
Es ist $p(x)=\sum_{k=0}^{n}a_k\varphi_k(x)\in \Pw_n$.
Dann hat $p\in \Pw_n$ mindestens $n+1$ Nullstellen.
Mit dem Fundamentalsatz der Algebra folgt $p\equiv 0$ und somit $a_0\dt{=}a_n=0$.
Also ist $A$ regulär und somit $p\in \Pw_n$ eindeutig bestimmt.
\hfill $\square$

\minisec{Bemerkung}
Interpolation $\Leftrightarrow Aa=f$ mit $A_{ik}=\varphi_k(x_i),~i,k=0\dt{,}n$.\\
1. Ansatz: Monombasis $\varphi_k(x)=x^k \rightsquigarrow p(x)=\sum_{i=0}^{n}a_ix^i$ \Index{Normalform} von $p\in \Pw_N$.
\begin{equation*}
\begin{aligned}
	\Rightarrow A= \begin{pmatrix}
	\varphi_0(x_0) & \dots & \varphi_n(x_0)\\
	\ddots & & \ddots\\
	\varphi_0(x_n) & \dots & \varphi_n(x_n)
	\end{pmatrix} = \begin{pmatrix}
	1 & x_0 & \dots & x_0^n\\
	1 & x_1 & \dots & x_1^n\\
	\ddots & & & \ddots\\
	1 & x_n & \dots & x_n^n
	\end{pmatrix}
\end{aligned}
\end{equation*}
Dies ist die \Index{Vandermondsche Matrix}, insbesondere ist $A$ vol besetzt und sie ist schlecht konditioniert.\\
Idee: Konstruiere eine Basis $\varphi_0\dt{,}\varphi_n$ so, dass gilt
\[
A=\id
\]
Dann wäre $a=f$, d.h. $a_i=f_i~\forall i=0\dt{,}n$ die Lösung des Interpolationsproblems.
\begin{enumerate}[(a)]
	\item Lagrange-Form des Interpolationsproblems:
	\[
	A=\id \Leftrightarrow \varphi_k(x_i)=\delta_{ik}~(0\le k,i\le n)
	\]
	Ansatz: $\varphi_k(x)=c\cdot \prod_{i=0,i\neq k}^{n}(x-x_i) \Rightarrow \varphi_k(x_i)=0~\forall i\neq k$.
	Aus $\varphi_k(x_k)=1$ folgt
	\[
	c=\enbrace{\prod_{i=0, i\neq k}^{n}(x_k-x_i)}^{-1}
	\]
	\[
	\Rightarrow \varphi_k(x)= \prod_{i=0,i\neq k}^{n}\frac{(x-x_i)}{(x_k-x_i)},~k=0\dt{,}n
	\]
\end{enumerate}
\subsection{Defintion 2 (Lagrange-Polynome)}
\label{sub:def_2}
Die Polynome 
\[
l_k^n(x)= \prod_{i=0,i\neq k}^{n}\frac{(x-x_i)}{(x_k-x_i)}
\]
heißen \Index{Lagrange-Polynome} $(l_0^n\dt{,}l_n^n)$ bilden eine Basis von $\Pw_n$ und 
\[
p(x)= \sum_{k=0}^n a_k l_k^n(x)
\]
heißt \Index{Lagrange-Form} von $p\in \Pw_n$.
Es ist 
\[
p(x)= \sum_{k=0}^{n}f_kl_k^n(x)
\]
die Lösung des Interpolationsproblems zu $(x_0,f_0)\dt{,}(x_n,f_n)$.
Für die Lagrange-Polynome gilt 
\[
l_i^n(x_j)=\delta_{ij}.
\]

\minisec{Bemerkung:}
Diese Darstellung ist insbesondere für die Theorie sehr nützlich, edr Nachteil ist, dass die Polynome sich bei Hinzunahme von Stützstellen ändern.

\begin{enumerate}[(b)]
	\item Newton-Form des Interpolationsproblems:\\
	Wähle eine Basis von $\Pw_n$, so dass $A$ eine untere Dreiecksmatrix wird:
	\[
	\varphi_k(x)= \prod_{j=0}^{k-1}(x-x_j) ~k=0\dt{,}n
	\]
	Dann gilt $\varphi_k\in \Pw_n$.
	Dann ist 
	\begin{equation*}
	\begin{aligned}
		\varphi_0(x)&=1 \text{ (verwende die Konvention, dass $\prod_{j=j_0}^{j_n}a_j=1$, falls $j_n<j_0)$}\\
		\varphi_1(x)&=(x-x_0)\\
		\varphi_2(x)&=(x-x_0)(x-x_1)
		&\ddots
	\end{aligned}
	\end{equation*}
	Es gilt $\varphi_k(x_i)=0$ für $i<k \Rightarrow A$ ist eine untere Dreiecksmatrix.
\end{enumerate}

\subsection{Definiton 3 (Newton-Polynome)}
\label{sub:def_3}
Die Polynome
\[
N_k^n:= \prod_{j=0}^{k-1}(x-x_j)
\]
heißen \Index{Newton-Polynome} und 
\[
p(x)=\sum_{k=0}^{n}a_kN_k^n(x)
\]
heißt \Index{Newton-Form} von $p\in \Pw_n$.
Für das Interpolationsproblem gilt:
\begin{equation*}
\begin{aligned}
	a_0&= \frac{f_0}{\varphi_0(x_0)}=f_0\\
	a_1&= \frac{f_1-\varphi_0(x_1a_0)}{\varphi_1(x_1)}=\frac{f_1-f_0}{x_1-x_0}=: f[x_0,x_1]\\
	a_2 &= \dots = \frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0} =: f[x_0,x_1,x_2]
\end{aligned}
\end{equation*}
Die Koeffizienten $a_0\dt{,}a_n$ werden iterativ über die sogenannten \Index{dividierten Differenzen} $f[x_0\dt{,}x_n]$ berechnet ($\rightsquigarrow$ § 3).

\section{Funktionsinterpolation durch Polynome}
\label{sec:funktionsinterpolation}
Gegeben: $x_0\dt{,}x_m$ und $f\in C^0(\R)$\\
Gesucht: Interpolationspolynom zu $(x_0,f(x_0))\dt{,}(x_n,f(x_n))$\\
Frage: Approximationsfehlerabschätzung:
\[
||f-p||_{\infty}\le ??
\]

\subsection{Satz 4 (Fehlerdarstellung)}
\label{sub:satz_4}
Sei $f\in C^{n+1}(a,b)$ und $p\in \Pw_n$ das Interpolationsproblem zu den Stützstellen $x_0\dt{,}x_n$ (paarweise verschieden). 
Dann existiert zu jedem $x\in (a,b)$ ein $\xi_x\in (a,b)$ mit
\[
f(x)-p(x)= \frac{1}{(n+1)!}f^{(n+1)}(\xi_x)\cdot \underbracket{\prod_{k=0}^{n}(x-x_k) }_{\text{Knotenpolynom}}\tag{$\ast$}
\]

\bet{Beweis:}\\
Setze
\[
\omega(t):= \prod_{k=0}^{n} (t-x_k)
\]
und betrachte die Funktion
\[
\phi(t):= f(t)-p(t)-\lambda\omega(t);~ \lambda:= \frac{f(x)-p(x)}{\omega(t)}
\]
daraus folgt, dass $t=x$ eine Nullstelle von $\phi(t)$ und $\phi(x_i)=0*\forall i=0\dt{,}n~x_i$ Stützstelle von $p$ ist.
Dann hat $\phi$ $n+2$ Nullstellen.
Dann folgt mit dem Satz von Rolle: $\phi'$ hat mindestens $n+1$ Nullstelle, also hat $\phi^{(n+1)}$ min. eine Nullstelle.
Diese Nullstelle nennen wir $\xi_x$.\\
Dann gilt:
\[
\phi^{(n+1)}(\xi_x)=0
\]
und 
\[
f^{(n+1)}(\xi_x)-(n+1)! \frac{f(x)-p(x)}{\omega(x)}=0
\]
\[
\Rightarrow f(x)-p(x)=f^{(n+1)}(\xi_x)\frac{1}{(n+1)!}\omega(x)
\]
\hfill $\square$

\subsection{Folgerung 5}
\label{sub:folgerung_5}
Gelten die Voraussetzungen aus Satz 4.
So gilt:
\[
\norm{f-p}_\infty\le \frac{1}{(n+1)!}\norm{f^{(n+1)}}_\infty\cdot\norm{\omega}_\infty
\]
mit $\omega(x)=\prod_{k=0}^{n}(x-x_k)$.\\

\bet{Bemerkung:}\\
Durch optimale Wahl der Stützstellen kann $\norm{\omega}_\infty$ minimiert werden.

\subsection{Beispiel 6 (Range)}
\label{sub:beispeil_6}
Betrachte $f(x)=\frac{1}{1+x^2},~x\in (-5,5)$ und 
\[
x_k^{(n)}:= -5+k\cdot h_n,~k=0\dt{,}n \text{ mit } h_n=\frac{10}{n}
\]
Dann kann man zeigen, dass Konvergenz von $p^{(n)}$ nur auf einem Teilintervall $x\in (-\tilde{x},\tilde{x}),~x\approx 3.6..$ vorliegt und Divergenz außerhalb.\\
\uline{Allgemein gilt:}\\
\begin{enumerate}[1.]
	\item Ist $f\in C^0(a,b)$ so existiert eine Folge von Stützstellen, so dass $p^{(n)}$ gegen $f$ gleichmäßig konvergiert.
	\item Zu jeder Folge von Stützstellen gibt es eine Funktion $f$, so dass $p^{(n)} \nrightarrow f$ gleichmäßig.
\end{enumerate}
\uline{Frage:}
Wie wählt man Stützstellen optimal?\\
Betrachte dazu Interpolation auf dem Referenzintervall $[-1,1]$.\\
\uline{Idee:}
$\omega(x)=\prod_{k=0}^{n}(x-x_k)= x^{n+1}+a_nx^n+... \Rightarrow$ $\omega$ ist ein \bet{normiertes Polynom} und $x_0\dt{,}x_n$ sind die Nullstellen dieses Polynoms.\\
\uline{Minimierungsaufgabe:}
Suchen $T_{n+1}:= \argmin\limits_{p\in N\Pw_{n+1}}\norm{p}_\infty$.

\subsection{Definition 7 (Tschebyschev-Polynome)}
\label{sub:def_7}
Auf $[-1,1]$ sind die \Index{Tschebyschev-Polynome} rekursiv definiert durch
\[
T_0(x)=1,~T_1(x)=x,~ T_{n+1}(x)2x T_n(x)-T_{n-1}(x) \tag{$\ast\ast$}
\]
Die normierten Tschebyschev-Polynome sind dann:
\[
\hat{T}_n(x)= 2^{1-n}T_n(x)
\]

\subsection{Satz 8}
\label{sub:satz_8}
Für $x\in [-1,1]$ gilt
\[
T_n(x)= \cos(n \cos^{-1}(x)) \tag{$\ast$}
\]
Weiterhin gilt
\begin{enumerate}[1.]
	\item $\abs{T_n(x)}\le 1$
	\item $T_n(\cos\enbrace{\frac{j\pi}{n}})=(-1)^j~ (0\le j\le n)$
	\item $T_n(\cos\enbrace{\pi\frac{2j-1}{n}})=0~ (0\le j\le n)$
	\item $T_n\in \Pw_n(-1,1)$
	\item $\hat{T}_n\in N\Pw_n(-1,1)$, das heißt Koeffizient $1$ vor $x^n$.
\end{enumerate}

\bet{Beweis:}\\
Nach Additionstheoremen gilt:
\[
\cos(A+B) =\cos(A)\cos(B)-\sin(A)\sin(B)
\]
Dann folgt
\[
\cos((n+1)\Theta)=\cos(n\Theta)\cos(\Theta)-\sin(n\Theta)\sin(\Theta)
\]
\[
\cos((n-1)\Theta)=\cos(n\Theta)\cos(\Theta)+\sin(n\Theta)\sin(\Theta)
\]
Also 
\[
\cos((n+1)\Theta)+\cos((n-1)\Theta) = 2\cdot \cos(n\Theta)\cos(\Theta)
\]
Wähle $\Theta=\cos^{-1}(x)$:
\[
\Rightarrow \cos((n+1)\cos^{-1}(x))-2\cos(n\cos^{-1}(x))\cdot x - \cos((n-1)\cos^{-1}(x))
\]
Setze $F_n:= \cos(n\cos^{-1}(x))$, dann erfüllt $F_n$ die Iterationsvorschrift $(\ast\ast)$.\\
Noch zu zeigen: $F_0(x)=1,~F_1(x)=x$.\\
Es ist 
\[
F_0(x)=\cos (0\cos^{-1}(x))=\cos(0)=1
\]
und
\[
F_1(x) = \cos(1\cos^{-1}(x))=x
\]
Die Eigenschaften 1.-4. folgen aus den Eigenschaften der $\cos$-Funktion.
5. folgt aus der Iterationsvorschrift induktiv.
\hfill $\square$

\subsection{Lemma 9}
\label{sub:lemma_9}
Sei $p\in \Pw_n$ ein normiertes Polynom auf $[-1,1]$.
Dann gilt $\norm{p}_\infty=\max\limits_{-1\le x\le 1} \abs{p(x)}\ge 2^{1-n}$ und $\norm{\hat{T}_n}_\infty=2^{1-n}$.\\

\bet{Beweis:}\\
\uline{Annahme:} 
Es gibt ein normiertes Polynom $p\in\Pw_n$ mit 
\[
\abs{p(x)}< 2^{1-n}~\forall x\in[-1,1]
\]
Sei $x_i=\cos\enbrace{\pi\frac{i}{n}}$. 
Nach Satz 8 (ii) folgt dann
\[
(-1)^ip(x_i)\le \abs{p(x_i)}<2^{1-n}=(-1)^i\hat{T}_n(x_i)
\]
Daraus folgt
\[
(-1)^i\underbracket{(\hat{T}_n(x_i)-p(x_i)}_{\in \Pw_{n-1}}>0~0\le i\le n.
\]
Daher wechselt $\hat{T}-p$ $(n-1)$-mal das Vorzeichen aus $[-1,1]$.
Da beide Polynome normiert sind ist $\hat{T}-p\in \Pw_{n-1}$ und es folgt ein Widerspruch zur Annahme.
Daraus folgt die Behauptung.
\hfill $\square$

\subsection{Folgerung 10 (Optimale Wahl der Stützstellen)}
\label{sub:folgerung_10}
Mit den Stützstellen $x_k=\cos\enbrace{\pi\frac{2k-1}{2(n+1)}},~k=1\dt{,}n+1$ als die Nullstellen von $T_{n+1}$ gilt, dass das Knotenpolynom $\hat{T}_{n+1}$ ist und es gilt 
\[
\norm{\hat{T}_{n+1}}_\infty=\norm{\omega}_\infty=2^{1-(n+1)}
\]

\section{Dividierte Differenzen}
\label{sec:dividierte_differenzen}
Newton-Form des Interpolationsproblems:
\begin{equation*}
\begin{aligned}
	p(x) &= \sum_{k=0}^{n}a_kN_k(x)\\
	N_k(x) &= \prod_{j=0}^{k-1}(x-x_j),~k=0\dt{,}n
\end{aligned}
\end{equation*}
\uline{Gesucht:}
Algorithmus zur Berechnung der $a_0\dt{,}a_n$.\\
\uline{Bemerkung:}
Setze $p_m(x)=\sum_{k=0}^{m}a_kN_k(x)$ für $m\le n$.
Dann gilt $p_m(x_j)=f_j~(0\le j\le n)$ und $p_m\in \Pw_m$.\\
Das heißt $p_m$ ist Interpolationspolynom zu den Daten $(x,f_0)\dt{,}(x_m,f_m)$, dann hängt insbesondere $a_k$ nur von $(x_0,f_0)\dt{,}(x_k,f_k)$ ab.\\
Schreibweise:
Für $a_k$ schreibe wir $f[x_0\dt{,}x_k]$.

\subsection{Definition 11 (Dividierte Differenzen)}
\label{sub:def_11}
Seien $i_0\dt{,}i_k\in \{0\dt{,}n\}$ paarweise verschieden und sie $P_{i_0\dt{,}i_k}$ das Interpolationspolynom zu $(x_{i_0},f_{i_0})\dt{,}(x_{i_k},f_{i_k})$.
Mit $f[x_{i_0}\dt{,}x_{i_k}]$ bezeichnen wir den Koeffizienten vor $x^k$ im Polynom $P_{i_0\dt{,}i_k}$.\\
$f[x_{i_0}\dt{,}x_{i_k}]$ heißen \Index{dividierte Differenzen} der Ordnung $k$.

\subsection{Satz 12}
\label{sub:satz_12}
\begin{enumerate}[(i)]
	\item Die Polynome $P_{i_0\dt{,}i_k}$ genügen der Rekursionsformel
	\[
	P_{i_0\dt{,}i_k} = \frac{(x-x_{i_0})P_{i_1\dt{,}i_k}-(x-x_{i_k})P_{i_0\dt{,}i_{k-1}}}{x_{i_k}-x_{i_0}} \tag{1}
	\]
	\item Die dividierte Differenzen genügen der Rekursionsformel
	\[
	f[x_{i_0}\dt{,}x_{i_k}] = \frac{f[x_{i_1}\dt{,}x_{i_k}]-f[x_{i_0}\dt{,}x_{i_{k-1}}]}{x_{i_k}-x_{i_0}};~ f[x_{i_k}]=f_{i_k} \tag{2}
	\]
	\item Die dividierten Differenzen sind unabhängig von der Reihenfolge ihrer Argumente, das heißt ist $x_{j_0}\dt{,}x_{j_k}$ eine Permutation von $x_{i_0}\dt{,}x_{i_k}$, so gilt
	\[
	f[x_{j_0}\dt{,}x_{j_k}]= f[x_{i_0}\dt{,}x_{i_k}].
	\]
\end{enumerate}

\uline{Bemerkung:}
Die div. Differenzen kann man in einem Tableau schreiben:




























\cleardoubleoddemptypage
\pagenumbering{Alph}
\setcounter{page}{1}


\printindex
\listoffigures
\end{document}