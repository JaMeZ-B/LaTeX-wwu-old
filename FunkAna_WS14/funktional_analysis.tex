\input{../!config/mitschrift.tex}
\newcommand{\fach}{Einführung in die Funktionalanalysis}
\newcommand{\semester}{WiSe 2014}
\newcommand{\homepage}{https://wwwmath.uni-muenster.de/u/wilhelm.winter/wwinter/funktionalanalysis.html}
\newcommand{\prof}{Prof.\,Dr.\,Wilhelm Winter}
\input{../!config/mitschrift_headings.tex}


\begin{document}
\pagenumbering{Roman}
\maketitle
\begin{abstract}
\input{../!config/vorwort.tex}
\section*{Anmerkung}
Innerhalb dieser Mitschrift wird man öfter den Ausdruck \enquote{Warum?} finden. Dies sind vom Dozenten bewusst weggelassene Details, die zu verstärktem Mitdenken beim Lesen 
animieren sollen. Oftmals sind dies schon aus vorherigen Semestern bekannte Sachverhalte. Nur an wenigen Stellen habe ich die fehlenden Details hinzugefügt.
\end{abstract}

\setcounter{page}{1}
\tableofcontents
\cleardoubleoddemptypage
\pagenumbering{arabic}
\setcounter{page}{1}

\section{Metrische Räume und der Satz von Baire} % (fold)
\label{sec:1}

\subsection[Definition: Metrischer Raum]{Definition} % (fold)
\label{sub:11}
Ein \Index{metrischer Raum} ist ein Paar $(X,d)$, wobei $X$ eine Menge und $d \colon X \times X \to [0,\infty)$ ist, sodass
\begin{enumerate}[1)]
	\item $d(x,y)=0 \iff x=y$ $\forall x,y \in X$
	\item $d(x,y) = d(y,x)$ $\forall x,y \in X$
	\item $d(x,z) \le d(x,y) + d(y,z)$ $\forall x,y,z \in X$
\end{enumerate}
% subsection 11 (end)

\subsection[Definition: Offen, abgeschlossen und Abschluss]{Definition} % (fold)
\label{sub:12}
Sei $(X,d)$ ein metrischer Raum
\begin{itemize}
	\item Eine Teilmenge $U \subseteq X$ heißt \Index{offen}, falls für jedes $x \in U$ ein $\varepsilon > 0$ existiert, so dass 
	\[
		B(x,\varepsilon) := \set[y \in X]{d(x,y) < \varepsilon} \subset U 
	\]
	\item Eine Teilmenge $A \subset X$ heißt \Index{abgeschlossen}, falls $X \setminus A$ offen ist (als Teilmenge von $X$). 
	\item $\mathcal{T}_X := \set[U \subset X]{U \text{ offen im obigen Sinne}} $ ist die von der Metrik induzierte Topologie auf $X$.
	\item Falls $W \subset X$ eine Teilmenge ist, dann bezeichnet $\overline{W}$ den \Index{Abschluss} von $W$, d.h. die kleinste abgeschlossene Teilmenge von $X$, die 
	$W$ enthält. Es gilt
	\[
		\overline{W} = \enspace\bigcap_{\mathclap{A \subset X \text{abg.}, W \subset A}} \enspace A 
	\]
	Für metrische Räume ist dies äquivalent zu
	\[
		\overline{W} = \set[x \in X]{\exists (x_n)_n \subset W : \lim_{n \to \infty} x_n = x} \tag*{(warum?)}
	\]
\end{itemize}
% subsection 12 (end)

\subsection[Definition: Stetigkeit, gleichmäßige Stetigkeit, Isometrie]{Definition} % (fold)
\label{sub:13}
Eine Abbildung $f \colon X \to Y$ zwischen zwei metrischen Räumen $(X,d_X)$ und $(Y,d_Y)$ heißt 
\begin{itemize}
	\item \bet{stetig in} $x \in X$, falls
	\(
		\forall \varepsilon >0 : \exists \delta >0 : \forall x' \in X : d_X(x,x') < \delta  \Longrightarrow d_Y \enbrace[\big]{f(x), f(x')} < \varepsilon 
	\)
	\item \Index{stetig}, falls $f$ in jedem Punkt $x \in X$ stetig ist.
	
	(Äquivalent: Für jede offene Menge $V$ in $Y$ ist $f ^{-1}(V)$ offen in $X$) 
	\item \Index{gleichmäßig stetig}, falls
	\(
		\forall \varepsilon>0 : \exists \delta >0 : \forall x,x' \in X : d_X(x,x') < \delta \Longrightarrow d_Y \enbrace[\big]{f(x), f(x')} < \varepsilon .
	\) 
	\item $f$ heißt \Index{Isometrie}, falls \marginnote{automatisch injektiv und stetig}
	\(
		\forall x,x' \in X : d_X(x,x') = d_Y \enbrace[\big]{f(x), f(x')}.
	\)
	\item $f$ heißt \Index{isometrischer Isomorphismus}, falls $f$ bijektiv und isometrisch ist. \marginnote{$f ^{-1}$ auch}
\end{itemize}
% subsection 13 (end)

\subsection[Definition: Cauchy-Folge und Vollständigkeit]{Definition} % (fold)
\label{sub:14}
Eine Folge $(x_n)_{n \in \mathds{N}}$ in einem metrischen Raum $(X,d)$ heißt \bet{Cauchy-Folge}\index{Cauchy-Folge}, falls
\[
	\forall \varepsilon>0 : \exists N \in \mathds{N} : \forall n,k >N : d(x_n,x_k) < \varepsilon.
\]
$(X,d)$ heißt \Index{vollständig}, falls jede Cauchy-Folge in $X$ konvergiert. 
% subsection 14 (end)

\subsection[Satz: Existenz einer eindeutigen Vervollständigung metrischer Räume]{Satz} % (fold)
\label{sub:15}
Sei $(X,d)$ ein metrischer Raum. Dann existiert ein vollständiger metrischer Raum $(\tilde{X}, \tilde{d})$ und eine Isometrie $\iota \colon X \hookrightarrow \tilde{X}$, 
sodass $\overline{\iota(X)} = \tilde{X}$ (d.h. $\iota(X)$ ist dicht in $\tilde{X}$). $(\tilde{X}, \tilde{d})$ heißt \Index{Vervollständigung} von $(X,d)$ und ist eindeutig 
bis auf isometrische Isomorphie.
\minisec{Beweis}
\begin{description}
	\item[Eindeutigkeit:] 
	Angenommen, $(\hat{X}, \hat{d})$ ist ein weiterer vollständiger metrischer Raum und $\kappa \colon X \to \hat{X}$ eine Isometrie mit 
	$\overline{\kappa(X)}=\hat{X}$. Definiere $\gamma\colon\hat{X} \to \tilde{X}$ wie folgt: Falls $y \in \hat{X}$, wähle eine Folge $(x_n)_{n \in \mathds{N}}$ in $X$, sodass
	$y= \lim_{n \to \infty} \kappa(x_n)$. Setze nun
	\[
		\gamma(y) := \lim_{ n \to \infty} \iota(x_n) \in \tilde{X} 
	\]
	Der Grenzwert existiert, da $\tilde{X}$ vollständig ist
	Zu zeigen: $\gamma$ ist ein wohldefinierter isometrischer Isomorphismus.
	\begin{description}
		\item[Wohldefiniertheit:] $(\kappa(x_n))_n$ ist konvergent, also auch Cauchy. Da $\kappa$ eine Isometrie ist, muss $(x_n)_n$ auch eine Cauchyfolge in $X$ sein.
		Nun ist aber auch $\iota$ eine Isometrie und somit $(\iota(x_n))_n$ eine Cauchyfolge in $\tilde{X}$. Da $\tilde{X}$ vollständig ist, existiert also obiger Grenzwert
		
		Wir müssen außerdem zeigen, dass die Definition nicht von der Wahl der Folge abhängt: Angenommen $(x_n')_n$ ist eine weitere Folge, sodass $\kappa(x_n') \to y$.
		Sei $z \in \tilde{X}$ der Grenzwert dieser Folge unter $\iota$. Dann folgt $\gamma(y)=z$ aus
		\begin{align*}
			\tilde{d}\enbrace[\big]{\gamma(y),z} = \lim_{n \to \infty} \tilde{d}\enbrace[\big]{\iota(x_n), \iota(x_n')} = \lim_{n \to \infty} d \enbrace[\big]{x_n,x_n'}
			= \lim_{n \to \infty} \hat{d}\enbrace[\big]{\kappa(x_n), \kappa(x_n')} = \hat{d}(y,y) =0
		\end{align*}
		\item[Isometrie:] Analog zu obiger Rechnung folgt für $z,z' \in \hat{X}$ und Folgen $(x_n)_n$, $(x_n')_n$ mit $\kappa(x_n) \to z$ und $\kappa(x_n')\to z'$
		\begin{align*}
			\tilde{d}\enbrace[\big]{\gamma(z),\gamma(z')} = \lim_{n \to \infty} \tilde{d}\enbrace[\big]{\iota(x_n), \iota(x_n')} = \lim_{n \to \infty} d \enbrace[\big]{x_n,x_n'}
			= \lim_{n \to \infty} \hat{d}\enbrace[\big]{\kappa(x_n), \kappa(x_n')} = \hat{d}(z,z')
		\end{align*}
		\item[Bijektivität:] Eine Umkehrabbildung lässt sich leicht konstruieren, indem man in obiger Konstruktion die Rollen von $\hat{X}$ und $\tilde{X}$ vertauscht.
	\end{description}
	\item[Existenz:] \begin{description}
		\item[Konstruktion von $(\tilde{X}, \tilde{d})$:] Setze $Y := \set[(x_n)_{n \in \mathds{N}}]{(x_n)_n \text{ ist Cauchy-Folge in }X}$. Definiere 
	\[
		(x_n)_n \sim (x_n') :\iff \lim_{ n \to \infty} d(x_n, x_n') = 0
	\]
	$\sim$ ist eine Äquivalenzrelation auf $Y$. Definiere nun $\tilde{X} := \nicefrac{Y}{\sim}$ und $\tilde{d} \colon \tilde{X} \times \tilde{X} \to [0, \infty)$ durch
	\[
		\tilde{d} \enbrace*{\benbrace[\big]{(x_n)_n} , \benbrace[\big]{(x_n')_n}  } := \lim_{ n \to \infty} d(x_n , x_n') 
	\]
	$\tilde{d}$ ist eine wohldefinierte Abbildung, d.h. falls $\benbrace*{(x_n)_n} = \benbrace*{(y_n)_n} $ und $\benbrace*{(x_n')_n} = \benbrace*{(y_n')_n}$, dann erhalten wir
	\[
		\lim_{n \to \infty} d(x_n , x_n') = \lim_{n \to \infty} d(y_n, y_n'). \tag*{(leichte Übung)}
	\]
	Weiter ist $\enbrace[\big]{d(x_n, x_n')}_n$ eine Cauchy-Folge in dem vollständigen Raum $[0,\infty)$ und somit 
	konvergent: Sei nun $\varepsilon>0$. Dann existiert ein $N \in \mathds{N}$, sodass $d(x_n,x_m) \le \frac{\varepsilon}{2}$ und $d(x_n', x_m') \le \frac{\varepsilon}{2}$ für alle $n,m \ge N$. Dann gilt nach der \hyperref[sub:vier_ungl]{Vierecksungleichung (siehe Anhang \ref*{sub:vier_ungl})}
	\[
		\abs[\big]{d(x_n,x_n') - d(x_m,x_m')} \le d(x_n, x_m) + d(x_n', x_m') \le \varepsilon
	\]
	Also existiert der Grenzwert und $\tilde{d}$ ist wohldefiniert.
	\item[Einbettung von $X$:] Definiere nun $\iota \colon X \to \tilde{X}$ durch $x \mapsto [(x,x,x, \ldots )] \in \tilde{X}$. $\iota$ ist Isometrie, da
	\[
		\tilde{d} \enbrace[\big]{\iota(x),\iota(y)} = \lim_{ n \to \infty} d(x,y) = d(x,y)
	\]
	% Falls $\benbrace*{(x_n)_n} \in \tilde{X}$, finde $y_k \in \iota(X)$, so dass $\benbrace*{(x_n)} = \lim_{ k} y_k$ und $y_k = \benbrace*{(x_k, x_k, \ldots )}= \iota(x_k)$.
% 	Dann gilt $y_k \xrightarrow{k \to \infty} \benbrace*{(x_n)_n} $, d.h.
% 	\[
% 		\lim_{ n \to \infty} d(x_k,x_n) = \tilde{d} \enbrace*{y_k, \benbrace*{(x_n)_n}} \xrightarrow{k \to \infty} 0
% 	\]
% 	$\leadsto \tilde{X}= \overline{\iota(X)}$.
	Sei nun $\benbrace*{(x_n)_n} \in \tilde{X}$ und $\varepsilon>0$. Da $(x_n)_n$ eine Cauchy-Folge ist, gibt es ein $N \in \mathds{N}$, sodass für alle
	$n,m \ge N$ gilt $d(x_n,x_m) <\varepsilon$. Dann gilt
	\[
		\tilde{d}\enbrace[\big]{\iota(x_N), \benbrace*{(x_n)_n} } = \lim_{ n \to \infty} d \enbrace*{x_N, x_n} < \varepsilon
	\]
	Damit hat $\iota$ ein dichtes Bild.
	\item[Vollständigkeit von $(\tilde{X}, \tilde{d})$:] Sei $\enbrace*{\overline{x}^m}_m$ eine Cauchyfolge in $\tilde{X}$.\marginnote{$\overline{x} \in \tilde{X}$}
	\[
		\Longrightarrow \forall \varepsilon >0 : \exists M(\varepsilon) \in \mathds{N} : \forall m,m' > M : \tilde{d} \enbrace*{\overline{x}^m, \overline{x}^{m'}} < \frac{\varepsilon}{3} 
	\]
	Wenn $\overline{x}^m = \benbrace*{(x^m_n)_n}$ und $\overline{x}^{m'} = \benbrace*{(x^{m'}_n)_n} $, dann gilt also für alle $m,m' >M(\varepsilon)$
	\begin{equation*}
		\lim_{ n \to \infty} d \enbrace*{x^m_n, x^{m'}_n} < \frac{\varepsilon}{3}  \label{eq:15:absch1}\tag{*}
	\end{equation*}
	Für alle $m$ ist $(x^m_n)_n$ eine Cauchyfolge, also gilt
	\begin{equation*}
		\forall m : \exists N(m) : \forall n,n' \ge N(m) : d \enbrace*{x^m_n, x^m_{n'}} < \frac{1}{m}  \label{eq:15:absch2}\tag{**}
	\end{equation*}
	Setze nun $z_n := x^n_{N(n)}$. Behauptung: $(z_n)_n$ ist eine Cauchyfolge. Sei $\varepsilon>0$. Dann gilt für $n,m>N=\max\set{M(\varepsilon), \frac{3}{\varepsilon}}$
	für ein $k > N(m), N(n)$ \marginnote{$d(z_n,z_m)$ ist unabhängig von $k$, also kann man $k$ beliebig groß wählen}
	\begin{align*}
		d(z_n,z_m) = d \enbrace*{x^n_{N(n)}, x^m_{N(m)}} \le 
		\underbrace{d\enbrace*{x^n_{N(n)}, x^n_k}}_{\stackrel{\mbox{\tiny\eqref{eq:15:absch2}}}{<} \frac{1}{n} < \frac{1}{N} \le \frac{\varepsilon}{3}} + 
		\underbrace{d\enbrace[\Big]{x^n_k, x^m_k}}_{\stackrel{\mbox{\tiny\eqref{eq:15:absch1}}}{\le} \frac{\varepsilon}{3}}   + 
		\underbrace{d\enbrace*{x^m_k, x^m_{N(m)}}}_{\stackrel{\mbox{\tiny\eqref{eq:15:absch2}}}{<} \frac{1}{m} < \frac{1}{N} \le \frac{\varepsilon}{3}} 
		< \varepsilon
	\end{align*}
	$\Rightarrow (z_n)_n$ ist eine Cauchyfolge, also 
	\begin{equation*}
		\forall \varepsilon>0 : \exists N_z(\varepsilon) : \forall n,m > N_z(\varepsilon)  : d(z_n,z_m) < \varepsilon  \label{eq:15:absch3} \tag{\#}
	\end{equation*}
	Es bleibt zu zeigen: $\lim_{m \to \infty} \overline{x}^m = \benbrace*{(z_n)_n}$. Sei dazu $\varepsilon>0$. Dann gilt für 
	$m \ge \max \set{\frac{2}{\varepsilon}, N_z(\varepsilon)}$
	\begin{align*}
		\tilde{d} \enbrace[\big]{\overline{x}^m, \benbrace*{(z_n)_n}} = \lim_{ n \to \infty} d \enbrace*{x^m_n, x^n_{N(n)}}\le \lim_{ n \to \infty} 
		\enbrace[\Bigg]{ \underbrace{d\enbrace*{x_n^m, x^m_{N(m)}}}_{\stackrel{\mbox{\tiny\eqref{eq:15:absch2}}}{<} \frac{1}{m} \le \frac{\varepsilon}{2}  } +
		\underbrace{d\enbrace*{x^m_{N(m)}, x^n_{N(n)}}}_{=d(z_m,z_n) \stackrel{\mbox{\tiny\eqref{eq:15:absch3}}}{<} \frac{\varepsilon}{2} } } < \varepsilon
	\end{align*}
	Also gilt $\overline{x}^m \xrightarrow{m \to \infty} \benbrace*{(z_n)_n}$ und $(\tilde{X},\tilde{d})$ ist vollständig. \bewende
	\end{description}
\end{description}
% subsection 15 (end)

\subsection[Definition: Raum der beschränkten, stetigen Abbildungen]{Definition} % (fold)
\label{sub:16}
Sei $(W,\mathcal{T})$ ein topologischer Raum und $(X,d)$ ein metrischer Raum. Sei 
\[
	C_b(W,X) := \set[f \colon W \to X]{f \text{ stetig und beschränkt}} 
\]
versehen mit der Metrik $d_{W,X}$, die definiert ist durch
\[
	d_{W,X}(f,g) = \sup_{t \in W} d \enbrace[\big]{f(t), g(t)} 
\]
% subsection 16 (end)

\subsection[Bemerkung: $d_{W,X}$ als Metrik auf $C(W,X)$]{Bemerkung} % (fold)
\label{sub:17}
Auf $C(W,X) = \set{f \colon W \to X \text{ stetig}} $ ist $d_{W,X}$ eine \Index{erweiterte Metrik}, d.h. der Wert $\infty$ ist möglich.
$\tilde{d}_{W,X} := \min \set{1, d_{W,X}} $ ist dann eine \enquote{echte} Metrik auf $C(W,X)$.
% subsection 17 (end)

\subsection[Proposition: Wenn $X$ vollständig ist, dann sind $C_b(W,X)$ und $C(W,X)$ vollständig]{Proposition} % (fold)
\label{sub:18}
Falls $X$ vollständig ist, dann sind $C_b(W,X)$ und $C(W,X)$ vollständig (bezüglich $d_{W,X}$ bzw. $\tilde{d}_{W,X}$).
\minisec{Beweis}
Sei $(f_n)_n$ eine Cauchy-Folge in $C_b(W,X)$, also
\[
	\forall \varepsilon>0 : \exists N \in \mathds{N} : \forall n,m >N : \sup_{t \in W} d \enbrace[\big]{f_n(t), f_m(t)}< \varepsilon 
\]
Also ist $\enbrace[\big]{f_n(t)}_n$ für alle $t$ eine Cauchyfolge in $X$. Da $X$ vollständig ist, existiert ein $x_t \in X$ sodass 
$f_n(t) \xrightarrow{n \to \infty} x_t$. Definiere also $f(t) := x_t$ punktweise.
\begin{description}
	\item[$f$ ist Grenzwert:] Sei $\varepsilon>0$. Dann gibt es ein $N\in\mathds{N}$, sodass für alle $k,l\ge N$ gilt $d_{W,X}(f_k,f_l)<\varepsilon$, da $(f_n)_n$ Cauchy ist.
	Für $n \ge N$ gilt nun $d_{W,X}(f,f_n)\le\varepsilon$, da 
	\[
		\sup_{t\in W} d \enbrace[\big]{f(t),f_n(t)} = \adjustlimits \sup_{t\in W} \lim_{k \to \infty} d \enbrace[\big]{f_k(t),f_n(t)} \le \varepsilon
	\]
	\item[$f$ ist stetig:] Sei $\varepsilon>0$ und $t \in W$. Wir finden ein $N \in \mathds{N}$ mit $d_{W,X}(f,f_N)<\frac{\varepsilon}{3}$. Da $f_N$ stetig ist in $t$,
	existiert eine offene Umgebung $U$ von $t$, sodass $d \enbrace[\big]{f_N(t),f_N(s)}< \frac{\varepsilon}{3}$ für alle $s \in U$. Es folgt
	\[
		d \enbrace[\big]{f(t),f(s)} \le d \enbrace[\big]{f(t), f_N(t)} + d \enbrace[\big]{f_N(t),f_N(s)} + d \enbrace[\big]{f_N(s),f(s)} < 
		\frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3} = \varepsilon     
	\]
	Damit ist $f$ stetig in $t \in W$, also auch auf ganz $W$, da $t$ beliebig war.
	\item[$f$ ist beschränkt:] Seien $x,y \in W$. Sei $c>0$. Finde $N \in \mathds{N}$, sodass $d_{W,X}(f,f_N) < c$. Dann gilt
	\[
		d \enbrace[\big]{f(x),f(y)} \le d \enbrace[\big]{f(x), f_N(x)} + d \enbrace[\big]{f_N(x),f_N(y)} + d \enbrace[\big]{f_N(y),f(y)} < 
		2c + d \enbrace[\big]{f_N(x),f_N(y)} < \infty
	\]
	da $f_N$ beschränkt ist. \bewende
	% Es gilt $\forall \varepsilon>0 : \exists N_t$ sodass $d(x_t, f_{N_t}(t)) \le \varepsilon$. Weiter ist
% 		\[
% 			\sup_{t \in W} d \enbrace[\big]{f_n(t), f_m(t)}\le \varepsilon \enspace \forall n,m \ge N_t
% 		\]
% 		$\Rightarrow \forall t : d \enbrace*{f_n(t), f_m(t)}\le \varepsilon \enspace \forall n,m \ge N$
\end{description}
% subsection 18 (end)

\subsection[Proposition über eine Folge von Bällen]{Proposition} % (fold)
\label{sub:19}
Sei $(X,d)$ ein vollständiger metrischer Raum und $\enbrace*{\overline{B}(x_n, \varepsilon_n)}_{n \in \mathds{N}}$ mit 
$\overline{B}(x_{n+1}, \varepsilon_{n+1}) \subseteq \overline{B}(x_n,\varepsilon_n)$ und $\varepsilon_n \xrightarrow{n \to \infty} 0$. Dann existiert genau 
ein Punkt in $\bigcap_{n \in \mathds{N}} \overline{B}(x_n, \varepsilon_n)$.
\minisec{Beweis}
\begin{description}
	\item[Existenz:] Behauptung: Die Folge der Mittelpunkte $(x_n)_{n \in\mathds{N}}$ ist eine Cauchy-Folge: Sei $\varepsilon >0$. Finde $N \in \mathds{N}$,
	sodass $\varepsilon_n \le \varepsilon$ für alle $n \ge N$. Dann gilt für alle $n \ge m \ge N$
	\[
		d(x_n, x_m) \le \varepsilon_m \le \varepsilon,
	\]
	da $x_n \in \overline{B}(x_m, \varepsilon)$ ist. Da $X$ vollständig ist, existiert ein $x \in X$ mit $x_n \xrightarrow{n \to \infty} x$.
	
	Behauptung: $x \in \bigcap_{n \in \mathds{N}} \overline{B}(x_n, \varepsilon_n)$. Wähle dazu ein $N \in \mathds{N}$. Dann ist $(x_n)_{n \ge N}$ eine
	Cauchy-Folge in $\overline{B}(x_N,\varepsilon_N)$. $\overline{B}(x_N,\varepsilon_N)$ ist ein abgeschlossener Teilraum von $X$ und somit vollständig. 
	Also ist $x \in \overline{B}(x_N,\varepsilon_N)$. Da $N$ beliebig war, gilt $x \in \overline{B}(x_n,\varepsilon_n)$ für jedes $n \in \mathds{N}$.
	\item[Eindeutigkeit:] Es seien $x,y \in \bigcap_{n \in \mathds{N}} \overline{B}(x_n,\varepsilon_n)$, also 
	$x,y \in \overline{B}(x_n, \varepsilon_n) $ für alle $ n \in \mathds{N}$. Dann gilt
	\[
		d(x,y) \le d(x,x_n) + d(x_n,y) \le 2 \cdot \varepsilon_n \xrightarrow{n \to \infty} 0
	\]
	für jedes $n \in \mathds{N}$ und somit $d(x,y)=0 \Leftrightarrow x=y$. \bewende
\end{description}
% subsection 19 (end)

\subsection[Satz von Baire]{Satz von Baire\protect\footnote{nach René Louis Baire, \url{https://de.wikipedia.org/wiki/René_Louis_Baire}}} % (fold)
\label{sub:110} 
Es gelten folgende äquivalente Formulierungen:
\begin{enumerate}[a)]
	\item Sei $(X,d)$ ein vollständiger metrischer Raum und $A_0, A_1, \ldots $ eine Folge abgeschlossener Teilmengen. Falls $\bigcup_{n \in \mathds{N}} A_n$ eine
	offene Kugel enthält, so auch eines der $A_n$.
	\item In einem vollständigen metrischen Raum hat eine abzählbare Vereinigung von abgeschlossenen Mengen ohne innere Punkte keine inneren Punkte.
	\item In einem vollständigen metrischen Raum ist ein abzählbarer Durchschnitt von dichten offenen Mengen wieder dicht.
\end{enumerate}
\minisec{Beweis}
Zunächst die Äquivalenz: 
\begin{description}
	\item[a)$\Rightarrow$b):] Klar, da b) Kontraposition von a) ist.
	\item[b)$\Rightarrow$c):] Für $n \in N$ sei $U_n \subseteq X$ offen und dicht. Dann ist $X \setminus U_n \subseteq X$ abgeschlossen und hat keine inneren Punkte. Aus b) 
	folgt nun, dass
	\[
		\bigcup_{n \in \mathds{N}}X \setminus U_n = X \setminus \bigcap_{n \in \mathds{N}} U_n
	\]
	keine inneren Punkte hat. Also ist $\bigcap_{n \in \mathds{N}} U_n$ dicht in $X$.
	\item[c)$\Rightarrow$a):] Ebenso (Zeige: Keines der $A_n$ enthält offene Kugel $\Rightarrow \bigcup_n A_n$ enthält keine offen Kugel).
\end{description}
Wir wollen nun a) durch Widerspruch beweisen, d.h. wir nehmen an, dass gilt: Jede offene Kugel schneidet $X \setminus A_k$ für jedes $k \in \mathds{N}$. Dazu wollen wir 
Folgen $(x_k)_{k \in \mathds{N}} \subset X$, $(\varepsilon_k)_{k \in \mathds{N}} \subset (0,1]$ finden mit 
\begin{enumerate}[(i)]
	\item $\varepsilon_k < \frac{1}{k+1}$,
	\item $\overline{B}(x_{k+1}, \varepsilon_{k+1}) \subset (X \setminus A_k) \cap B(x_k, \varepsilon_k) \subset \overline{B}(x_k, \varepsilon_k)$,
	\item $\overline{B}(x_k, \varepsilon_k) \subset \bigcup_{n \in \mathds{N}} A_n$
\end{enumerate}
für $k \in \mathds{N}$. Dann gilt 
\[
	\bigcup_{k \in \mathds{N}} A_k \stackrel{\text{(iii)}}{\supset} \bigcap_{k \in \mathds{N}} \overline{B}(x_k, \varepsilon_k) \stackrel{\text{(ii)}}{\subset} 
	\bigcap_{k \in \mathds{N}}(X \setminus A_k) \cap B(x_k, \varepsilon_k) \subset \bigcap_{k \in \mathds{N}} X \setminus A_k = X \setminus 
	\enbrace*{\bigcup_{k \in \mathds{N}} A_k} 
\]
Aber wegen (i) und (ii) existiert nach Proposition \ref{sub:19} ein $x \in \bigcap_{k \in \mathds{N}}\overline{B}(x_k,\varepsilon_k)$. \light \medskip \\
Wir suchen also eine Abbildung $\overline{c} \colon \mathds{N} \to X \times (0,1], k  \mapsto(x_k, \varepsilon_k)$ mit (i),(ii),(iii) für $k \in \mathds{N}$. Wir setzen
\begin{align*}
	P_m &:= \set[{c \colon \set{0, \ldots ,m} \to X \times (0,1] }]{\begin{array}{l}
		\text{(i),(iii) gilt für }k \in \set{0, \ldots , m}, \\
		\text{(ii) gilt für }k \in \set{0, \ldots , m-1}
	\end{array} }  \\
	P_\infty &:= \set[{c \colon \mathds{N} \to X \times (0,1]}]{\text{(i),(ii),(iii) gilt für } k \in \mathds{N} \rule{0cm}{0.3cm}} 
\end{align*}
Die Menge $P := \enbrace*{\bigcup_{m \in \mathds{N}} P_m} \cup P_\infty$ ist partiell geordnet bezüglich \enquote{$\prec$}:
\[
	c \prec c' :\iff m \le m' \text{ und } c'\big|_{\set{0,\ldots ,m}} =c , \text{ bzw. } c'=c \text{ falls } m=m'=\infty
\]
$P$ ist nicht leer, denn nach Vorraussetzung existiert $(x_0,\varepsilon_0)$ mit $0 < \varepsilon_0 < 1$ und $\overline{B}(x_0, \varepsilon_0) \subset 
\bigcup_{n \in\mathds{N}} A_n$, d.h. $\enbrace[\big]{0 \mapsto (x_0,\varepsilon_0)} \in P_0$. Jede total geordnete Teilmenge $\emptyset \not= \Gamma$ von $P$ besitzt eine 
obere Schranke:
\begin{itemize}
	\item Falls $\Gamma$ ein $c\in P_\infty$ enthält, so ist $c$ obere Schranke (Warum?).
	\item Ebenso falls $\Gamma$ ein $c \in P_{\overline{m}}$ enthält und $\Gamma \cap P_{m'}= \emptyset$ für alle $\overline{m} < m' \in \mathds{N} \cup \set{\infty}$.
	\item Falls $\Gamma \subset \bigcup_{\mathds{N}} P_m$, aber $\Gamma \not\subset \bigcup_{m \le \overline{m}} P_m$ für jedes $\overline{m}$, so definieren wir eine
	obere Schranke in $P_\infty$ durch Einschränkung (Wie genau?).
\end{itemize}
Mit dem Lemma von Zorn folgt, dass $P$ ein maximales Element $\overline{c}$ besitzt. Wir müssen noch zeigen, dass $\overline{c}\in P_\infty$ wie gewünscht:

Falls $\overline{c} \in P_m$ für ein $m \in \mathds{N}$, so gilt nach Annahme, dass $\enbrace*{X \setminus A_m} \cap B(x_m, \varepsilon_m) \not= \emptyset$. Dann 
existiert aber $(x_{m+1},\varepsilon_{m+1}) \in X \times (0,1]$ mit $\varepsilon_{m+1} < \frac{1}{m+2}$ und 
\[
	\overline{B}(x_{m+1}, \varepsilon_{m+1}) \subset  \enbrace*{X \setminus A_m} \cap B(x_m,\varepsilon_m) ,
\]
da $\enbrace*{X \setminus A_m} \cap B(x_m,\varepsilon_m)$ offen ist. Definiere $\overline{\overline{c}} : \set{0, \ldots , m+1} \to X \times (0,1]$ durch
\[
	k \longmapsto \begin{cases}
		(x_k,\varepsilon_k), &\text{ falls } k=m+1\\
		\overline{c}(k), &\text{ falls } k \in \set{0, \ldots , m} 
	\end{cases},
\]
dann gilt $\overline{c} \prec \overline{\overline{c}} \in P_{m+1}$ im Widerspruch zur Maximalität von $\overline{c}$. \bewende
% subsection satz_baire (end)

\subsection[Bemerkungen zum Satz von Baire]{Bemerkungen} % (fold)
\label{sub:111}
\begin{enumerate}[(i)]
	\item Die Aussage gilt auch für lokalkompakte Hausdorffräume.\index{lokalkompakt} \hfill(Übung, Blatt 2, Aufgabe 2)
 	\item Tatsächlich genügt eine schwächere Form des Auswahlaxioms (DC); das abzählbare Auswahlaxiom jedoch nicht.
	\item Falls $X$ \Index{separabel} ist (d.h. falls eine abzählbare dichte Teilmenge von $X$ existiert), dann lässt sich der Satz auch ohne (AC) beweisen.\hfill (Übung, 
	Blatt 2, Aufgabe 1)
\end{enumerate}
% subsection 111 (end)

\subsection{Corollar: Satz der gleichmäßigen Beschränktheit} % (fold)
\label{sub:112}
Sei $(X,d)$ ein vollständiger metrischer Raum. Sei $F \subset C(X,\mathds{R})$ eine Menge, die \Index{punktweise gleichmäßig beschränkt} ist, d.h. für jedes $x \in X$ 
existiert $K_x \in \mathds{R}$, sodass
\[
	\abs*{f(x)} \le K_x \enspace \forall f \in F. 
\] 
Dann existieren $\emptyset \not= U \subset X$ offen und $K \in \mathds{R}$ so, dass 
\[
	\abs*{f(x)} \le K \enspace \forall x \in U, f \in F 
\]
\minisec{Beweis}
Definiere für $n \in \mathds{N}$
\begin{align*}
	X \supset A_n := \set[x \in X]{\abs*{f(x)} \le n \enspace \forall f \in F \rule{0cm}{0.3cm}} = \bigcap_{f \in F} f ^{-1} \enbrace[\big]{[-n,n]} 
\end{align*}
Also sind die $A_n \subset X$ abgeschlossen. Es ist $\bigcup_{n \in \mathds{N}} A_n = X$, wegen der Voraussetzung von punktweiser gleichmäßiger Beschränktheit. 
Da $X$ eine nichtleere, offene Teilmenge $U$ enthält, existiert nach dem \hyperref[sub:110]{Satz von Baire} ein $n \in N$ mit $U \subset A_n$. \bewende
% subsection 112 (end)
% section 1 (end)
\newpage

\section{Normierte Räume, Hahn-Banach Sätze} % (fold)
\label{sec:2}

\subsection[Definition: Topologischer Vektorraum]{Definition} % (fold)
\label{sub:21}
Sei $X$ ein $\mathds{K}$-Vektorraum mit einer Topologie $\mathcal{T}$. Wir sagen, $X$ ist ein \Index{topologischer Vektorraum}, falls Addition und Skalarmultiplikation stetig sind:
\begin{align*}
	+ \colon X \times X  \to X \quad &, \quad(x,y) \mapsto x+y \\
	\makebox[\widthof{$+$}]{$\cdot$} \colon \mathds{K} \times X \to X \quad &, \quad (\lambda ,x) \mapsto \lambda \cdot x
\end{align*}
% subsection 21 (end)

\subsection[Proposition: Unterräume topologischer Vektorräume sind topologische Vektorräume]{Proposition} % (fold)
\label{sub:22}
Sei $X$ ein topologischer $\mathds{K}$-Vektorraum und $Y \subset X$ ein Untervektorraum. Dann sind $Y$ und $\overline{Y}$ topologische Vektorräume mit der Unterraumtopologie.
\minisec{Beweis}
\begin{itemize}
	\item Klar für $Y$.
	\item Zu zeigen: $\overline{Y}$ ist ein Untervektorraum. Seien $x_0, y_0 \in \overline{Y}$ und sei $U$ eine offene Menge in $X$ mit $x_0 + y_0 \in U$. Dann existieren
	offene Mengen $V,W$ von $X$ mit $x_0 \in V$, $y_0 \in W$ und $V+W \subset U$, da die Addition stetig ist. Da $x_0 \in \overline{Y}$ und $x_0 \in V$, existiert 
	$x_1 \in Y$ mit $x_1 \in V$. Analog existiert $y_1 \in Y$ mit $y_1 \in W$. Daher ist $x_1 + y_1 \in Y \cap U$. Da $U$ eine beliebige offene Umgebung um $x_0+ y_0$ ist,
	folgt $x_0 +y_0 \in \overline{Y}$.
	
	Skalarmultiplikation genauso. \bewende
\end{itemize}
% subsection 22 (end)

\subsection[Proposition: Normierte Vektorräume sind topologische Vektorräume]{Proposition} % (fold)
\label{sub:23}
Ein normierter Vektorraum $(X, \norm{\cdot})$ ist ein topologischer Vektorraum bezüglich der von $\norm{\cdot}$ induzierten Topologie.
\minisec{Beweis}
Es gilt 
\begin{align*}
	\norm[X]{(x+y) - (x'+y')} \le \norm[X]{x-x'} + \norm[X]{y-y'}   
\end{align*}
Daher ist $+ \colon X \times X \to X$ gleichmäßig stetig bezüglich der Norm $\norm[X \times X]{(z,z')} := \norm[X]{z} + \norm[X]{z'}$ auf $X \times X$. Ähnlich folgt
\begin{align*}
	\norm[X]{\lambda x - \lambda' x'} = \norm[X]{\lambda (x-x') + (\lambda -\lambda')x'} \le \abs*{\lambda } \cdot \norm[X]{x-x'} + \abs*{\lambda -\lambda'} \cdot 
	\norm[X]{x'}      
\end{align*}
Daher ist $\cdot \colon \mathds{K} \times X \to X$ stetig, denn falls $(\lambda_i)_{i \in I}$ ein Netz in $\mathds{K}$ und $(x_i)_{i \in I}$ ein Netz in $X$ ist mit 
$\lambda_i \to \lambda $, $x_i\to x$, dann  gilt $\lambda_i x_i \to \lambda x$, also $\norm[X]{\lambda_i x_i - \lambda x} \to 0$. \bewende
% subsection 23 (end)

\subsection[Proposition: Stetigkeit einer linearen Abbildung zwischen topologischen Vektorräumen]{Proposition} % (fold)
\label{sub:24}
Seien $X,Y$ topologische Vektorräume, $T : X \to Y$ linear. Dann sind äquivalent:\index{stetig}
\begin{enumerate}[(1)]
	\item $T$ ist stetig.
	\item $T$ ist stetig in $0$.
	\item $T$ ist stetig in einem Punkt $\overline{x}$.
\end{enumerate}
\minisec{Beweis}
(1) $\Rightarrow$ (2) $\Rightarrow $ (3) ist klar. Wir zeigen (3) $\Rightarrow$ (1):

Für $y \in X$ definiere $L_y \colon X \to X$, $x \mapsto y+ x$. Dann ist $L_y$ bijektiv und stetig. Da auch $L_{-y}$ stetig ist, ist $L_y$ ein Homöomorphismus. Sei nun $T$ in 
$\overline{x}$ stetig. Sei $x_0 \in X$. Wir zeigen: $T$ ist stetig in $x_0$. Setze $y := \overline{x} - x_0$. Dann ist
\begin{align*}
	T(x_0) = T(x_0+y-y) = T(-y) + T(x_0+y) = L_{T(-y)} \circ T \circ L_y (x_0)
\end{align*}
Da $L_y(x_0) = \overline{x}$, $T$ stetig in $\overline{x}$ ist und $L_y$, $L_{T(-y)}$ stetig sind, ist somit $T$ stetig in $x_0$. \bewende
% subsection 24 (end)

\subsection[Definition: Stetige Funktionale und Operatoren]{Definition} % (fold)
\label{sub:25}
Für topologische Vektorräume $X,Y$ definieren wir 
\[
	\mathcal{L}(X,Y) = \set[T \colon X \to Y]{T \text{ linear und stetig}}  
\]
Wir schreiben $X^*:= \mathcal{L}(X,\mathds{K})$ für den \Index{Dualraum} von $X$. Die Elemente von $X^*$ heißen (stetige) \Index{Funktionale}.\footnote{Aus der linearen Algebra: $X^* = \Hom(X,\mathds{R}) = \set[T : X \to Y]{T \text{ linear}}$. Die Elemente von $X^*$ heißen Funktionale.}
Die Elemente von $\mathcal{L}(X,X)$ heißen (stetige) \Index{Operatoren} auf $X$. 
% subsection 25 (end)

\subsection[Bemerkung: Vektorraumstruktur auf $\mathcal{L}(X,Y)$, Algebrastruktur auf $\mathcal{L}(X,X)$]{Bemerkung} % (fold)
\label{sub:26}
$\mathcal{L}(X,Y)$ und $\mathcal{L}(X,\mathds{K}) = X^*$ sind Vektorräume, wobei die Vektorraumstruktur punktweise definiert ist. $\mathcal{L}(X,X)$ ist sogar eine Algebra mittels Komposition.\index{Algebra}
% subsection 26 (end)

\subsection[Proposition: Stetigkeit linearer Abbildungen mittels Normabschätzung]{Proposition} % (fold)
\label{sub:27}
Seien $X,Y$ normierte Vektorräume, $T \colon X \to Y$ linear. Dann ist $T$ stetig genau dann, wenn ein $\mu \ge 0$ existiert, sodass 
\[
	\norm[Y]{T(x)} \le \mu \cdot \norm[X]{x} \quad \forall x \in X  
\]
Ein stetiger Operator zwischen normierten Vektorräumen heißt deswegen auch \bet{beschränkt}\index{Operator!beschränkter Operator}.
\minisec{Beweis}
\begin{description}
	\item[\enquote{$\Leftarrow$}:] Klar: Wenn $x_n \to 0$, dann $T(x_n) \to 0$, denn $\norm{T(x_n)} \le \mu \norm{x_n } \to 0$. Also ist $T$ stetig in $0$ und nach
	\ref{sub:24} überall stetig.
	\item[\enquote{$\Rightarrow $}:] Angenommen $T$ ist stetig. Dann setzen wir
	\[
		\mu := \sup \set[{\frac{1}{\norm[X]{x}} \cdot \norm[Y]{T(x)}  }]{x \in X \setminus \set{0} } 
	\] 
	Falls $\mu = \infty$, dann existieren $x_n \in X$ mit $\frac{1}{\norm[X]{x_n}} \cdot \norm[Y]{T(x_n)} \ge n$. Betrachte 
	$x_n' :=  \frac{1}{n \cdot \norm[X]{x_n}} \cdot x_n $, dann $\norm{x_n'} = \frac{1}{n}$, also $x_n' \to 0$. Aber es gilt $T(x_n')\not\longrightarrow 0$, denn
	\begin{align*}
		\norm[Y]{T(x_n')} = \frac{1}{n \cdot \norm[X]{x_n} } \cdot \norm[Y]{T(x_n)} \ge 1 
	\end{align*}
	für alle $n$ im Widerspruch zur Stetigkeit von $T$. Also $\mu < \infty$. \bewende 
\end{description}
% subsection 27 (end)

\subsection[Definition: Operatornorm]{Definition} % (fold)
\label{sub:28}
Seien $X,Y$ normierte Vektorräume, $T \in \mathcal{L}(X,Y)$. Die \bet{Norm} (oder \Index{Operatornorm}) von $T$ ist
\[
	\norm{T} = \sup \set[{\frac{1}{\norm[X]{x}} \cdot \norm[Y]{T(x)}}]{x \in X \setminus \set{0} } = \sup \set[{\norm[Y]{T(x)} }]{x \in X \text{ mit } \norm[X]{x} 
	= 1\rule{0cm}{0.4cm}}   
\] 
Nach \ref{sub:27} ist $\norm{T}<  \infty $ und $\norm[Y]{T x} \le \norm{T} \cdot \norm[X]{x}$ für alle $x \in X$.
% subsection 28 (end)

\subsection[Propostion: $\mathcal{L}(X,Y)$ ist ein normierter Raum mit der Operatornorm]{Proposition} % (fold)
\label{sub:29}
Seien $X,Y$ normierte Räume, betrachte $\mathcal{L}(X,Y)$ mit $\norm{\cdot}$. Dann ist $\mathcal{L}(X,Y)$ ein normierter Raum.
\minisec{Beweis}
\begin{itemize}
	\item $\norm{T} \ge 0 $ für alle $T \in \mathcal{L}(X,Y)$.
	\item Es gilt
	\begin{align*}\SwapAboveDisplaySkip
		\norm{T}=0 \iff \forall x \in X, \norm[X]{x}=1 : \norm[Y]{Tx}=0 &\iff \forall x \in X, \norm[X]{x}=1 : Tx=0 \\ &\iff \forall x \in X : Tx=0 \iff T=0
	\end{align*}
	\item Sei $\lambda  \in \mathds{K}$, $T \in \mathcal{L}(X,Y)$. Dann gilt
	\begin{equation*}
		\norm{\lambda \cdot T} = \sup_{\norm[X]{x}=1} \norm[Y]{\lambda  \cdot T(x)}  = \sup_{\norm[X]{x}=1 } \abs*{\lambda } \cdot \norm[Y]{Tx} =
		\abs*{\lambda } \cdot \sup_{\norm[X]{x}=1 } \norm[Y]{Tx} = \abs*{\lambda } \cdot \norm{T}
	\end{equation*}
	\item Seien $T_1, T_2 \in \mathcal{L}(X,Y)$ und $x \in X$ mit $\norm[X]{x}=1 $. Dann gilt
	\begin{align*}
		\norm[Y]{(T_1 + T_2)(x)} = \norm[Y]{T_1 x + T_2 x} &\le \norm[Y]{T_1 x} + \norm[Y]{T_2 x}\\
		\shortintertext{Damit folgt weiter}
		\norm{T_1 +T_2} = \sup_{\norm[X]{x}=1 } \norm[Y]{(T_1+ T_2) (x)} &\le \sup_{\norm[X]{x}=1} \enbrace[\big]{\norm[Y]{T_1 x} + \norm[Y]{T_2 x}  }   \\
		&\le \sup_{\norm[X]{x}=1} \sup_{\norm[X]{x'}=1} \enbrace[\big]{\norm[Y]{T_1 x} + \norm[Y]{T_2 x'}} = \norm{T_1} + \norm{T_2} \bewende   
	\end{align*}
\end{itemize}
% subsection 29 (end)

\subsection[Definition: Normierte $\mathds{K}$-Algebra]{Definition} % (fold)
\label{sub:210}
Eine $\mathds{K}$-Algebra $A$ heißt \bet{normiert}\index{normierte $\mathds{K}$-Algebra}, falls $A$ mit einer Norm $\norm{\cdot}$ versehen ist, so dass $(A,\norm{\cdot})$ 
ein normierter Vektorraum ist und folgende Ungleichung gilt
\[
	\norm{a b} \le \norm{a} \cdot \norm{b} \quad \forall a,b \in A   
\]
Falls $A$ \Index{unital} ist, d.h. es existiert ein Einselement $1_A$, dann gilt $\norm{1_A} \le 1$.
Wenn $\norm{1_A} < 1$, dann ist $\norm{1_A} = \norm{1_A \cdot 1_A} \le \norm{1_A} \cdot \norm{1_A} \Rightarrow \norm{1_A} =0 $. Also $1_A=0$ und damit $A= \set{0}$.
% subsection 210 (end)

\subsection[Beispiele für normierte Algebren]{Beispiele} % (fold)
\label{sub:211}
\begin{enumerate}[(i)]
	\item $C(\Omega,\mathds{K})$, wobei $\Omega$ ein kompakter Hausdorffraum und die Multiplikation punktweise ist. Betrachte die Norm $\norm[\infty]{.}$. Für 
	$f,g \colon \Omega \to K$ gilt dann $\norm[\infty]{f \cdot g} \le \norm[\infty]{f} \!\cdot \norm[\infty]{g}$
	\item $\mathcal{L}(X,X)$ für einen normierten Raum $X$ mit $\norm{.}$, denn für $S,T \in \mathcal{L}(X,X)$ gilt
	\[
		\norm{S T} = \sup_{\norm[X]{x}=1} \norm[X]{S(T(x))} \le \sup_{\norm[X]{x}=1} \enbrace[\big]{\norm{S} \cdot \norm[X]{Tx}} = \norm{S} \cdot \norm{T}.     
	\]
\end{enumerate}
% subsection 211 (end)

\subsection[Proposition: Norm auf dem Produkt normierter Räume]{Proposition} % (fold)
\label{sub:212}
Seien $X,Y$ normierte Räume. Dann ist $X \times Y$ ein normierter Raum mit 
\begin{align*}\SwapAboveDisplaySkip
	 \norm[\minwidthbox{1\hfill}{1em}]{(x,y)} &:= \norm[X]{x} + \norm[Y]{y}   \quad \text{ für } (x,y) \in X \times Y \\
	\text{oder } \quad \norm[\minwidthbox{\infty}{1em}]{(x,y)} &:= \max \set{\norm[X]{x}, \norm[Y]{y}}  
\end{align*}
Beide Normen sind äquivalent.
\minisec{Beweis}
\emph{Übung!} \bewende
% subsection 212 (end)

\subsection[Proposition und Definition: Norm auf dem Quotientenraum]{Proposition und Definition} % (fold)
\label{sub:213}
Sei $X$ ein normierter Raum und $Y \subseteq X$ ein abgeschlossener Unterraum. Dann ist $\nicefrac{X}{Y} = \set[x +Y]{x \in X}$ ein normierter Raum mit
\[
	\norm[\nicefrac{X}{Y}]{x +Y} := \inf \set[{\norm[X]{x+y} }]{y \in Y}  
\]
Die Quotientenabbildung $q \colon X \twoheadrightarrow \nicefrac{X}{Y}$, $x \mapsto x+ Y$ ist stetig und linear. Weiter ist $\norm{q} \le 1$ und $q$ offen, d.h. bildet offene Mengen 
in $X$ auf offene Mengen in $\nicefrac{X}{Y}$ ab.
\minisec{Bemerkung}
Daraus folgt, dass Normtopologie und Quotiententopologie auf $\nicefrac{X}{Y}$ gleich sind.
\minisec{Beweis}
\begin{enumerate}[a)]
	\item $\nicefrac{X}{Y}$ ist ein Vektorraum mit $(x+Y) + (x'+Y) = (x+x') + Y$.
	\item Sei $\overline{x} = x+ Y \in \nicefrac{X}{Y}$. Es gilt
	\begin{itemize}
		\item $\norm{\overline{x}} \ge 0 $ ist klar für alle $\overline{x} \in \nicefrac{X}{Y}$
		\item Angenommen $\norm[\nicefrac{X}{Y}]{\overline{x}} =0$, d.h. $\inf_{y \in Y} \norm[X]{x+y} =0$. Also existiert eine Folge $(y_n)_n \subset Y$ mit
		$\norm[X]{x+y_n} \xrightarrow{n \to \infty} 0 \Rightarrow y_n \xrightarrow{n \to \infty} -x$. Also ist $-x$ und damit auch $x$ in $Y$, da $Y$ abgeschlossen ist.
		Damit ist $\overline{x}=0 $ 
	\end{itemize}
	\item Sei $0 \not= \lambda \in \mathds{K}$, $x \in X$. Dann gilt
	\begin{align*}
		\norm[\nicefrac{X}{Y}]{\lambda \cdot \overline{x}} = \inf_{y \in Y} \norm[X]{\lambda x + y} = \inf_{y \in Y} \norm[X]{\lambda x + \lambda y} 
		= \inf_{y \in Y} \abs*{\lambda } \cdot \norm[X]{x+y}  =   \abs*{\lambda } \cdot \norm[\nicefrac{X}{Y}]{\overline{x}}  
	\end{align*}
	\item Seien $\overline{x}, \overline{y} \in \nicefrac{X}{Y}$. Dann gilt
	\begin{align*}
		\norm[\nicefrac{X}{Y}]{\overline{x} + \overline{y}} = \inf_{z \in Y} \norm[X]{x+y+z} = \inf_{z,z' \in Y} \norm[X]{x+y+z+z'} &\le \inf_{z,z' \in Y}
		\enbrace[\Big]{\norm[X]{x+z} + \norm[X]{y+z'}} \\ &= \norm[\nicefrac{X}{Y}]{\overline{x}} + \norm[\nicefrac{X}{Y}]{\overline{y}}
	\end{align*}
	Damit ist $\norm[\nicefrac{X}{Y}]{.}$ eine Norm auf $\nicefrac{X}{Y}$.
	\item Die Linearität von $q$ ist klar. Es gilt
	\(
		\norm[\nicefrac{X}{Y}]{\overline{x}} = \inf_{y \in Y} \norm[X]{x+y} \le \norm[X]{x}   
	\). Also
	\[
		\norm{q} = \sup_{\norm[X]{x}=1} \norm[\nicefrac{X}{Y}]{q(x)} \le \sup_{\norm[X]{x}=1 } \norm[X]{x} = 1   
	\]
	\item Zu zeigen: $q$ ist offen. Sei $x \in X$, $\varepsilon>0$. Betrachte die offene Kugel $B(x,\varepsilon) \subset X$. Wir zeigen 
	$B(\overline{x}, \varepsilon) \subset q \enbrace*{B(x,\varepsilon)}$. Sei also $\overline{z} = z + Y$ in $B(\overline{x},\varepsilon)$. Dann gilt
	$\norm[\nicefrac{X}{Y}]{\overline{x}- \overline{z}} < \varepsilon$, worauf folgt
	\[
		\inf_{y \in Y} \norm[X]{x-z+y} < \varepsilon 
	\]
	$\Rightarrow$ es existiert $y \in Y : \norm[X]{x-z+y}<\varepsilon $. Es gilt $\overline{z}=\overline{z-y}\in q\enbrace*{B(x,\varepsilon)}$, da $z-y\in B(x,\varepsilon)$.
	
	Sei $V \subset X$ offen, zeige $q(V) \subset \nicefrac{X}{Y}$ offen. Für $x \in V$ finde $\varepsilon>0$, sodass $B(x,\varepsilon) \subset V$. Dann folgt
	$B(\overline{x},\varepsilon) \subseteq q \enbrace[\big]{ B(x,\varepsilon)} \subseteq q(V)$. \bewende
\end{enumerate}
% subsection 213 (end)

\subsection[Definition: Banachraum und Banachalgebra]{Definition} % (fold)
\label{sub:214}
Ein \Index{Banachraum} ist ein vollständiger, normierter Raum. Eine \Index{Banachalgebra} ist eine vollständige normierte Algebra.  
% subsection 214 (end)

\subsection[Beispiel für eine Banachalgebra]{Beispiel} % (fold)
\label{sub:215}
$C(\Omega, \mathds{K} )$, wobei $\Omega$ ein kompakter Hausdorffraum ist, ist eine Banachalgebra.
% subsection 215 (end)

\subsection[Proposition: Produkte und Quotienten von Banachräumen]{Proposition} % (fold)
\label{sub:216}
Produkte und Quotienten (nach abgeschlossenen Unterräumen) von Banachräumen sind wieder Banachräume.
\minisec{Beweis}
\begin{description}
	\item[Produkt $X \times Y$:] Es gilt $\norm[\infty]{(x,y)} = \max \set{\norm{x}, \norm{y}}$. Sei $\enbrace*{(x_n, y_n)}_n \subset X \times Y$ eine Cauchyfolge.
	Dann sind $(x_n)_n$ und $(y_n)_n$ Cauchyfolgen und es gilt $x_n \to x$ und $y_n \to y$ und somit auch $(x_n, y_n) \to (x,y)$.
	\item[Quotient $\nicefrac{X}{Y}$:] Sei $X$ ein Banachraum und $Y \subseteq X$ ein abgeschlossener Unterraum. 	
	Falls $\overline{x}, \overline{y} \in \nicefrac{X}{Y}$, so existiert $y' \in X$, sodass $\overline{y} = \overline{y'}$ und 
	\[
		\norm[X]{x-y'} \le 2 \cdot \norm[\nicefrac{X}{Y}]{\overline{x} - \overline{y}}  
	\] 
	Sei nun $\enbrace*{\overline{x}_n }_n$ eine Cauchyfolge in $\nicefrac{X}{Y}$. Es gibt eine Teilfolge $\enbrace*{\overline{x}_{n_k}}_k$, sodass
	\[
		\sum_{k=1}^{\infty} \norm[\nicefrac{X}{Y}]{\overline{x}_{n_{k-1}} - \overline{x}_{n_k}} < \infty 
	\]
	Falls $\enbrace*{\overline{x}_{n_k} }_k $ konvergiert, dann auch $\enbrace*{\overline{x}_n}_n$. Wir dürfen also ohne Einschränkungen annehmen, dass bereits 
	$\sum_{n=1}^{\infty} \norm{\overline{x}_{n-1} - \overline{x}_n }< \infty$ gilt. Wähle $x_1' \in \overline{x}_1$. Wähle induktiv $x_n' \in X$  mit
	\[
		\norm[X]{x_{n-1}' - x_n'} \le 2 \cdot \norm[\nicefrac{X}{Y}]{\overline{x}_{n-1} - \overline{x}_n}  
	\]
	und $x_n' \in \overline{x}_n$. Dann gilt $\sum_{n=1}^{\infty} \norm[X]{x_{n-1}' - x_n'} < \infty $. Daher ist $(x_n')_n$ eine Cauchyfolge in $X$. Also existiert 
	$x \in X$ mit $x_n \to x$. Dann gilt auch $\overline{x}_n  \to \overline{x}$. \bewende
\end{description}
% subsection 216 (end)

\subsection[Definition: Sublineare Abbildung]{Definition} % (fold)
\label{sub:217}
Sei $X$ ein $\mathds{R}$-Vektorraum. Eine Abbildung $\varphi \colon X \to \mathds{R}$ heißt \Index{sublinear}, falls gilt:
\begin{enumerate}[(i)]
	\item $\varphi(\lambda \cdot x) = \lambda \cdot \varphi(x)$, für $x \in X$, $\lambda \in [0,\infty)$
	\item $\varphi(x+y) \le \varphi(x) + \varphi(y)$, für $x,y \in X$.
\end{enumerate}
Setze $\mathcal{S}(X) := \set[\varphi \colon X \to \mathds{R}]{\varphi \text{ sublinear}}$. $\mathcal{S}(X)$ ist partiell geordnet mit 
\[
	\varphi \le \psi :\iff \varphi(x) \le \psi(x) ,\quad  x \in X
\]
% subsection 217 (end)

\subsection[Beispiele für sublineare Abbildungen]{Beispiele} % (fold)
\label{sub:218}
\begin{minipage}{0.55\textwidth}
	\begin{enumerate}[(i)]
		\item Halbnormen, also nicht positiv-definite Normen, sind sublinear.
		\item Folgende Abbildung $X \to \mathds{R}$ ist sublinear
		
		Benutze $0=\varphi(0)=\varphi(x-x)\le\varphi(x)+ \varphi(-x)$.	
	\end{enumerate}
\end{minipage}\hfill
\begin{minipage}{0.40\textwidth}
	\hfill\begin{tikzpicture}[scale=0.5]
		\draw (-3,0) -- (3,0) node[right]{$X$};
		\draw (0,-3) -- (0,3) node[right]{$\mathds{R}$};
		\draw[very thick, dashed,OrangeRed2] (0,0) -- (220:4);
		\draw[very thick,SeaGreen4] (40:4) -- (0,0) -- (205:4);
	\end{tikzpicture}
	%\captionof{figure}{Eine sublineare Abbildung}
\end{minipage}

\begin{figure}[h]
	
\end{figure}
% subsection 218 (end)

\subsection[Proposition: Die sublinearen Abbildungen $\mathcal{S}(X)$ sind nach unten induktiv geordnet]{Proposition} % (fold)
\label{sub:219}
Sei $X$ ein $\mathds{R}$-Vektorraum. $\mathcal{S}(X)$ ist nach unten \Index{induktiv geordnet}, d.h. jede nichtleere total geordnete Teilmenge besitzt eine untere Schranke.
\minisec{Beweis}
Sei $\emptyset \not= (\varphi_i)_{I} \subseteq \mathcal{S}(X)$ total geordnet. Setze $\varphi(x) := \inf_{i \in I} \varphi_i(x)$, dann gilt 
\[
	- \varphi_i(-x) \le \varphi_i(x) \enspace\Longrightarrow \enspace- \varphi(-x) \le \varphi(x) \le \varphi_i(x) < \infty
\]
ebenso $-\varphi(x) \le \varphi(-x) \le \varphi_i(-x)$ für $x \in X$, $i \in I$. Insbesondere gilt $- \infty <  \varphi(x) < \infty$, $x \in X$. 
Die Sublinearität von $\varphi$ und $\varphi \le \varphi_i$, $i \in I$ sind klar (Warum?). \bewende
% subsection 219 (end)

\subsection[Propostion: Minimale Elemente in $\mathcal{S}(X)$ sind genau die lineare Abbildungen]{Proposition} % (fold)
\label{sub:220}
Sei $X$ ein $\mathds{R}$-Vektorraum und $\varphi \in \mathcal{S}(X)$. Dann ist $\varphi$ in $\mathcal{S}(X)$ minimal genau dann, wenn $\varphi$ linear ist.
\minisec{Beweis}
\begin{description}
	\item["$\Leftarrow$":] Sei $\psi \le \varphi$ mit $\psi$ sublinear und $\varphi$ linear. Dann folgt $\psi(x) \le \varphi(x)$ und $\psi(-x) \le \varphi(-x)$ für 
	$x \in X$.
	\[
		 \Longrightarrow \enspace- \psi(x) \le \psi(-x) \le \varphi(-x) = - \varphi(x)
	\]
	Daraus folgt $\varphi(x) \le \psi(x)$ und somit muss $\varphi=\psi$ gelten. Also ist $\varphi$ minimal.
	\item["$\Rightarrow $":] Sei $\varphi \in \mathcal{S}(X)$ minimal. Zu $\overline{x} \in X$ definiere $\varphi_{\overline{x}} \colon X \to \mathds{R}$ durch
	\[
		\varphi_{\overline{x}}(x) := \inf_{\lambda \ge 0} \enbrace[\Big]{\varphi \enbrace*{x+ \lambda \cdot \overline{x} } - \lambda \cdot \varphi(\overline{x})}
	\]
	Es gilt $\varphi_{\overline{x}}(x) \in (-\infty,\infty)$, denn für $x \in X, \lambda \ge 0$ ist
	\begin{align*}
		&\hphantom{-}\,\, \varphi(\lambda \cdot \overline{x}) = \varphi(x+ \lambda \cdot \overline{x} -x ) \le \varphi(x + \lambda \cdot \overline{x}) + \varphi(-x)\\
		\iff &- \varphi(\lambda \cdot \overline{x}) \ge - \varphi(-x) - \varphi(x + \lambda \cdot \overline{x}) \\
		\iff & - \varphi(-x) \le \varphi(x + \lambda \cdot \overline{x}) - \lambda \cdot \varphi(\overline{x} ) \stackrel{\mbox{\scriptsize \ref{sub:217} (ii)}}{\le} \varphi(x)
	\end{align*}
	$\varphi_{\overline{x}}$ ist sublinear:
	\begin{enumerate}[(i)]
		\item Sei $\mu >0$. Dann gilt für $x \in X$.
		\begin{align*}
			\varphi_{\overline{x}}(\mu \cdot x) = \inf_{\lambda \ge 0} \enbrace[\Big]{ \varphi \enbrace*{\mu \cdot x + \lambda  \cdot \overline{x} } 
			- \lambda \cdot \varphi(\overline{x} )} &= \inf_{\lambda \ge 0 } \mu \cdot  \enbrace*{ \varphi \enbrace*{x + \frac{\lambda }{\mu} \cdot \overline{x}}
			- \frac{\lambda }{\mu} \cdot \varphi(\overline{x} )  } \\
			&= \mu \cdot \inf_{\lambda' \ge 0} \enbrace[\Big]{ \varphi \enbrace*{x+ \lambda' \cdot \overline{x} } - \lambda' \varphi(\overline{x})} \\
			&= \mu \cdot \varphi_{\overline{x}}(x) 
		\end{align*}
		$\varphi_{\overline{x}}(0 \cdot x) = 0$ ist klar.
		\item Zu $x,y \in X$, $\varepsilon>0$ wähle $\lambda_x, \lambda_y \ge 0$ mit 
		\begin{align*}
			\varphi_{\overline{x}}(x) &\ge \varphi \enbrace*{x + \lambda_x \cdot \overline{x}} - \lambda_x \cdot \varphi(\overline{x}) - \varepsilon \\
			\varphi_{\overline{x}}(y) &\ge \varphi \enbrace*{y + \lambda_y \cdot \overline{x}} - \lambda_y \cdot \varphi(\overline{x}) - \varepsilon 
		\end{align*}
		Setze $\lambda  := \lambda_x + \lambda_y$, dann gilt
		\begin{align*}
			\varphi_{\overline{x}}(x) + \varphi_{\overline{x}}(y) &\ge \varphi \enbrace*{ x+ \lambda_x \cdot \overline{x}} + 
			\varphi \enbrace*{y+ \lambda_y \cdot \overline{x} } - \lambda  \cdot \varphi(\overline{x}) - 2 \varepsilon \\
			&\ge  \varphi \enbrace*{x +y + \lambda \cdot \overline{x}} - \lambda  \cdot \varphi(\overline{x}) - 2 \varepsilon \\
			&\ge \varphi_{\overline{x}} (x+y) - 2 \varepsilon \tag*{(nach Definition)}
		\end{align*}
		Da $\varepsilon>0$ beliebig war, gilt $\varphi_{\overline{x}}(x) + \varphi_{\overline{x}}(y) \ge \varphi_{\overline{x}}(x+y)$. Also ist $\varphi_{\overline{x}}$
		sublinear.
	\end{enumerate}
	$\varphi_{\overline{x}} \le \varphi$ ist klar mit $\lambda=0$. Da $\varphi$ minimal ist, folgt $\varphi_{\overline{x}} = \varphi$. Wir erhalten 
	\[
		\varphi(x) + 1 \cdot \varphi(\overline{x}) \le \varphi(x+ 1 \cdot \overline{x}) \stackrel{\text{sublinear}}{\le} \varphi(x) + 1 \cdot \varphi(\overline{x})
	\]
	für jedes $x, \overline{x}\in X$. Also ist $\varphi$ additiv und somit linear. \bewende
\end{description}
% subsection 220 (end)

\subsection{Satz von Hahn-Banach} % (fold)
\label{sub:221}
Sei $X$ ein $\mathds{R}$-Vektorraum und $\varphi \in \mathcal{S}(X)$.\marginnote{$X$ ist nicht zwangsweise endlich-dimensional!}
Dann existiert $\psi \colon X \to \mathds{R}$ linear mit $\psi \le \varphi$.
\minisec{Beweis}
Sei $\mathcal{S}_\varphi : =\set[\varphi' \in \mathcal{S}(X)]{\varphi' \le \varphi} \ni \varphi$. Nach Proposition \ref{sub:219} ist $\mathcal{S}(X)$ und damit auch 
$\mathcal{S}_\varphi$ nach unten induktiv geordnet. Nach dem Lemma von Zorn enthält $\mathcal{S}_\varphi$ ein minimales Element $\psi$. $\psi$ ist auch minimal in 
$\mathcal{S}(X)$ (Warum?).
Also ist $\psi$ linear nach Proposition \ref{sub:220}. \bewende \medskip \\
Wichtige Folgerungen: Fortsetzungs- und Trennungssätze.
% subsection 221 (end)

\subsection[Satz (Hahn-Banach): Existenz einer linearen Fortsetzung (mit sublinearer Schranke)]{Satz} % (fold)
\label{sub:222}
Sei $X$ ein $\mathds{R}$-Vektorraum und $\varphi \colon X \to \mathds{R}$ sublinear. Sei $Y \subset X$ ein linearer Unterraum und $\psi \colon Y \to \mathds{R}$ linear mit
$\psi \le \varphi\big|_{Y}$. Dann existiert $\overline{\psi} \colon X \to \mathds{R}$ linear mit $\overline{\psi}\big|_Y = \psi$ und $\overline{\psi} \le \varphi$.
\minisec{Beweis}
Definiere $\tilde{\varphi} \colon X \to \mathds{R}$ durch $\tilde{\varphi}(x) := \inf_{y \in Y} \enbrace[\big]{ \varphi(x-y) + \psi(y)}$, $x \in X$. Es gilt
$0 \le \varphi(x +y - x-y) \le \varphi(x-y) + \varphi(-x) + \varphi(y)$, womit folgt
\[
	\varphi(x-y) + \psi(y) \ge \varphi(-y) - \varphi(-x) - \psi(-y) \ge - \varphi(-x) > -\infty \qquad \forall x \in X, y \in Y
\]
Also ist $\tilde{\varphi}$ wohldefiniert. Analog zu \ref{sub:220} folgt, dass $\tilde{\varphi}$ sublinear ist. Nach dem Satz von Hahn-Banach (\ref{sub:221}) existiert ein 
$\overline{\psi} \colon X \to \mathds{R}$ linear mit $\overline{\psi} \le \tilde{\varphi} \le \varphi$. Es bleibt nur $\overline{\psi}|_Y=\psi$ zu zeigen.

Für $y_0 \in Y$ gilt $\tilde{\varphi}(y_0) = \inf_{y \in Y}\enbrace[\big]{\varphi(y_0-y) + \psi(y)}$ und somit folgt $\tilde{\varphi}|_{Y} \le \psi$ mittels $y=y_0$. Weiter 
folgt aus \ref{sub:220}, dass $\psi$ minimal in $\mathcal{S}(Y)$ ist. Also muss insgesamt gelten:
\[
	\overline{\psi}\big|_Y \le \tilde{\varphi}\big|_{Y} \le \psi \quad \Longrightarrow \quad \psi = \overline{\psi}\big|_{Y} \bewende  
\]
% subsection 222 (end)

\subsection[Satz (Hahn-Banach): Existenz einer linearen Fortsetzung (mit Halbnorm)]{Satz} % (fold)
\label{sub:223}
Sei nun $X$ ein $\mathds{K}$-Vektorraum und $p \colon X \to \mathds{R}$ eine Halbnorm. Sei $Y \subset X$ ein Untervektorraum und $\psi \colon Y \to \mathds{K}$ linear mit
$\abs*{\psi(y)} \le p(y)$ für $y \in Y$. Dann existiert $\overline{\psi} \colon X \to \mathds{K}$ linear mit $\overline{\psi}\big|_{Y} = \psi$ und 
\[
	\abs*{\overline{\psi}(x) } \le p(x) \quad \text{ für } x \in X 
\]
\minisec{Beweis}
Sei zunächst $\mathds{K}=\mathds{R}$. $p$ ist sublinear und es gilt $\psi \le p|_{Y}$. Nach \ref{sub:222} existiert eine lineare Fortsetzung 
$\overline{\psi} \colon X \to \mathds{R}$ mit $\overline{\psi} \le p$. Es gilt auch
\begin{align*}
	- \overline{\psi}(x) =  \overline{\psi}(-x) \le p(-x)  = p(x) \quad \Longrightarrow \quad \abs*{\overline{\psi}(x) } \le p(x) \quad \text{ für } x \in X
\end{align*}
Sei nun $\mathds{K}=\mathds{C}$. Definiere $\psi_1 := \re(\psi) \colon Y \to \mathds{R}$. Dann ist $\psi_1$ $\mathds{R}$-linear (warum?). Es gilt 
$\abs*{\psi_1(y)} \le \abs*{\psi(y)} \le p(y)$, $y \in Y$. Es existiert also ein $\mathds{R}$-lineares $\overline{\psi}_1 \colon X \to \mathds{R}$  mit
\[
	\overline{\psi}_1 (y) = \psi_1(y), \enspace y \in Y \quad \text{ und } \quad \abs*{\overline{\psi}_1(x)} \le p(x) , \enspace x \in X 
\]
Definiere jetzt $\overline{\psi} \colon X \to \mathds{C}$ durch $\overline{\psi}(x) := \overline{\psi}_1(x) - i \cdot \overline{\psi}_1(i \cdot x)$, $x \in X$.
Dann ist $\overline{\psi}$ $\mathds{C}$-linear (warum?). Weiter gilt $\overline{\psi}(y) = \psi(y)$ für $y \in Y$.
($\re(\overline{\psi}\big|_{Y})= \re(\psi)$ und $\overline{\psi}\big|_{Y}$ und $\psi$ sind beide $\mathds{C}$-linear).
Zu $x \in X$ wähle $\lambda \in \mathds{C}$ mit $\abs*{\lambda }=1$ und 
\[
	\abs*{\overline{\psi}(x)} = \lambda  \cdot \overline{\psi}(x) = \overline{\psi}(\lambda \cdot x) = \overline{\psi}_1(\lambda \cdot x) \le p(\lambda \cdot x)
	= \abs*{\lambda } \cdot p(x) = p(x) \bewende     
\]
% subsection 223 (end)

\subsection[Satz (Hahn-Banach): Existenz einer stetigen linearen Fortsetzung]{Satz} % (fold)
\label{sub:224}
Sei $X$ ein normierter $\mathds{K}$-Vektorraum, $Y \subset X$ ein Unterraum und $\psi \colon Y \to \mathds{K}$ linear und stetig. Dann existiert eine stetige, lineare 
Fortsetzung $\overline{\psi} \colon X \to \mathds{K}$ mit $\norm[\mathcal{L}(X,\mathds{K})]{\overline{\psi}} = \norm[\mathcal{L}(Y,\mathds{K})]{\psi}$.
\minisec{Beweis}
Definiere eine Halbnorm durch $p(x) := \norm{x} \cdot \norm{\psi}$. Wegen \ref{sub:27} gilt $\abs*{\psi(x)} \le p(x)$, also
besitzt $\psi$ nach Satz \ref{sub:223} eine lineare Fortsetzung $\overline{\psi}$ mit $\abs[\big]{\overline{\psi}(x)} \le p(x)= \norm{\psi} \cdot \norm{x}$, $x \in X$. 
Es folgt $\norm{\overline{\psi}} \le \norm{\psi}$. $\norm{\psi} \le \norm{\overline{\psi}}$ ist trivial, da $\psi|_{Y} = \overline{\psi}$. \bewende
% subsection 224 (end)

\subsection[Definition: Konvexe Teilmenge eines $\mathds{K}$-Vektorraums, konvexe Hülle]{Definition} % (fold)
\label{sub:225}
Sei $X$ ein $\mathds{K}$-Vektorraum. Sei $M \subset X$ eine Teilmenge. $M$ heißt \Index{konvex}, falls für $a,b \in M$, $\lambda \in [0,1]$ gilt
\[
	(1-\lambda ) \cdot a + \lambda \cdot b \in M
\]
Ist $X$ ein topologischer Vektorraum und $M \subset X$ konvex, so ist auch $\overline{M}$ konvex.\footnote{für normierte Vektorräume siehe Anhang \ref{sub:ab_konvex}}
Ist $N \subset X$ eine beliebige Teilmenge, so definieren wir die \Index{konvexe Hülle} von $N$ durch  
\[
	\conv(N) := \smashoperator{\bigcap_{\substack{N \subset M \subset X \\ M \text{ konvex} }}} \, M
\]
$\conv(N)$ ist konvex. (warum?)
% subsection 225 (end)

\subsection[Satz (Hahn-Banach): Existenz einer linearen Fortsetzung (mit konvexer Teilmenge)]{Satz} % (fold)
\label{sub:226}
Sei $X$ ein $\mathds{R}$-Vektorraum, $\emptyset \not= M \subset X$ konvex, $\varphi \in \mathcal{S}(X)$. Dann existiert $\psi \colon X \to \mathds{R}$ linear mit 
$\psi \le \varphi$ und 
\begin{equation*}
	\inf_{y \in M} \varphi(y) = \inf_{y \in M} \psi(y) \label{eq:226:1}\tag{*}
\end{equation*}
\minisec{Beweis}
Setze $\mu := \inf_{y \in M} \varphi(y)$. Falls $\mu = -\infty$, so folgt die Behauptung aus dem Satz von Hahn-Banach \ref{sub:221}. $\eqref{eq:226:1}$ ist dann 
trivialerweise erfüllt. Sei also $\mu \in \mathds{R}$. Definiere $\tilde{\varphi} \colon X \to \mathds{R}$ durch
\[
	\tilde{\varphi}(x) := \inf_{y \in M, \lambda  \ge 0} \enbrace[\big]{ \varphi(x+ \lambda \cdot y) - \lambda  \cdot \mu} , \quad x \in X
\]
Es gilt $\varphi(x + \lambda \cdot y)- \lambda \cdot \mu \ge - \varphi(-x)$ für $x \in X$, $y\in M$, $\lambda \ge 0$ (siehe \ref{sub:220}). Also ist 
$\tilde{\varphi}(x) \ge - \varphi(-x) >-\infty$ und $\tilde{\varphi}$ somit wohldefiniert. $\tilde{\varphi}$ ist sublinear:
\begin{enumerate}[(i)]
	\item $\tilde{\varphi}(\gamma \cdot x) = \gamma \cdot \tilde{\varphi}(x)$, für $x \in X$, $\gamma \in \ge 0$, wie in \ref{sub:222}.
	\item Seien $x,z \in X$ und $\varepsilon>0$. Wähle $y_x, y_z \in M$, $\lambda_x, \lambda_z \ge 0$ mit
	\[
		\tilde{\varphi}(x) \ge \varphi (x +\lambda_x \cdot y_x) - \lambda_x \cdot \mu -\varepsilon \quad \text{ und }\quad 
		\tilde{\varphi}(x) \ge \varphi (z +\lambda_z \cdot y_z) - \lambda_z \cdot \mu -\varepsilon
	\] 
	Es folgt 
	\begin{align*}
		\tilde{\varphi}(x) + \tilde{\varphi}(z) &\ge \varphi \enbrace[\big]{x+z+ \lambda_x \cdot y_x + \lambda_z \cdot y_z} - (\lambda_x + \lambda_z) \cdot \mu 
		- 2 \varepsilon \\
		&= \varphi \enbrace[\Bigg]{x+z + (\lambda_x+ \lambda_z)\cdot \underbrace{\enbrace*{\frac{\lambda_x}{\lambda_x + \lambda_z} \cdot y_x + \frac{\lambda_z}{\lambda_x + \lambda_z} \cdot y_z }}_{\in M} } - (\lambda_x + \lambda_z) \cdot \mu -2 \varepsilon \\
		&\ge \tilde{\varphi}(x+z) - 2 \varepsilon
	\end{align*}
	Da $\varepsilon>0$ beliebig war, gilt $\tilde{\varphi}(x) + \tilde{\varphi}(z) \ge \tilde{\varphi}(x+z)$ für alle $x,z \in X$.
\end{enumerate}
Nach Hahn-Banach (\ref{sub:221}) existiert $\psi \colon X \to \mathds{R}$ linear mit $\psi \le \tilde{\varphi} \le \varphi$. Für $y \in M$ gilt 
\[
	-\psi(y) = \psi(-y) \le \tilde{\varphi}(-y) \le \varphi(-y + 1 \cdot y)- 1 \cdot \mu= -\mu
\]
also $\mu \le \psi(y) \le \varphi(y)$, woraus \eqref{eq:226:1} folgt. \bewende
% subsection 226 (end)

\subsection[Satz (Hahn-Banach): Existenz einer linearen Fortsetzung (zwei konvexe Teilmengen)]{Satz} % (fold)
\label{sub:227}
Sei $X$ ein normierter $\mathds{R}$-Vektorraum und $A,B \subset X$ nichtleere konvexe Teilmengen mit \index{Distanz $\dist$}
\[
	\dist(A,B) := \inf \set[\norm{a-b}]{a \in A, b \in B} > 0
\]
Dann existiert $\psi \colon X \to \mathds{R}$ stetig und linear mit $\psi(A) \cap \psi(B) = \emptyset$.
\minisec{Beweis}
$A - B := \set[a-b]{a \in A, b \in B} \subset X$ ist konvex. (Warum?) Nach Satz \ref{sub:226} existiert, da Normen sublinear sind, $\psi \colon X \to \mathds{R}$ linear mit 
$\psi \le \norm{.}$, und 
\[
	0 < \dist(A,B)=  \inf_{y \in A -B} \norm{y} = \inf_{y \in A-B} \psi(y) = \inf_{a \in A} \psi(a) - \sup_{b \in B} \psi(b). \bewende
\]
% subsection 227 (end)
% section 2 (end)
\newpage

\section[Operatoren zwischen Banachräumen, Satz von der offenen Abbildung]{Operatoren zwischen Banachräumen. Die Sätze von der offenen Abbildung und vom abgeschlossenen Graphen} % (fold)
\label{sec:3}

\subsection[Proposition: Wenn $Y$ vollständig ist, dann ist $\mathcal{L}(X,Y)$ auch vollständig]{Proposition} % (fold)
\label{sub:31}
Seien $X,Y$ normierte Vektorräume, $Y$ vollständig. Dann ist $\mathcal{L}(X,Y)$ vollständig bezüglich $\norm[\mathcal{L}(X,Y)]{.}$.
\minisec{Beweis}
Sei $(T_n)_{n \in \mathds{N}} \subseteq \mathcal{L}(X,Y)$ eine Cauchy-Folge bezüglich $\norm[\mathcal{L}(X,Y)]{.}$. Für $x \in X$ ist dann auch 
$(T_n x)_{n \in \mathds{N}} \subset Y$ Cauchy bezüglich $\norm[Y]{.}$ (Warum?)\footnote{mit $\norm[Y]{T_n x- T_m x} \le \norm{T_n -T_m} \cdot \norm{x}$}.
Da $Y$ ein Banachraum ist, folgt $T_n x \to y$ für ein $y \in Y$. Wir definieren $T \colon X \to Y$ durch 
\[
	T(x) := \lim_{ n \to \infty} T_n(x)
\]
für $x \in X$. $T$ ist linear: Klar, da die $T_n$ linear sind. $T$ ist stetig: $(T_n)_{n \in \mathds{N}}$ ist Cauchy, also auch 
$\enbrace*{\norm{T_n}}_{n \in \mathds{N}} \subset \mathds{R}$ eine Cauchyfolge, da wegen der umgekehrten Dreiecksungleich gilt
\[
	\abs[\big]{\norm{T_n} - \norm{T_m}} \le \norm{T_n -T_m} \le \varepsilon 
\]%
% \footnote{mit $\norm{T_n} = \norm{T_n - T_{n_0}+ T_{n_0}} \le \norm{T_n- T_{n_0}} + \norm{T_{n_0}}$} 
Dann existiert $C \ge 0$ mit $\norm[\mathcal{L}(X,Y)]{T_n} \le C $ für alle $n \in \mathds{N}$, also
gilt $\norm[Y]{T x} \le C \cdot \norm{x}$, $x \in X$, woraus $\norm[\mathcal{L}(X,Y)]{T} \le C$ und die Stetigkeit von $T$ folgt. Also ist $T \in \mathcal{L}(X,Y)$.

Wir müssen nun noch zeigen, dass $(T_n)_n \to T$ in der Operatornorm gilt. Da $(T_n)_{n \in \mathds{N}}$ eine Cauchyfolge ist, existiert für $\varepsilon >0$ ein
$n_0 \in \mathds{N}$ mit $\norm[\mathcal{L}(X,Y)]{T_n - T_m}  < \frac{\varepsilon}{2} $ für $n,m \ge n_0$. Insbesondere gilt
\begin{align*}
	\norm[Y]{(T_n- T_m)(x)} \le \norm[\mathcal{L}(X,Y)]{T_n -T_m} \cdot \norm[X]{x} < \frac{\varepsilon}{2}  \cdot \norm[X]{x}    
\end{align*}
für $n,m \ge n_0$, $x \in X$. Weiter gilt 
\begin{align*}
	\norm[Y]{(T_n- T)(x)} \le \norm[Y]{(T_n - T_m)(x)} + \norm[Y]{(T_m-T)(x)} &\le \norm{T_n - T_m} \cdot \norm[X]{x} + \norm[Y]{(T_m-T)x} \\
	&\le \frac{\varepsilon}{2}  \cdot \norm[X]{x} + \frac{\varepsilon}{2}  \cdot \norm[X]{x}  = \varepsilon \cdot \norm[X]{x} 
\end{align*}
für $n,m \ge n_0$ und $m$ groß genug, $x \in X$. Damit folgt nun
$\norm[\mathcal{L}(X,Y)]{T_n -T} \le \varepsilon$ für $n \ge n_0$, also $T_n \to T$ bezüglich $\norm[\mathcal{L}(X,Y)]{.}$. \bewende
% subsection 31 (end)

\subsection[Corollar: Dualraum vollständig und $\mathcal{L}(X,X)$ ist Banachalgebra, falls $X$ Banachraum]{Corollar} % (fold)
\label{sub:32}
\begin{enumerate}[(i)]
	\item Der Dualraum $X^*$ eines normierten $\mathds{K}$-Vektorraumes $X$ ist vollständig. 
	\item Falls $\set{0} \not= X$ ein Banachraum ist, so ist $\mathcal{L}(X,X)$ eine Banachalgebra.
\end{enumerate}
\minisec{Beweis}
\begin{enumerate}[(i)]
	\item $X^*=\mathcal{L}(X,\mathds{K})$ und $\mathds{K}$ ist vollständig. Wende \ref{sub:31} an.
	\item $\mathcal{L}(X,X)$ ist ein Banachraum nach \ref{sub:31}. $\mathcal{L}(X,X)$ ist eine normierte Algebra: 
	\begin{itemize}
		\item Es gilt $\norm[\mathcal{L}(X,X)]{\id_X}=1$
		\item Für $S,T \in \mathcal{L}(X,X)$, $x \in X$ gilt
		\begin{align*}
			\norm[X]{(S \circ T)(x)} \le \norm{S} \cdot \norm[X]{T x} \le \norm{S} \cdot \norm{T} \cdot \norm[X]{x}   
		\end{align*}
		Also ist $\norm{S \circ T} \le \norm{S} \cdot \norm{T}$. \bewende
	\end{itemize}
\end{enumerate}
% subsection 32 (end)

\subsection[Definition und Proposition: Lineare Isometrie $\iota_X \colon X \to X^{**}$ (kanonische Inklusion)]{Definition und Proposition} % (fold)
\label{sub:33}
Sei $X$ ein normierter Raum. Definiere $\iota_X \colon X \to X^{**} \enspace{\color{light_gray}\enbrace[\big]{=(X^*)^*}}$ durch
\[
	x \longmapsto \enbrace[\big]{\varphi \mapsto \varphi(x) } 
\]
für $x \in X$. Die Abbildung $\iota_X$ ist eine lineare Isometrie.\index{kanonische Inklusion}
\minisec{Beweis}
\begin{description}
	\item[Wohldefiniertheit:] Die Linearität von $\iota_X(x)$ ist klar. Mit
	\begin{align*}
		\abs*{\iota_X(x)(\varphi)} = \abs*{\varphi(x)} \le \norm[X^*]{\varphi} \cdot \norm[X]{x} = \norm[X]{x} \cdot \norm[X^*]{\varphi}      
	\end{align*}
	folgt $\norm[X^{**}]{\iota_X(x)} \le \norm[X]{x}$, also ist $\iota_X(x)$ stetig.
	\item[$\iota_X$ ist linear:] Für $\alpha, \beta \in \mathds{K}$, $x,y \in X$, $\varphi \in \mathcal{L}(X,\mathds{K})= X^*$ gilt
	\begin{align*}
		\iota_X(\alpha \cdot x + \beta \cdot y)(\varphi)= \varphi(\alpha \cdot x + \beta \cdot y) = \alpha \cdot \varphi(x) + \beta \cdot \varphi(y) 
		&= \alpha \cdot \iota_X(x)(\varphi) + \beta \cdot \iota_X(y)(\varphi) \\
		&= \enbrace[\big]{\alpha \cdot \iota_X(x) + \beta \cdot \iota_X(y)}(\varphi) 
	\end{align*}
	\item[Isometrie:] Es bleibt zu zeigen: $\norm[X^{**}]{\iota_X(x)} \ge \norm[X]{x}$ für $0 \not= x \in X$. Sei $Y := \mathds{K}\cdot x$ und 
	$\psi \colon Y \to \mathds{K}$ gegeben durch $\psi(\alpha \cdot x) := \alpha \cdot \norm[X]{x}$. Dann ist $Y \subset X$ ein linearer Unterraum und 
	$\psi$ linear mit $\norm{\psi}=1$. Nach Satz \ref{sub:224} existiert nun $\overline{\psi} \colon X \to \mathds{K}$ linear mit 
	$\overline{\psi}(x) = \psi(x) = \norm[X]{x}$ und $\norm{\overline{\psi}}= \norm{\psi} = 1$. Es gilt 
	\begin{align*}
		\norm[X^{**}]{\iota_X(x)} \ge \abs*{\iota_X(x)(\overline{\psi})} = \abs*{\overline{\psi}(x)} = \norm[X]{x} \bewende
	\end{align*}
\end{description}
% subsection 33 (end)

\subsection[Definition und Proposition: Die transponierte Abbildung]{Definition und Proposition} % (fold)
\label{sub:34}
Seien $X,Y$ normierte Räume. Definiere eine Abbildung $.^{\tr} \colon \mathcal{L}(X,Y) \to \mathcal{L}(Y^*,X^*)$ durch $T \mapsto T^{\tr} = (\varphi \mapsto \varphi \circ T)$.
$.^\tr$ ist eine lineare Isometrie und das Diagramm\index{transponierte Abbildung}
\[
	\begin{tikzcd}
		X \rar["\iota_X"] \dar["T"]& X^{**} \dar["T^{\tr \, \tr}= (T^\tr)^\tr"]\\
		Y \rar["\iota_Y"]& Y^{**}
	\end{tikzcd}
\]
kommutiert für jedes $T \in \mathcal{L}(X,Y)$. Für $\iota_{X^*} \colon X^* \to X^{***}$ und $\iota_{X}^\tr \colon X^{***} \to X^*$ gilt 
\[
	\iota_X^\tr \circ \iota_{X^*} = \id_{X^*}
\]
\minisec{Beweis}
\begin{enumerate}[(i)]
	\item Für $T,S \in \mathcal{L}(X,Y)$, $\alpha, \beta \in \mathds{K}$, $\varphi \in Y^*$ gilt
	\begin{align*}
		\enbrace*{\alpha \cdot T + \beta \cdot S}^\tr (\varphi) = \varphi \circ \enbrace*{\alpha \cdot T + \beta \cdot S} \!\!\stackrel{\text{linear}}{=} \!\!
		\alpha \cdot (\varphi \circ T) + \beta \cdot (\varphi \circ S)  &= \alpha \cdot T^\tr(\varphi) + \beta \cdot S^\tr(\varphi)\\
		&= \enbrace*{\alpha \cdot T^\tr + \beta \cdot S^\tr}(\varphi) 
	\end{align*}
	Damit ist $.^\tr$ eine lineare Abbildung.
	\item Für $x \in X$, $\psi \in Y^*$ gilt $\enbrace[\big]{\iota_Y \circ T(x)}(\psi) = \psi \enbrace[\big]{T(x)}$. Weiter gilt dann
	\begin{align*}
		\enbrace[\big]{T^{\tr \, \tr} \circ \iota_X(x)} (\psi) = \enbrace[\big]{(T^\tr)^\tr \circ \iota_X(x)}(\psi)  = \enbrace[\big]{\iota_X(x) \circ  T^\tr}(\psi)
		&= \iota_X(x) \enbrace*{T^\tr(\psi)}  \\
		&= T^\tr(\psi)(x) = \psi \circ T(x) \\
		&= \iota_Y \enbrace[\big]{T(x)} (\psi) 
	\end{align*}
	$\Rightarrow T^{\tr \, \tr} \iota_X(x) = \iota_Y \enbrace[\big]{T(x)} = \iota_Y \circ T (x)$. Also $T^{\tr \, \tr} \circ \iota_X = \iota_Y  \circ T$ und 
	das Diagramm kommutiert.
	\item Es ist $\abs{(T^\tr \psi)(x)} = \abs*{\psi \circ T(x)} \le \norm[Y^*]{\psi} \cdot \norm[Y]{T x} \le \norm[Y^*]{\psi} \cdot \norm[\mathcal{L}(X,Y)]{T} \cdot \norm[X]{x}$. Also
	ist $\norm[X^*]{T^\tr \psi} \le  \norm[Y^*]{\psi} \cdot  \norm[\mathcal{L}(X,Y)]{T}$ und damit folgt 
	\[
		\norm[\mathcal{L}(Y^*,X^*)]{T^\tr} \le \norm[\mathcal{L}(X,Y)]{T}  
	\]
	Ebenso ist $\norm[\mathcal{L}(X^{**},Y^{**})]{T^{\tr \, \tr}} \le \norm[\mathcal{L}(Y^*,X^*)]{T^\tr} \le \norm[\mathcal{L}(X,Y)]{T}$. Andererseits gilt 
	$\norm[\mathcal{L}(X,Y)]{T} \le \norm[\mathcal{L}(X^{**},Y^{**})]{T^{\tr\, \tr}}$ nach \ref{sub:33} und da das Diagramm kommutiert (Warum?). 
	\[
		\Rightarrow \norm[\mathcal{L}(X,Y)]{T} \le \norm[\mathcal{L}(Y^*,X^*)]{T^\tr}  \le  \norm[\mathcal{L}(X,Y)]{T} 
	\]
	also ist $.^\tr$ eine Isometrie.
	\item Für $\varphi \in X^*$, $x \in X$ gilt
	\begin{align*}
		\enbrace[\big]{\iota_X^\tr \circ \iota_{X^*} (\varphi)} (x) = (\iota_{X^*}(\varphi) \circ \iota_X)(x) = \iota_X(x)(\varphi) = \varphi(x)
	\end{align*}
	$\Rightarrow \iota_X^\tr \circ \iota_{X^*}(\varphi)= \varphi$, also erhalten wir $\iota_X^\tr \circ  \iota_{X^*} = \id_{X^*}$. \bewende 
\end{enumerate}
% subsection 34 (end)

\subsection{Satz: Prinzip der gleichmäßigen Beschränktheit für Banachräume} % (fold)
\label{sub:35}
Sei $X$ ein Banachraum und $Y$ ein normierter Raum. Sei $M \subset \mathcal{L}(X,Y)$, sodass die Menge\marginnote{Vergleiche \ref{sub:112}, diese Aussage ist 
deutlich stärker! Dies verdanken wir in erster Linie der Linearität der betrachteten Abbildungen.} 
\[
	\set[{x \mapsto \norm[Y]{T x}}]{T \in M \rule{0cm}{1em}} \subset C(X,\mathds{R} )
\]
\Index{punktweise gleichmäßig beschränkt} ist, d.h. für $x \in X$ existiert $C_x \ge 0$ mit $\norm[Y]{T x} \le C_x$, $T \in M$. Dann existiert $C \ge 0$ mit 
$\norm[\mathcal{L}(X,Y)]{T} \le C$, $T \in M$.
\minisec{Beweis}
Nach Corollar \ref{sub:112} (Prinzip der gleichmäßigen Beschränktheit für metrische Räume) existieren eine offene Kugel $\emptyset \not= B(x_0,\varepsilon)\subset X$ und 
$K \ge 0$ mit $\norm[Y]{T x} \le K$ für $x \in B(x_0, \varepsilon)$, $T \in M$. Für $x \in X$ mit $\norm{x} \le 1$ gilt 
\begin{align*}
	\norm[Y]{T x} = \frac{2}{\varepsilon} \cdot  \norm[Y]{T \enbrace*{\frac{\varepsilon}{2} \cdot x}} &= \frac{2}{\varepsilon} \cdot 
	\norm[Y]{T \enbrace*{\frac{\varepsilon}{2} \cdot x + x_0 - x_0 } }  \\
	&\le \frac{2}{\varepsilon} \cdot   \enbrace*{ \doppelstrich[\Bigg]{T \enbrace[\Bigg]{\underbrace{\frac{\varepsilon}{2} \cdot x + x_0}_{\in B(x_0,\varepsilon)} } }_Y +
	 \doppelstrich[\Bigg]{T  \enbrace[\bigg]{\underbrace{x_0}_{\in B(x_0,\varepsilon)}}}_Y} \\
	&\le \frac{2}{\varepsilon} (K+K) = \frac{4K}{\varepsilon}   
\end{align*}
Es folgt $\norm[\mathcal{L}(X,Y)]{T} \le \frac{4 K}{\varepsilon} =: C$, also haben wir die gesuchte Konstante gefunden. \bewende
% subsection 35 (end)

\subsection[Corollar über Beschränktheit einer Teilmenge eines normierten Raumes]{Corollar} % (fold)
\label{sub:36}
Sei $Z$ ein normierter Raum und $N \subset Z$ eine Teilmenge, so dass gilt: Für alle $\varphi \in Z^*$ existiert $C_\varphi \ge 0$ mit $\abs*{\varphi(z)} \le C_\varphi$, 
$z \in N$.  Dann ist $N$ beschränkt.
\minisec{Beweis}
$\iota_Z \colon Z \to Z^{**}$ ist eine Isometrie nach \ref{sub:33}. Wende Satz \ref{sub:35} an mit 
\begin{align*}
	X &=Z^{*} \\
	Y &=\mathds{K} \\
	M &= \iota_Z(N) \subset Z^{**}= \mathcal{L}(Z^*,\mathds{K})
\end{align*}
Für $\varphi \in X = Z^{*}$ existiert $C_\varphi \ge 0$ mit $\abs*{T \varphi} = \abs*{\iota_Z(z) (\varphi)} = \abs*{\varphi(z)} \le C_\varphi$, wo $T= \iota_Z(z) \in M$
für ein $z \in N$. Nach \ref{sub:35} folgt: Es existiert ein $C \ge 0$ mit $\norm[Z]{z} = \norm[Z^{**}]{\iota_Z(z)}  \le C$, $z \in N$. \bewende
% subsection 36 (end)

\subsection[Corollar über Beschränktheit einer Teilmenge von $\mathcal{L}(X,Z)$]{Corollar} % (fold)
\label{sub:37}
Sei $X$ ein Banachraum und $Z$ ein normierter Raum. $M \subset \mathcal{L}(X,Z)$ eine Teilmenge, so dass gilt: Für alle $\varphi \in Z^*$ und $x \in X$ existiert $C_{x,\varphi} \ge 0$
mit
\[
	\abs*{\varphi \enbrace[\big]{Tx}} \le C_{x,\varphi} \enspace, \qquad T \in M
\]
Dann ist $M$ beschränkt.
\minisec{Beweis}
Für jedes $x \in X$ folgt aus Corollar \ref{sub:36} mit $N := \set[T(x)]{T \in M}$, dass $N$ beschränkt ist. Es folgt, dass $ \set[x \mapsto \norm{T(x)} ]{T \in M}$ 
punktweise gleichmäßig beschränkt ist. Mit Satz \ref{sub:35} folgt: $M$ ist beschränkt. \bewende
% subsection 37 (end)

\subsection{Satz: Prinzip der offenen Abbildung} % (fold)
\label{sub:38}
Seien $X,Y$ Banachräume, $T \in \mathcal{L}(X,Y)$ surjektiv. Dann ist $T$ offen, d.h. für eine offene Menge $U \subset X$ ist $T(U) \subset Y$ offen.
\minisec{Beweis}
\begin{enumerate}[(i)]
	\item Sei $0 \in W \subset X$ offen, dann existiert $\emptyset \not= V \subset Y$ offen mit $V \subset \overline{T(W)}$: 
	
	$0 \in W \subset X$ $\Rightarrow $ $B(0,\varepsilon) \subset W$ für ein $\varepsilon>0$. Dann gilt
	\[
		X=\bigcup_{n \in \mathds{N}} n \cdot B(0,\varepsilon) = \bigcup_{n \in \mathds{N}} n \cdot W
	\]
	Aus der Surjektivität von $T$ folgt $Y = \bigcup_{n \in \mathds{N} }n \cdot T(W) = \bigcup_{n \in \mathds{N}} n \cdot \overline{T(W)}$. Nach dem Satz von Baire 
	(\ref{sub:110}) folgt: Ein $n \cdot \overline{T(W)}$ enthält eine nichtleere offene Teilmenge, also auch $\overline{T(W)}$.
	\item Sei $0 \in M \subset X$ offen, dann ist $0$ ein innerer Punkt von $\overline{T(M)}$: 
	
	Die Abbildung $X \times X \to X$, $(x_1,x_2) \mapsto x_1 -x_2$ ist stetig.\footnote{Banachräume sind topologische Vektorräume} $\Rightarrow$ es gibt $0 \in W \subset X$ 
	offen mit\marginnote{mit $W$ als Urbild einer offenen Umgebung der $0$ in $M$}
	\[
		W - W = \set[x_1 -x_2]{x_1,x_2 \in W} \subset M 
	\]
	Es folgt 
	\[
		\overline{T(M)} \supset \overline{T(W-W)} = \overline{T(W)- T(W)} \stackrel{\text{(Warum?)}}{\supset} \overline{T(W)}- \overline{T(W)} \stackrel{\text{(i)}}{\supset}
		V -V
	\]
	für $\emptyset \not= V$ offen in $Y$ wie in (i). Aber $0 \in V-V = \bigcup_{y \in V}\set{y}- V$ ist offen in $Y$. Also ist $0$ ein innerer Punkt von $\overline{T(M)}$.
	\item Ist $0 \in N \subset X$ offen, so existiert $0 \in Z \subset Y$ offen mit $Z \subset T(N)$:
	
	Sei $\varepsilon_0 >0$. Wähle $\varepsilon_i >0$, $1 \le i \in \mathds{N}$ mit $\sum_{i=1}^{\infty} \varepsilon_i < \varepsilon_0$. Nach (ii) gilt: Zu $\varepsilon_i$, 
	$i \in N$ existiert ein $\delta_i >0$ mit 
	\[
		B_Y(0,\delta_i) \subset \overline{T \enbrace*{B_X(0,\varepsilon_i)}}. 
	\]
	Es gilt $\delta_i \xrightarrow{i \to \infty} 0$. Sei 
	nun $y \in B_Y(0,\delta_0) =: Z$. Dann gilt $y \in \overline{T \enbrace*{B_X(0,\varepsilon_0)}}$, also existiert $x_0 \in B_X(0,\varepsilon_0)$ mit 
	$\norm[Y]{y- T(x_0)}< \delta_1$. Folglich existiert $x_1 \in B_X(0,\varepsilon_1)$ mit $\norm[Y]{y- T(x_0) - T(x_1)} < \delta_2$. Per Induktion folgt
	\[
		\exists x_i \in B_X(0,\varepsilon_i) \text{ mit } \norm[Y]{y- \sum\nolimits_{j=0}^{i}T(x_i)} < \delta_{i+1}, \qquad i \in \mathds{N} 
	\]
	$\sum_{i \in \mathds{N}}x_i$ konvergiert in $X$ und $\norm[X]{\sum_{i\in \mathds{N}} x_i} < 2 \cdot \varepsilon_0 $, es gilt $T \enbrace*{\sum_{i \in \mathds{N}}x_i}=y$.
	Es folgt
	\[
		Z = B_Y(0,\delta_0) \subset {T \enbrace[\big]{B_X(0,2 \cdot \varepsilon_0)} } \subset T(N)
	\]
	% ($x_i \in N$, falls $B(0,2 \varepsilon_0) \subset N$)
	\item Sei $U \subset X$ offen. Dann ist $T(U) \subset Y$ offen:
	
	Zu $x \in U$ existiert $0 \in N \subset X$ offen mit $x+ N \subset U$, also $T(x)+ T(N)\subset T(U)$. Nach (iii) existiert $0 \in Z \subset Y$ offen mit $Z \subset T(N)$,
	also $T(x)+ Z \subset T(U)$. Aber $T(x) \in T(x)+ Z$ ist offen in $Y$ und $T(U)$ ist Umgebung von $T(x)$. $x \in U$ war beliebig, also ist $T(U)$ offen. \bewende
\end{enumerate}
% subsection 38 (end)

\subsection{Corollar: Satz von der inversen Abbildung} % (fold)
\label{sub:39}
Seien $X,Y$ Banachräume, $T \in \mathcal{L}(X,Y)$ bijektiv. Dann gilt $T ^{-1} \in \mathcal{L}(Y,X)$, d.h. $T$ ist ein Homöomorphismus\index{Homöomorphismus}.
\minisec{Beweis}
Nach Satz \ref{sub:38} ist $T$ offen und somit ist $T ^{-1}$ stetig. \bewende
% subsection 39 (end)

\subsection[Corollar: Gleichheit von Topologien von Banachräumen, wenn $\mathcal{T}_1 \subset \mathcal{T}_2$]{Corollar} % (fold)
\label{sub:310}
Sei $X$ ein Vektorraum und seien $\mathcal{T}_1, \mathcal{T}_2$ zwei Topologien auf $X$, die durch Banachraumnormen $\norm[1]{\cdot }$ und $\norm[2]{\cdot}$ induziert sind.
Falls $\mathcal{T}_1 \subset \mathcal{T}_2$, so gilt bereits $\mathcal{T}_1 = \mathcal{T}_2$.
\minisec{Beweis}
$\mathcal{T}_1 \subset \mathcal{T}_2$ $\Rightarrow $ $(X,\mathcal{T}_2) \xrightarrow{\id_X} (X,\mathcal{T}_1)$ ist stetig und bijektiv, also ist $\id_X$ nach 
\ref{sub:39} ein Homöomorphismus. \bewende
% subsection 310 (end)

\subsection[Corollar: Wenn $X= X_1 \oplus X_2$, so ist $X$ isomorph zu $X_1 \times X_2$]{Corollar} % (fold)
\label{sub:311}
Sei $X$ ein Banachraum mit abgeschlossenen Teilräumen $X_1,X_2$, sodass $X_1 \cap X_2 = \set{0}$ und $X= X_1 + X_2$. Dann sind $X$ und $X_1 \times X_2$ als topologische
Vektorräume isomorph.
\minisec{Beweis}
Die Abbildung $X_1 \times X_2 \to X$, $(x_1,x_2) \mapsto x_1 + x_2$ ist linear und bijektiv. Sie ist stetig, wegen 
\[
	\norm[X]{x_1 + x_2} \le \norm[X]{x_1} + \norm[X]{x_2} = \norm[1,X_1 \times X_2]{(x_1,x_2)} 
\]
Mit Corollar \ref{sub:39} folgt die Behauptung. \bewende
% subsection 311 (end)
% section 3 (end)
\newpage

\section{$L^p$-Räume und der Satz von Riesz-Fischer} % (fold)
\label{sec:4}

\subsection[Erinnerung: $L^1(\mu)$ ist ein Banachraum]{Erinnerung} % (fold)
\label{sub:41}
Sei $(X,\Sigma, \mu)$ ein \Index{Maßraum}.\footnote{$X$ Menge, $\Sigma$ eine $\sigma$-Algebra auf $X$, $\mu$ Maß} Eine Abbildung $f\colon X \to (-\infty,\infty]$ heißt 
\Index{messbar}, falls $f ^{-1}(E) \in \Sigma$ gilt für $E \subset \mathcal{B}(\mathds{R})$ (Borelmengen\footnote{$\sigma$-Algebra, die von den offenen Intervallen mit 
rationalen Endpunkten aufgespannt wird, siehe auch \url{https://de.wikipedia.org/wiki/Borelsche_σ-Algebra}}). Eine Abbildung $s\colon X \to (-\infty,\infty]$ heißt 
\Index{einfach}, falls $s(X)$ endlich ist. Für $f \colon X \to [0,\infty]$ messbar setzen wir
\[
	\int \! f  \, \mathd \mu := \sup \set[\int \! s \, \mathd \mu]{s \text{ einfach, messbar mit } 0 \le s \le f} 
\] 
Für $f\colon X \to (-\infty,\infty]$ messbar setze $\int\! f \, \mathd \mu := \int \! f_+ \, \mathd \mu - \int \! f_- \, \mathd \mu$, falls 
$\int \! f_- \, \mathd \mu < \infty$. Betrachte
\begin{align*}
	\mathcal{L}^1(\mu) := \set[f \colon X \to \mathds{R} \text{ messbar}]{\int \abs*{f}\,\mathd \mu < \infty } 
\end{align*}
mit der Halbnorm $\norm[1]{f} := \int \abs*{f} \, \mathd \mu $. Setze nun $\mathcal{N}^1 := \set[f\colon X\to\mathds{R} \text{ messbar}]{\int \abs*{f} \, \mathd \mu =0 }$.
$L^1(\mu)$ ist ein topologischer Vektorraum (Topologie induziert durch $\norm[1]{\cdot}$) und $\mathcal{N}^1 \subset \mathcal{L}^1(\mu)$ ist ein abgeschlossener 
Untervektorraum. Definiere 
\[
	L^1(\mu) := \nicefrac{\mathcal{L}^1(\mu)}{\mathcal{N}^1}
\]
$\norm[1]{\cdot }$ induziert eine wohldefinierte Norm auf $L^1(\mu)$ -- die wir auch wieder mit $\norm[1]{\cdot }$ bezeichnen -- via  
\[
	\norm[1]{f+ \mathcal{N}^1} := \norm[1]{f} \quad , \quad f \in \mathcal{L}^1(\mu) \tag*{(Übung)}
\]
$(L^1(\mu),\norm[1]{\cdot})$ ist ein Banachraum (siehe Analysis \RM{3}, 9.23).
% subsection 41 (end)

\subsection[Beispiele für $L^1$-Räume mit Lebesgue-Maß und Zählmaß]{Beispiele} % (fold)
\label{sub:42}
\begin{enumerate}[(i)]
	\item $L^1(\mathds{R}^n) := L^1(\mathds{R}^n, \Lambda, \lambda)$, wobei $\Lambda$ die Lebesgue-messbaren Mengen sind und $\lambda $ das Lebesgue-Maß 
	ist.\index{Lebesgue-Maß}
	\item $\ell^1(\mathds{N}) := L^1 \enbrace[\big]{\mathds{N}, \mathcal{P}(\mathds{N}), \delta} = \set[f\colon \mathds{N} \to \mathds{R}]{\sum \abs*{f(n)} < \infty } $, 
	wobei $\delta$ das Zählmaß\footnote{siehe auch \url{http://de.wikipedia.org/wiki/Zählmaß_(Maßtheorie)}}\index{Zählmaß} ist.
\end{enumerate}
% subsection 42 (end)

\subsection[Definition und Proposition: Der Raum $L^p(\mu)$]{Definition und Proposition} % (fold)
\label{sub:43}
Ähnlich wie oben definieren wir für einen Maßraum $(X,\Sigma,\mu)$
\begin{align*}
	\mathcal{L}^p(\mu) &:= \set[f \colon X \to \mathds{R} \text{ messbar}]{\int \abs*{f}^p \mathd \mu < \infty } \\
	\mathcal{N}^p &:= \set[f \colon X \to \mathds{R} \text{ messbar}]{\int \abs*{f}^p \mathd \mu = 0 } 
\end{align*}
$\mathcal{L}^p(\mu)$ und $\mathcal{N}^p$ sind Vektorräume. Wir setzen wieder
\[
	L^p(\mu) := \nicefrac{\mathcal{L}^p(\mu)}{\mathcal{N}^p}
\]
Definiere $\norm[p]{\cdot } \colon \mathcal{L}^p(\mu) \to [0,\infty)$ durch $\norm[p]{f} := \enbrace*{\int \abs*{f}^p \mathd \mu }^{1/p}$. $\norm[p]{\cdot}$ induziert wieder 
eine Abbildung $\norm[p]{\cdot } \colon L^p(\mu) \to [0,\infty) $ via $\norm[p]{f + \mathcal{N}^p} := \norm[p]{f} $ für $f \in \mathcal{L}^p$.
\minisec{Beweis}
\begin{description}
	\item[$\mathcal{L}^p(\mu)$ ist Vektorraum:]  Für $f,g \in \mathcal{L}^p(\mu)$ und $\alpha, \beta \in \mathds{R}$
	\begin{align*}
		\int \abs*{\alpha \cdot f + \beta \cdot g}^p \mathd \mu \le \int \!\enbrace[\big]{\abs*{\alpha \cdot f} + \abs*{\beta \cdot g}}^p \mathd \mu
		&\le \int \!\enbrace[\big]{2 \cdot \sup \set{\abs*{\alpha \cdot f}, \abs*{\beta \cdot g}  } }^p \mathd \mu \\ 
		&\le \int \enbrace*{2^p \cdot \sup \set{\abs*{\alpha \cdot f}^p, \abs*{\beta \cdot g}^p  \rule{0cm}{1em}} } \mathd \mu \\ 
		&\le \int \!2^p \cdot  \enbrace[\big]{\abs*{\alpha \cdot f}^p + \abs*{\beta \cdot g}^p} \mathd \mu < \infty 
	\end{align*}
	\item[$\mathcal{N}^p$ ist Vektorraum:] Wie obige Ungleichung, dann Linearität des Integrals anwenden.
	\item[{$\norm[p]{\cdot}$ wohldefiniert auf $L^p(\mu)$:}] Übung (Blatt 6, Aufgabe 3). \bewende
\end{description}
% subsection 43 (end)

\subsection[Proposition: Speziallfall der Youngschen Ungleichung]{Proposition} % (fold)
\label{sub:44}
Für $\alpha, \beta \ge 0$, $p,q >1$ mit $\frac{1}{p} + \frac{1}{q}=1$ gilt die folgende Ungleichung
\begin{equation*}
	\alpha \cdot \beta \le \frac{\alpha^p}{p} + \frac{\beta^q}{q}  \label{eq:44}\tag{\#}
\end{equation*}
Dies ist ein Spezialfall der Youngschen Ungleichung,  \hrefsym{https://de.wikipedia.org/wiki/Youngsche_Ungleichung_(Produkt)}{Wikipedia-Link}.\index{Young-Ungleichung}
\minisec{Beweis}
\begin{align*}
	\frac{1}{p} + \frac{1}{q}=1 \iff p+q= pq \iff (p-1)(q-1)= 1 \iff p (q-1) = q \iff q (p-1) =p
\end{align*}
Falls $\beta = \alpha^{p-1}$, so gilt $\alpha \cdot \beta = \alpha^p$ und 
\[
	\frac{\alpha^p}{p} + \frac{\beta^q}{q} = \frac{\alpha^p}{p} + \frac{(\alpha^{p-1})^q}{q} = \frac{\alpha^p}{p} + \frac{\alpha^p}{q} = \alpha^p = \alpha \cdot \beta   
\]
Also gilt in diesem Fall Gleichheit in \eqref{eq:44}. Für festes $\alpha \ge 0$ betrachte $f_\alpha \colon [0,\infty) \to [0,\infty)$ gegeben durch
$f_\alpha(\beta) := \frac{\alpha^p}{p} + \frac{\beta^q}{q} - \alpha \cdot \beta$. Dann ist $f_\alpha$ stetig auf $[0,\infty)$ und differenzierbar auf $(0,\infty)$. Es gilt
$f_\alpha'(\beta) = \beta^{q-1} -\alpha$ und weiter
\[
	f_\alpha'(\beta)=0 \iff \beta^{q-1} = \alpha \iff \beta= \alpha^{p-1}
\]
Es ist $f_\alpha''(\alpha^{p-1}) \ge 0$, also hat $f_\alpha$ bei $\beta = \alpha^{p-1}$ ein lokales Minimum. Es gilt $f_\alpha(\alpha^{p-1})= 0 \le f_\alpha(0)$, also
hat $f_\alpha$ bei $\beta=\alpha^{p-1}$ ein globales Minimum. Damit ist \eqref{eq:44} bewiesen. \bewende
% subsection 44 (end)

\subsection{Satz: Höldersche Ungleichung} % (fold)
\label{sub:45}
Sei $(X,\Sigma,\mu)$ ein Maßraum. Seien $p,q \ge 1$ mit $\frac{1}{p} +\frac{1}{q}=1 $, $f \in \mathcal{L}^p(\mu)$, $g \in \mathcal{L}^q(\mu)$. Dann gilt 
$f \cdot g \in \mathcal{L}^1(\mu)$ und
\[
	\norm[1]{f \cdot g} \le \norm[p]{f} \cdot \norm[q]{g}   
\]
Die entsprechenden Aussagen gelten dann auch für $L^p(\mu)$, $L^q(\mu)$ und $L^1(\mu)$.\index{Hölder-Ungleichung}
\minisec{Beweis}
Klar, falls $\norm[p]{f} =0 $ oder $\norm[q]{g} =0$ (Warum?). Sei also $\norm[p]{f}, \norm[q]{g} \not= 0$. Nach Proposition \ref{sub:44} gilt für $x \in X$
\[
	0 \le \underbrace{\underbrace{\frac{\abs*{f(x)} }{\norm[p]{f}}}_{=\alpha} \cdot \underbrace{\frac{\abs*{g(x)} }{\norm[q]{g} }}_{=\beta} }_{\text{messbar}}
	\le \underbrace{\frac{1}{p} \cdot \frac{\abs*{f(x)}^p }{\norm[p]{f}^p } + \frac{1}{q} \cdot \frac{\abs*{g(x)}^q }{\norm[q]{g}^q }}_{\text{integrierbar}}
\]
Es ist $\abs*{f}^p , \abs*{g}^q  \in \mathcal{L}^1(\mu)$, also folgt aus obiger Ungleichung 
$\frac{\abs*{ f \cdot g }}{\norm[p]{f} \cdot \norm[q]{g}}  \in \mathcal{L}^1(\mu)$. Weiter gilt nun
\begin{align*}
	\frac{\int \abs*{f \cdot g} \mathd \mu}{\norm[p]{f} \cdot \norm[q]{g}} \le \frac{1}{p} \cdot \frac{\int \abs*{f}^p \mathd \mu }{\norm[p]{f}^p } +
	\frac{1}{q} \cdot \frac{\int \abs*{g}^q \mathd \mu }{\norm[q]{g}^q} = \frac{1}{p} + \frac{1}{q} = 1       
\end{align*}
Es folgt also $\int \abs*{f \cdot g} \mathd \mu  \le \norm[p]{f}  \cdot \norm[q]{g}$. \bewende
% subsection 45 (end)

\subsection{Satz: Minkowskische Ungleichung} % (fold)
\label{sub:46}
Für $1<p < \infty$, $f,g \in \mathcal{L}^p(\mu)$ (bzw. $L^p(\mu)$) gilt die \Index{Minkowski-Ungleichung}
\[
	\norm[p]{f +g} \le \norm[p]{f} + \norm[p]{g}   
\]
\minisec{Beweis}
Sei $\norm[p]{f+g} >0 $ (sonst ist die Aussage trivial). Es gilt 
\begin{align*}
	\abs*{f +g}^p = \abs*{f+g}^{p-1} \cdot \abs*{f+g} \le \abs*{f+g}^{p-1} \cdot \enbrace[\big]{\abs*{f} + \abs*{g}}     
\end{align*}
Sei $q>1$ mit $\frac{1}{p} + \frac{1}{q}=1$. Es gilt $\abs*{f+g}^{p-1} \in \mathcal{L}^q(\mu)$, da $\abs*{f+g}^{(p-1)q} = \abs*{f+g}^p \in \mathcal{L}^1(\mu)$. Nach der 
Hölderschen Ungleichung \ref{sub:45} gilt
\begin{align*}
	\int \!\abs*{f+g}^p \mathd \mu \le \int \!\abs*{f+g}^{p-1} \abs*{f} \mathd \mu + \int \!\abs*{f+g}^{p-1} \abs*{g}\mathd \mu &\le \norm[p]{f} \cdot \norm[q]{\abs*{f+g}^{p-1}}
	+ \norm[p]{g} \cdot \norm[q]{\abs*{f+g}^{p-1} } \\        
	&= \enbrace*{\norm[p]{f} + \norm[p]{g}} \cdot \enbrace*{\int \abs*{f+g}^{(p-1)q} \mathd \mu }^{\frac{1}{q}} \\
	&= \enbrace*{\norm[p]{f} + \norm[p]{g}} \cdot \enbrace*{\int \abs*{f+g}^{p} \mathd \mu }^{\frac{1}{q}} 
\end{align*}
Nach Umstellen der Ungleichung gilt also
\[
	\enbrace*{\int \abs*{f+g}^p \mathd \mu}^{1- \frac{1}{q}} = \enbrace*{\int \abs*{f+g}^{p} \mathd \mu }^{\frac{1}{p}} \le \norm[p]{f} + \norm[p]{g} \bewende    
\]
% subsection 46 (end)

\subsection[Corollar: $L^p(\mu)$ ist ein normierter Vektorraum für $1 \le p < \infty$]{Corollar} % (fold)
\label{sub:47}
$L^p(\mu)$ ist ein normierter Vektorraum für $1 \le p < \infty$.
% subsection 47 (end)

\subsection[Definition und Proposition: Der normierte Raum $L^\infty(\mu)$ und ]{Definition und Proposition} % (fold)
\label{sub:48}
\begin{align*}
	\mathcal{L}^\infty(\mu) &:= \set[f \colon X \to \mathds{R} \text{ messbar}]{\exists C \ge 0 : \abs*{f} \le C \text{ fast überall} \rule{0cm}{0.7em}\rule{0cm}{0.9em}}  \\
	\mathcal{N}^\infty &:= \set[f \colon X \to \mathds{R}]{f=0 \text{ fast überall}\rule{0cm}{0.9em}} 
\end{align*}
Wir definieren $L^\infty(\mu) := \nicefrac{\mathcal{L}^\infty(\mu)}{\mathcal{N}^\infty}$. Wir definieren eine Halbnorm auf $\mathcal{L}^\infty(\mu)$ durch
\[
	\norm[\infty]{f} := \inf \set[C]{\abs*{f} \le C \text{ fast überall}\rule{0cm}{0.9em}}
\]
Dies induziert wieder eine Norm auf $L^\infty(\mu)$ durch $\norm[\infty]{f + \mathcal{N}} := \norm[\infty]{f}$. Damit ist $\enbrace*{L^\infty(\mu), \norm[\infty]{\cdot}}$
ein normierter Vektorraum. Für $f \in L^1(\mu)$, $g \in L^\infty(\mu)$ gilt die Höldersche Ungleichung $\norm[1]{f \cdot g} \le \norm[1]{f} \cdot \norm[\infty]{g}$, also gilt
$f \cdot g \in L^1(\mu)$.
\minisec{Beweis}
\emph{Übung!} \bewende
% subsection 48 (end)

\subsection[Beispiele für $L^p$-Räume]{Beispiele} % (fold)
\label{sub:49}
\begin{enumerate}[(i)]
	\item $(\mathds{R}^n, \norm[p]{\cdot})$, $1 \le p < \infty$, also $X=\set{1,\ldots,n}, \Sigma=\mathcal{P}(X), \mu =$ Zählmaß.
	\item $\enbrace[\big]{\ell^p(\mathds{N}), \norm[p]{\cdot }}$, $1 \le p \le \infty$, also $X=\mathds{N}, \Sigma=\mathcal{P}(\mathds{N}), \mu=$ Zählmaß.
	Dabei ist \marginnote{auch: Raum der $p$-summierbaren Folgen}
	\[
		\ell^p(\mathds{N}) = \set[(a_n)_n]{a_n \in \mathds{R}, \sum_{n \in \mathds{N}} \abs*{a_n}^p < \infty} 
	\]
	für $p < \infty$ bzw. $\ell^p(\mathds{N})= \set{\text{beschränkte Folgen}}$ für $p=\infty$.
\end{enumerate}
% subsection 49 (end)

\subsection[Proposition: Lineare Isometrie $\iota_p \colon L^p(\mu) \to L^q(\mu)^*$]{Proposition} % (fold)
\label{sub:410}
Sei $(X,\Sigma,\mu)$ ein Maßraum und $p,q >1$ mit $\frac{1}{p} + \frac{1}{q}=1$, bzw. $p=1$ und $q=\infty$.\marginnote{In \ref{sub:611} werden wir diese Aussage noch etwas verstärken} 
Die Abbildung $\iota_p \colon L^p(\mu) \to L^q(\mu)^*$, gegeben 
durch
\[
	\iota_p(f)(g) := \int \! f \cdot g \, \mathd \mu
\]
ist eine lineare Isometrie.
\minisec{Beweis}
\begin{description}
	\item[$\iota_p(f) \colon L^q(\mu) \to \mathds{R}$ ist linear und wohldefiniert:] Übung!
	\item[$\iota_p$ ist linear und wohldefiniert:] Übung!
\end{description}
$\iota_p(f)$ ist beschränkt durch $\norm[p]{f}$ nach Hölder (\ref{sub:45}), also ist $\iota_p$ beschränkt durch $1$. Noch zu zeigen: Zu $f \in L^p(\mu)$ existiert 
$g \in L^q(\mu)$ mit $\abs*{\iota_p(f)(g)} = \norm[p]{f} \cdot \norm[q]{g}$, denn dann gilt $\norm[L^q(\mu)^*]{\iota_p(f)}  \ge \norm[p]{f}$.
Gegeben $f \in L^p(\mu)$ setzen wir $g := \sgn(f) \cdot \abs*{f}^{p-1}$. Dann ist $\abs*{g}^q= \abs*{f}^p$, also $g \in L^q(\mu)$. Weiter gilt dann
\begin{align*}
	\int \!f \cdot g \,\mathd \mu = \int \! \abs*{f \cdot g} \mathd \mu = \int \!\abs*{f}^p \mathd \mu  = \enbrace*{\int\! \abs*{f}^p \mathd \mu }^{\frac{1}{p}}
	\!\cdot  \enbrace*{\int \!  \abs*{f}^p \mathd\mu}^{\frac{1}{q}} = \norm[p]{f} \cdot \norm[q]{g}    
\end{align*}
Für $p=1$ und $q=\infty$ ist die Aussage eine Übungsaufgabe. \bewende
% subsection 410 (end)

\subsection{Satz von Riesz-Fischer} % (fold)
\label{sub:411}
Sei $(X,\Sigma,\mu)$ ein Maßraum. $L^p(\mu)$ ist vollständig für $p \in [1,\infty]$.\index{Riesz-Fischer}
\minisec{Beweis}
Für $p=\infty$ ist dies eine Übung (Blatt 7, Aufgabe 1). Sei also $p <\infty$ und sei $(f_n)_{n \in \mathds{N}} \subset L^p(\mu)$ Cauchy.
\begin{description}
	\item[Behauptung:] Es gibt $E_l \subset X$, $l \in \mathds{N}$, messbare Teilmenge mit $\mu(E_l) <\infty$ und sodass für $E := \bigcup_{l \in \mathds{N}} E_l$ gilt 
	$\chi_{X \setminus E} \cdot f_n = 0$ in $L^p(\mu)$ für $n \in \mathds{N}$. Beweis: Übung (Blatt 7, Aufgabe 2, siehe Anhang \ref{sub:blatt7_aufg2})
\end{description}
Wähle $n_0 < n_1 < \ldots $ mit $\norm[p]{f_n - f_{n_k}} < \frac{1}{2^k}$ für $n > n_k$, $k \in \mathds{N}$. Für jedes $l,k \in \mathds{N}$ gilt
$\abs*{f_{n_{k+1}} - f_{n_k}} \in L^p(\mu)$ und $\chi_{E_l} \in L^q(\mu)$, wobei $\frac{1}{p} + \frac{1}{q}=1$. Also ist nach Hölder (\ref{sub:45}) auch
\(
	\abs*{f_{n_{k+1}} - f_{n_k}} \cdot \chi_{E_l} \in L^1(\mu)
\).
Weiter gilt für $j \in \mathds{N}$, $l \in \mathds{N}$
\[
	\sum_{k=0}^{j}  \doppelstrich[\Big]{\abs*{f_{n_{k+1}}- f_{n_k}} \cdot \chi_{E_l}}_{1}
	\le \sum_{k=0}^{j} \norm[p]{f_{n_{k+1}}- f_{n_k}} \cdot \norm[q]{\chi_{E_l}} \le \sum_{k=0}^{j} \frac{1}{2^k} \cdot \norm[q]{\chi_{E_l}}
\]
$\enbrace[\big]{\sum_{k=0}^{j} \abs*{f_{n_{k+1}} - f_{n_k}} \cdot \chi_{E_l}}_{j \in \mathds{N}}$ ist eine aufsteigende Folge von integrierbaren Funktionen. Nach dem Satz von
Lebesgue über monotone Konvergenz gilt: $\sum_{k=0}^{\infty} \abs*{f_{n_{k+1}} - f_{n_k}} $ konvergiert auf $E_l$ fast überall. Da abzählbare Vereinigungen von Nullmengen
Nullmengen sind, folgt: $\sum_{k=0}^{\infty} \abs*{f_{n_{k+1}}- f_{n_k}}$ konvergiert auf $E$ fast überall. Also konvergiert $\sum_{k=0}^{\infty} \abs*{f_{n_{k+1}}- f_{n_k}}$
auf $X$ fast überall. Damit konvergiert auch $\sum_{k=0}^{\infty} f_{n_{k+1}}- f_{n_k}$ auf $X$ fast überall. Nach Auflösen der Wechselsumme stellen wir fest, dass also
auch $(f_{n_k})_{k \in \mathds{N}}$ auf $X$ fast überall konvergiert. Setze
\[
	f(x) := \begin{cases}
		\lim_{k\to \infty} f_{n_k}(x), &\text{ falls der Limes existiert}\\
		0, &\text{ sonst}
	\end{cases}
\]
$f$ ist als Limes messbarer Funktionen messbar. Weiter gilt $\abs*{f_{n_k}}^p \to \abs*{f}^p$ fast überall. 
Noch zu zeigen: $f \in L^p(\mu)$. Die Folge $\enbrace[\big]{\norm[p]{f_{n_k}}^p}_{k}$ ist beschränkt, da $(f_n)_{n}$ eine Cauchyfolge bezüglich $\norm[p]{\cdot}$ ist. Es gilt
\marginnote{$\abs*{f}^p \stackrel{\text{f.ü.}}{=} \liminf \abs*{f_{n_k}}^p$}
\[
	\int \abs*{f}^p \mathd \mu \le \liminf\limits_{k \to \infty} \int \abs*{f_{n_k}}^p \mathd \mu < \infty
\]
nach dem Lemma von Fatou\footnote{siehe Analysis III. 9.23 oder \url{https://de.wikipedia.org/wiki/Lemma_von_Fatou}}. Also gilt $f \in L^p(\mu)$.
Noch zu zeigen: $f_n \xrightarrow{n \to \infty} f$ bezüglich $\norm[p]{\cdot }$. Sei dazu $j \in \mathds{N}$. Sei $n \ge n_j$. Für $i>j$ gilt
\[
	\norm[p]{f_n - f_{n_i}} \le \norm[p]{f_n - f_{n_j}} + \norm[p]{f_{n_j}- f_{n_i}} < 2 \cdot \frac{1}{2^j}    
\]
Weiter gilt $\abs*{f_{n}- f_{n_i}}^p \xrightarrow{i \to \infty} \abs*{f_n -f}^p$ fast überall. Mit dem Lemma von Fatou folgt 
\[
	\int\!\abs*{f_n- f}^p \mathd\mu \le \liminf_{i \to \infty} \int \! \abs*{f_n - f_{n_i}}^p \mathd\mu  \le \enbrace*{2 \cdot \frac{1}{2^j}}^p
\]
$\Rightarrow \norm[p]{f_n -f} \le 2 \cdot \frac{1}{2^j}$, falls $n \ge n_j$. Also gilt $\norm[p]{f_n -f} \xrightarrow{n \to \infty} 0 $. \bewende
% subsection 411 (end)
% section 4 (end)
\newpage

\section{Schwache Topologien, Reflexivität} % (fold)
\label{sec:5}
\subsection[Definition: Schwache Topologie]{Definition} % (fold)
\label{sub:51}
Sei $X$ ein normierter $\mathds{K}$-Vektorraum. Die \Index{schwache Topologie} $\weakT{X}$ ist die gröbste\footnote{auch: \enquote{dollste}} Topologie auf $X$, sodass alle 
$\varphi \in X^*$ stetig sind. 
% subsection 51 (end)

\subsection[Bemerkungen zur schwachen Topologie]{Bemerkung} % (fold)
\label{sub:52}
\begin{enumerate}[(i)]
	\item Jedes $\varphi \in X^*$ ist -- nach Definition von $X^*=\mathcal{L}(X,\mathds{K})$ -- stetig bezüglich $\mathcal{T}^{\norm{\cdot}}_X$, d.h. 
	$\weakT{X} \subset \mathcal{T}_X^{\norm{\cdot}}$. Mit anderen Worten: Die Abbildung 
	$\id_X \colon \enbrace*{X,\mathcal{T}_X^{\norm{\cdot}}}\to\enbrace[\Big]{X,\weakT{X}}$ ist stetig.
	\item Mengen der Form $\varphi ^{-1}(U)$ für $U \subset K$ offen, $\varphi \in X^*$ bilden eine Subbasis für $\weakT{X}$. Mengen der Form 
	\[
		\bigcap_{i=1}^n \varphi_i ^{-1} (U_i) \quad, \qquad U_i \subset \mathds{K} \text{ offen, } \varphi_i \in X^*
	\]
	bilden eine Basis für $\weakT{X}$.
\end{enumerate}
% subsection 52 (end)

\subsection[Proposition: Die schwache Topologie ist Hausdorffsch]{Proposition} % (fold)
\label{sub:53}
Die schwache Topologie $\enbrace[\big]{X,\weakT{X}}$ ist Hausdorffsch.
\minisec{Beweis}
Seien $x \not= y \in X$. Nach \hyperref[sub:227]{Hahn-Banach} existiert $\varphi \in X^*$ mit $\varphi(x) \not= \varphi(y)$. Setze 
$\varepsilon := \frac{\abs*{\varphi(x)- \varphi(y)} }{2}>0$, dann sind $\varphi ^{-1} \enbrace[\big]{B_\mathds{K} \enbrace*{\varphi(x),\varepsilon} } $ und
$\varphi ^{-1} \enbrace[\big]{B_\mathds{K} \enbrace*{\varphi(y), \varepsilon}}$ disjunkte offene Umgebungen von $x$ bzw. $y$. \bewende
% subsection 53 (end)

\subsection[Proposition: Äquivalenz zu schwacher Konvergenz]{Proposition} % (fold)
\label{sub:54}
Sei $X$ ein normierter Raum, $x \in X$, $(x_\lambda )_\Lambda \subset X$ ein Netz. Dann sind äquivalent:
\begin{enumerate}[(i)]
	\item $x_\lambda \xrightarrow{\w}  x$, d.h. $x_\lambda$ konvergiert gegen $x$ in der schwachen Topologie $\weakT{X}$.\index{schwache Konvergenz}
	\item $\varphi(x_\lambda ) \to \varphi(x)$ für alle $\varphi \in X^*$.
\end{enumerate}
\minisec{Beweis}
\begin{description}
	\item[(i)$\Rightarrow$(ii):] Sei $\varphi \in X^*$ und $\varepsilon>0$. Es ist $\varphi(x) \in B_\mathds{K}\enbrace[\big]{\varphi(x),\varepsilon} \subset \mathds{K}$ 
	offen und $\varphi$ stetig bezüglich $\weakT{X}$, also
	\[
		x \in \varphi ^{-1} \enbrace[\Big]{ B_\mathds{K}\enbrace[\big]{\varphi(x), \varepsilon} } \underset{\text{$\w$-offen}}{\subset} X 
	\] 
	Da $x_\lambda \xrightarrow{\w} x$ existiert ein $ \overline{\lambda } \in \Lambda$ mit 
	$x_\lambda \in \varphi^{-1}\enbrace[\big]{B_\mathds{K}\enbrace*{\varphi(x), \varepsilon}}$, falls $\lambda \ge \overline{\lambda }$. Dann folgt
	$\varphi(x_\lambda ) \in B_\mathds{K}\enbrace*{\varphi(x),\varepsilon}$ falls $\lambda \ge \overline{\lambda}$, also $\varphi(x_\lambda ) \to \varphi(x)$ in $\mathds{K}$.
	\item[(ii)$\Rightarrow$(i):] Sei $x \in V \subset X$ $\w$-offen. Nach \ref{sub:52}(ii) existieren $\varphi_1, \ldots , \varphi_m \in X^*$, $U_1, \ldots , U_m$ offen in
	$\mathds{K}$ mit 
	\[
		x \in \varphi_1 ^{-1}(U_1) \cap \ldots  \cap \varphi_m ^{-1}(U_m) \subset V
	\]
	Wegen (ii) existieren $\lambda_1, \ldots , \lambda _m \in \Lambda$ mit $\varphi_i(x_\lambda ) \in U_i$, falls $\lambda  \ge \lambda_i$.
	Wähle $\overline{\lambda} \in \Lambda$ mit $\overline{\lambda} \ge \lambda_1, \ldots , \lambda_m$. Dann gilt $\varphi_i(x_\lambda) \in U_i$, falls 
	$\lambda \ge \overline{\lambda}$ für alle $i$. Damit ist dann auch $x_\lambda \in \varphi_i ^{-1}(U_i)$ für alle $i$, falls $\lambda \ge \overline{\lambda}$. Also gilt
	$x_\lambda \in \bigcap_{i=1}^m \varphi_i ^{-1}(U_i) \subset V$, falls $\lambda \ge \overline{\lambda}$. \bewende
\end{description}
% subsection 54 (end)

\subsection[Corollar: Schwach konvergente Netze sind beschränkt]{Corollar} % (fold)
\label{sub:55}
Wenn $x_\lambda \xrightarrow{\minwidthbox{\w}{1.2em}} x$, dann ist $(x_\lambda)_\Lambda \subset X$ beschränkt.
\minisec{Beweis}
Wir betrachten zunächst die folgende Menge 
\[
	\set[\iota_X(x_\lambda)]{\lambda\in \Lambda}\subset \mathcal{L}(X^*,\mathds{K})=X^{**}.
\]
Wir wollen zeigen, dass diese punktweise gleichmäßig beschränkt ist. Sei dazu $\varphi\in X^*$ beliebig. Da $\varphi(x_\lambda)$ nach \ref{sub:54} konvergent und somit auch 
beschränkt ist, gibt es ein $C_\varphi>0$ mit
\[
	\abs*{\iota_X(x_\lambda)(\varphi)} = \abs*{\varphi(x_\lambda)} \le C_\varphi.
\]
Nach \ref{sub:31} ist $X^*$ vollständig, also können wir nun das \hyperref[sub:35]{Prinzip der gleichmäßigen Beschränktheit} anwenden und erhalten ein $C>0$ mit 
$\norm{\iota_X(x_\lambda)} \le C$ für alle $\lambda\in \Lambda$. Da $\iota_X$ isometrisch ist, gilt also auch $\norm{x_\lambda} \le C$ für alle $\lambda\in \Lambda$, also 
ist das Netz beschränkt. \bewende
% subsection 55 (end)

\subsection[Beispiel: Konvergenz impliziert schwache Konvergenz, aber nicht umgekehrt]{Beispiel} % (fold)
\label{sub:56}
Im Allgemeinen gilt
\[
	x_\lambda \xrightarrow{\norm{\cdot}} x \quad \begin{matrix}
		\Rightarrow \\
		\nLeftarrow 
	\end{matrix} \quad  x_\lambda \xrightarrow{\w} x 
\]
Betrachte $X=\ell^2(\mathds{N})$ mit $\norm[2]{\cdot }$. 
Sei $e_n \in \ell^2(\mathds{N})$ gegeben durch $e_n(m) := \delta_{n,m}$\marginnote{Kronecker-Delta}. Dann gilt $e_n  \not\to 0$ für $n \to \infty$ in 
$\mathcal{T}^{\norm[2]{\cdot }}$, denn $\norm[2]{e_n -0} = \norm[2]{e_n} \equiv 1 \not\to 0$. Aber $e_n \xrightarrow{\w} 0$ für $n \to	\infty$, denn es ist
-- wie wir später sehen werden --  $\ell^2(\mathds{N})^* \cong \ell^2(\mathds{N})$ via $\sprod*{a}{b} = \sum \overline{a(k)} \cdot b(k)$. Es gilt aber 
\[
	\sprod*{a}{e_n} = \sum \overline{a(k)} \cdot e_n(k) = \overline{a(n)} \xrightarrow{n \to \infty} 0.
\] 
Also gilt $e_n \xrightarrow{\w}0$ für $n \to \infty$. \smallskip\\
\emph{Direktes Argument: Übung (Aufgabe 3 von Blatt 7)}
% subsection 56 (end)

\subsection[Proposition: Elemente aus $\mathcal{L}(X,Y)$ sind auch stetig bezüglich schwachen Topologie]{Proposition} % (fold)
\label{sub:57}
Seien $X,Y$ normierte Räume, $T \in \mathcal{L}(X,Y)$ ein beschränkter (d.h. normstetiger) linearer Operator. Dann ist $T$ auch stetig bezüglich der schwachen Topologie auf
$X$ und $Y$.
\minisec{Beweis}
Zu zeigen: Falls $U \subset Y$ schwach-offen ist, so ist $T ^{-1}(U)$ schwach-offen in $X$. Wir können annehmen, dass $U$ von der Form
$\varphi ^{-1} \enbrace*{B_\mathds{K}(\lambda,\varepsilon)}$, $\varphi \in Y^*, \lambda \in \mathds{K}, \varepsilon>0$ ist, da diese Mengen eine Subbasis bilden.
Es gilt nun
\begin{align*}
	T ^{-1} \enbrace[\Big]{ \varphi ^{-1} \enbrace[\big]{ B_\mathds{K}(\lambda,\varepsilon)} } = \enbrace[\Big]{\underbrace{\varphi \circ T}_{\in X^*}} ^{-1} 
	\enbrace[\big]{B_\mathds{K}(\lambda,\varepsilon)} \underset{\text{$\w$-offen}}{\subset} X   \bewende
\end{align*}
% subsection 57 (end)

\subsection[Definition: Die $\w^*$-Topologie auf $X^*$]{Definition} % (fold)
\label{sub:58}
Sei $X$ ein normierter Raum. Die \bet{$\mathbf{w}^{\mathbf{*}}$-Topologie} $\weakTstar{ }$\index{w-Topologie@$\w^*$-Topologie} ist die gröbste Topologie auf $X^*$, sodass alle $\iota_X(x) \in X^{**}$ stetig sind.
% subsection 58 (end)

\subsection[Bemerkungen zur $\w^*$-Topologie auf $X^*$]{Bemerkung} % (fold)
\label{sub:59}
\begin{enumerate}[(i)]
	\item Es gilt $\weakTstar{X^*} \subset \weakT{X^*} \subset \mathcal{T}^{\norm{\cdot}}_{X^*}$.
	\item Mengen der Form $W(\varphi,x, \varepsilon) := \set[\psi \in X^*]{\abs*{\psi(x)- \varphi(x)}< \varepsilon}$ bilden eine Subbasis für $\weakTstar{X^*}$.
	Mengen der Form $\set[\psi]{ \psi(x) \in U}$ für $U \subseteq \mathds{K}$ offen bilden ebenfalls eine Subbasis für $\weakTstar{X^*}$.
	\item Ein Netz $(\varphi_\lambda)_{\lambda \in \Lambda} \subset X^*$ konvergiert $\w^*$ gegen $\varphi \in X^*$ genau dann, wenn 
	\[
		\iota_X(x)(\varphi_\lambda)=\varphi_\lambda(x) \xrightarrow{\minwidthbox{\lambda}{1.5em}} \varphi(x) = \iota_X(x)(\varphi)
	\]
	für jedes $x \in X$.
	\item $\weakTstar{X^*}$ ist Hausdorff.
\end{enumerate}
\minisec{Beweis}
\emph{Übung!} 
% subsection 59 (end)

\subsection{Satz (Banach-Alaoglu)} % (fold)
\label{sub:510}
Sei $X$ ein normierter Raum. Dann ist die abgeschlossene Einheitskugel in $X^*$
\[
	\overline{B}_{X^*}(0,1) = \set[\varphi \in X^*]{\norm[X^*]{\varphi} \le 1}  
\]
$\w^*$-kompakt.
\minisec{Beweis}
Für $x \in X$ setze $F_x := \mathds{K}$ und $F := \prod_{x \in X} F_x$. Wir versehen $F$ mit der Produkttopologie $\mathcal{T}_F^\Pi$. Definiere eine Abbildung
$\kappa \colon X^* \to F$ durch $\kappa(\varphi)_x := \varphi(x)$, $x \in X$. Dann ist $\kappa$ injektiv (warum?) und $\weakTstar{X^*}$ stimmt überein mit der durch $\kappa$ und
$\mathcal{T}_F^\Pi$ induzierten Topologie auf $X^*$. Diese besteht aus den Mengen $\set[\kappa ^{-1}(U)]{U \in \mathcal{T}_F^\Pi}$. Eine Basis für $\mathcal{T}_F^\Pi$ sind
Mengen der Form 
\begin{equation*}
	\prod_{x \in X} U_x, \quad U_x \subset \mathds{K} \text{ offen} 
\end{equation*}
mit $U_x = \mathds{K}$ für alle bis auf endlich viele $x \in X$. Eine Basis, für die durch
$\kappa$ und $\mathcal{T}_F^\Pi$ induzierte Topologie auf $X^*$ sind Mengen der Form
\[
	\kappa ^{-1} \enbrace*{ \prod_{x \in X} U_x} = \underbrace{\bigcap_{i=1}^n \set[\varphi]{\varphi(x_i) \in U_{x_i}} }_{
	\mbox{\scriptsize bilden Basis für $\weakTstar{X^*}$, \ref{sub:59}(ii)}}
\]
mit $U_x$ wie oben. Dabei erhalten wir die $x_i$ durch $\kappa ^{-1} \enbrace*{\prod_{x \in X} V_x} = \set[\varphi]{\varphi(x_i)\in U_{x_i}}$, wobei 
\[
	V_x = \begin{cases}
		\mathds{K}, &\text{ falls } x \not= x_i\\
		U_{x_i}, &\text{ falls } x=x_i
	\end{cases}
\]
Falls $\norm{\varphi} \le 1$, so gilt $\abs*{\varphi(x)} \le \norm[X]{x}$, also 
\[
	\kappa \enbrace*{\overline{B}_{X^*}(0,1)} \subset \underbrace{\prod_{x \in X} \overline{B}_{\mathds{K}}(0, \norm{x})}_{\text{kompakt nach Tychonov}} \subset F
\]
Bleibt zu zeigen: $\kappa \enbrace*{\overline{B}_{X^*}(0,1)} \subset F$ ist abgeschlossen. Sei $f \in \overline{\kappa \enbrace*{\overline{B}_{X^*}(0,1)}}$. Dies definiert
eine Abbildung $\varphi_f \colon X \to \mathds{K}$ durch $\varphi_f(x) := f_x$. Falls nun $\varphi_f$ linear und stetig ist mit $\norm{\varphi_f} \le 1$, so gilt
$\kappa(\varphi_f) = f$, denn $\kappa(\varphi_f)_x = \varphi_f(x) = f_x$. Damit folgt $f \in \kappa \enbrace*{ \overline{B}_{X^*}(0,1)}$ und dann ist
$\overline{B}_{X^*}(0,1)$ kompakt.
\begin{description}
	\item[Additivität:] Seien $y,z \in X$, $\varepsilon>0$. Dann ist 
	\[
		W_\varepsilon := \set[g \in \prod_{x \in X} \overline{B}_\mathds{K}(0, \norm{x} ) ]{\begin{array}{r}
			\abs*{g_{y+z}- f_{y+z}} < \varepsilon \\ \enspace\abs*{g_y-f_y} < \varepsilon \\ \enspace\abs*{g_z -f_z}  < \varepsilon
		\end{array}  }  
		\underset{\text{offen}}{\subset} \prod_{x \in X} \overline{B}_\mathds{K}(0,\norm{x}) 
	\]
	Da $f \in \overline{\kappa \enbrace*{\overline{B}_{X^*}(0,1)}}$ existiert ein $ g \in W_\varepsilon \cap \kappa \enbrace*{\overline{B}_{X^*}(0,1)}$. Sei 
	$g' \in \overline{B}_{X^*}(0,1)$ mit $\kappa(g')=g$. $g' \in X^*$ ist linear, also gilt auch $g_{y+z}= g_y + g_z$ und daher 
	\[
		\abs*{f_{y+z} - (f_y + f_z)} < \varepsilon + \varepsilon + \varepsilon= 3 \varepsilon
	\]
	$\varepsilon>0$ war beliebig, also $f_{y+z}= f_y+f_z$ und $\varphi_f(y+z)= \varphi_f(y)+ \varphi_f(z)$. Also ist $\varphi$ additiv.
	\item[Skalarmultiplikation:] analog.
\end{description}
Es gilt $\abs*{\varphi_f(x)} = \abs*{f_x} \le \norm{x}$, $x \in X$, also $\norm[X^*]{\varphi_f} \le 1$. \bewende
% subsection 510 (end)

\subsection[Erinnerung: separabel, 1. abzählbar, 2. abzählbar]{Erinnerung} % (fold)
\label{sub:511}
\begin{enumerate}[(a)]
	\item Ein topologischer Raum $X$ heißt
	\begin{enumerate}[(i)]
		\item \Index{separabel}, falls $X$ eine abzählbare dichte Teilmenge enthält.
		\item \Index{1. abzählbar}, falls gilt: Jedes $x \in X$ besitzt abzählbar viele Umgebungen $U_i$, $i \in \mathds{N}$, sodass jede Umgebung $V$ von $x$ wenigstens ein
		$U_i$ enthält.  
		\item \Index{2. abzählbar}, falls gilt: $\mathcal{T}_X$ besitzt eine abzählbare Basis, d.h. es gibt offene Mengen $W_i$, $i \in N$, sodass für jedes $V \subset X$ 
		offen gilt: $V= \bigcup_{W_i \subset V} W_i$.
	\end{enumerate}
	\item 2. abzählbar $\Rightarrow $ 1. abzählbar und separabel.
	\item Ist $X$ kompakt und 1. abzählbar, so besitzt jede Folge in $X$ eine konvergente Teilfolge.
	
	Beweisskizze: $(x_n)_n \subset X$, setze $A_n := \overline{\set{x_n, x_{n+1}, \ldots }}$. Dann ist $\bigcap_n A_n$ nichtleer wegen Kompaktheit, also
	hat $(x_n)_n$ einen Häufungspunkt. Benutze nun 1. abzählbar.
	\item Sei $X$ kompakt und Hausdorff. Dann ist $X$ metrisierbar $\iff$ $X$ ist 2. abzählbar.
	
	Die Hinrichtung ist eine einfache Übungsaufgabe (Blatt 10, Aufgabe 3). Die Rückrichtung benutzt Urysohn und Metrisierungssätze. Den Satz von Urysohn haben wir auch in 
	\enquote{Grundlagen der Analysis, Topologie und Geometrie} im letzen Semester bewiesen.
\end{enumerate}
% subsection 511 (end)

\subsection[Satz: Metrisierbarkeit von $\overline{B_{X^*}(0,1)}$ bezüglich der $\w^*$-Topologie]{Satz} % (fold)
\label{sub:512}
Sei $X$ ein separabler normierter Raum. Dann ist $\overline{B}_{X^*}(0,1)$ bezüglich $\weakTstar{X^*}$ metrisierbar. Insbesondere ist $\overline{B}_{X^*}(0,1)$ bezüglich
$\weakTstar{X^*}$ 2. abzählbar und kompakt, und jede Folge besitzt eine konvergente Teilfolge.
\minisec{Beweis}
Sei $\set[x_n]{n \in \mathds{N}} \subset \overline{B}_X(0,1)$ dicht.\footnote{Im Allgemeinen sind Unterräume separabler topologischer Räume nicht separabel. Für metrische 
Räume allerdings schon.} Definiere eine Abbildung $d \colon X^* \times X^* \to [0,\infty)$ durch \marginnote{Konvergenz mit Abschätzung durch Operatornorm und 
$\norm{x_n}\le 1$}
\[
	d(\varphi, \psi) := \sum_{n \in \mathds{N}} \frac{1}{2^n} \cdot \abs[\big]{\varphi(x_n) - \psi(x_n)}
\]
Wir zeigen, dass $d$ eine Metrik ist: $d$ ist offensichtlich symmetrisch und erfüllt die Dreiecksungleichung. Noch zu zeigen: $d$ ist definit. Wenn $\varphi \not= \psi$, dann
existiert $x \in \overline{B}_X(0,1)$ mit $\varphi(x) \not= \psi(x)$. Da $\varphi$ und $\psi$ stetig sind und $\set[x_n]{n \in \mathds{N}}$ dicht ist,
existiert $x_n$ mit $\varphi(x_n) \not= \psi(x_n)$. Dann ist $d(\varphi,\psi) \not= 0$.

\noindent Sei nun $(\varphi_\lambda)_{\lambda \in \Lambda} \subset \overline{B}_{X^*}(0,1)$ ein Netz und $\varphi \in \overline{B}_{X^*}(0,1)$. Es genügt zu zeigen
\[
	\varphi_\lambda \xrightarrow{\w^*} \varphi \enspace \iff \enspace  d(\varphi_\lambda, \varphi) \longrightarrow 0
\]
denn dann stimmen $\weakTstar{X^*}$ und $\mathcal{T}_{X^*}^d$ überein. (siehe Nachtrag)
\begin{description}
	\item["$\Rightarrow $":] Es gelte $\varphi_\lambda \xrightarrow{\w^*} \varphi$, also $\varphi_\lambda(x_n) \to \varphi(x_n)$ für $n \in \mathds{N}$ nach 
	\ref{sub:59}(iii). Sei $\varepsilon>0$. Wähle $n_0 \in \mathds{N}$ mit $\sum_{n \ge n_0} \frac{1}{2^n} < \frac{\varepsilon}{4}$ und $\overline{\lambda} \in \Lambda$
	mit $\abs*{\varphi_\lambda(x_n)- \varphi(x_n)} < \frac{\varepsilon}{4}$ falls $n < n_0$, $\lambda \ge \overline{\lambda}$. Dann gilt für $\lambda \ge\overline{\lambda}$
	\begin{align*}
		d(\varphi_\lambda,\varphi) &\le \sum_{n < n_0} \frac{1}{2^n} \cdot \abs[\big]{\varphi_\lambda(x_n)- \varphi(x_n)} + \sum_{n \ge n_0} \frac{1}{2^n} \cdot 
		\underbrace{\abs[\big]{\varphi_\lambda(x_n) - \varphi(x_n)}}_{\le 2} \\
		&< 2 \cdot \frac{\varepsilon}{4} + 2 \cdot \frac{\varepsilon}{4}  = \varepsilon
	\end{align*}
	\item["$\Leftarrow$":] Da $d(\varphi_\lambda, \varphi) = \sum_{n \in \mathds{N}} \frac{1}{2^n} \cdot \abs[\big]{\varphi_\lambda(x_n) - \varphi(x_n)} \to 0$, muss bereits
	$\abs*{\varphi_\lambda(x_n)- \varphi(x_n)} \to 0$ für alle $n \in \mathds{N}$ gelten, da alle Summanden positiv sind. Wir haben also 
	$\forall n : \varphi_\lambda(x_n) \to \varphi(x_n)$. Sei nun $x \in X$ und $\varepsilon>0$. Wähle $N \in \mathds{N}$ sodass $\norm{x-x_N} < \frac{\varepsilon}{3} $. Weiter finden wir 
	$\overline{\lambda} \in \Lambda$, sodass $\abs*{\varphi_\lambda(x_n)- \varphi(x_n)}< \frac{\varepsilon}{3}$ für $\lambda \ge \overline{\lambda}$. Dann gilt
	\begin{align*}
		\abs*{\varphi_\lambda(x)- \varphi(x)} &\le \abs[\big]{\varphi_\lambda(x)- \varphi_\lambda(x_N)} + 
		\underbrace{\abs[\big]{\varphi_\lambda(x_N) - \varphi(x_N)}}_{< \frac{\varepsilon}{3}} + \abs[\big]{\varphi(x_N)- \varphi(x)}    \\
		&\le \underbrace{\norm{\varphi_\lambda}}_{\le 1} \cdot \underbrace{\norm{x-x_N}}_{\le \frac{\varepsilon}{3}} 
		+ \underbrace{\abs*{\varphi_\lambda(x_N) - \varphi(x_N)}}_{< \frac{\varepsilon}{3}} 
		+ \underbrace{\norm{\varphi}}_{\le 1} \cdot \underbrace{\norm{x_N-x}}_{\le \frac{\varepsilon}{3} } = \varepsilon
	\end{align*}
	Damit haben wir $\varphi_\lambda(x) \to \varphi(x)$ für alle $x$ gezeigt. Mit \ref{sub:59}(iii) folgt also $\varphi_\lambda \xrightarrow{\w^*} \varphi$. \bewende
\end{description}
\minisec{Nachtrag}
Topologien sind durch Konvergenz von Netzen bestimmt: Sei $(X,\mathcal{T})$ ein topologischer Raum. 
\begin{align*}
	U \subset X \text{ offen } &\iff \forall x \in U : \exists x \in V \subset U, V \in \mathcal{T} \\
	U \subset X \text{ nicht offen } &\iff \exists x \in U : \text{ für jedes } x \in V \underset{\mathclap{\text{offen}}}{\subset} X \text{ gilt } V \cap (X \setminus U) 
	\not= \emptyset \\
	&\stackrel{\text{(\#)}}{\iff} \exists x \in U, \exists \text{ Netz } (x_\lambda)_{\lambda \in \Lambda} \subset X \setminus U \text{ mit } x_\lambda \to x
\end{align*}
Die Implikation $\stackrel{\text{(\#)}}{\Longleftarrow}$ ist trivial. Für $\stackrel{\text{(\#)}}{\Longrightarrow}$ betrachten wir die Menge 
$\Lambda := \set[V]{x \in V \subset X \text{ offen}}$. Diese ist gerichtet bezüglich "$\subset$". Zu jedem $V \in \Lambda$ wählen wir\marginnote{Auswahlaxiom wird benötig!} 
$x_V \in (X \setminus U) \cap V$. Dann gilt $(x_V)_{V \in \Lambda} \subset X \setminus U$ und $x_V \to x$. \bewende
% subsection 512 (end)

\subsection[Definition: Reflexiver normierter Raum]{Definition} % (fold)
\label{sub:513}
Der normierte Raum $X$ heißt \Index{reflexiv}, falls die kanonische Abbildung $\iota_X \colon X \to X^{**}$ ein Isomorphismus ist. 
($\iota_X$ ist ein Isomorphismus genau dann, wenn $\iota_X$ surjektiv ist.)
% subsection 513 (end)

\subsection[Proposition: Abgeschlossene Unterräume von reflexiven Banachräumen sind reflexiv]{Proposition} % (fold)
\label{sub:514}
Sei $X$ ein reflexiver Banachraum und $Y \subset X$ ein abgeschlossener Teilraum. Dann ist $Y$ reflexiv.
\minisec{Beweis}
Sei $\kappa \colon Y \hookrightarrow X$ die Inklusion. Dann ist $\kappa \in \mathcal{L}(Y,X)$ und nach \ref{sub:34} kommutiert das Diagramm
\[
	\begin{tikzcd}
		Y \rar[hook, "\iota_Y"] \dar["\kappa"', hook]& Y^{**}  \dar["\kappa^{\tr \,\tr}"] \\
		X \rar["\iota_X","\cong"', hook] & X^{**}
	\end{tikzcd}
\]
Sei $f\in Y^{**}$, dann existiert $x \in X$ mit $\iota_X(x)=\kappa^{\tr\,\tr}(f)$. Für $\varphi \in X^*$ gilt dann also
\[
	\varphi(x)=\iota_X(x)(\varphi)=\kappa^{\tr\,\tr}(f)(\varphi).
\]
Aber $(\kappa^\tr)^\tr (f) = f \circ \kappa^\tr$ und $\kappa^\tr(\varphi) = \varphi \circ \kappa = \varphi|_{Y}$, also gilt $\varphi(x)= f \enbrace*{\varphi|_Y}$ für jedes 
$\varphi \in X^*$. Falls $x \not\in Y$, so existiert nach Hahn-Banach ein $\psi \in X^*$ mit $\psi|_{Y}=0$, $\psi(x)\not= 0$. Damit folgt 
\[
	0 \not= \psi(x) = f \enbrace*{\psi|_{Y}}=0 \enspace\light
\]
Also gilt $x \in Y$. Weiter gilt $\iota_Y(x)=f$, denn $\kappa^{\tr\,\tr}$ ist injektiv. Andernfalls existiert $0 \not= g \in Y^{**}$ mit $\kappa^{\tr\,\tr}(g)=0$. 
Da $g\not= 0$, existiert ein $\sigma \in Y^*$ mit $g(\sigma) \not= 0$. Nach Hahn-Banach existiert eine Fortsetzung $\rho \in X^*$ mit $\rho|_{Y}=\sigma$. Nun gilt 
\[
	0 = \kappa^{\tr\,\tr}(g)(\rho) = g \circ \kappa^\tr(\rho)= g \enbrace*{\rho|_Y} = g(\sigma) \not= 0 \enspace \light 
\]
Also ist $\kappa^{\tr\,\tr}$ ist injektiv. \bewende
% subsection 514 (end)

\subsection[Proposition: Für Banachräume gilt: $X$ reflexiv $\iff$ $X^*$ reflexiv]{Proposition} % (fold)
\label{sub:515}
Für einen Banachraum $X$ gilt: $X$ reflexiv $\iff$ $X^*$ reflexiv.
\minisec{Beweis}
\begin{description}
	\item["$\Rightarrow$":] $\iota_X\colon X\xrightarrow{\cong} X^{**}$ ist ein isometrischer Isomorphismus. Nach Blatt 6, Aufgabe 2 ist $\iota_X^{\tr}\colon X^{***}\to X^*$
	auch ein isometrischer Isomorphismus. Nach \ref{sub:34} gilt  $\iota_X^{\tr} \circ \iota_{X^*}= \id_{X^{*}}$, also ist $\enbrace*{\iota_X^\tr}^{-1} = \iota_{X^*}$
	ebenfalls ein isometrischer Isomorphismus.
	\item["$\Leftarrow$":] Wenn $X^*$ reflexiv ist, dann ist nach "$\Rightarrow$" auch $X^{**}$ reflexiv. Nun ist aber $\iota_X(X) \subset X^{**}$ abgeschlossen (da 
	$\iota_X$ Isometrie und $X$ vollständig ist). Mit \ref{sub:514} folgt dann, dass auch $\iota_X(X) \cong X$ reflexiv ist.\bewende
	
	\emph{Alternativ:} $\iota_{X^*}$ ist ein isometrischer Isomorphismus und es gilt wieder $\iota_{X}^\tr \circ \iota_{X^*}= \id_{X^*}$. Also ist auch $\iota_{X}^\tr$ ein 
	isometrischer Isomorphismus und mit der Übungsaufgabe folgt dann wieder, dass auch $\iota_X$ ein isometrischer Isomorphismus ist. \bewende
\end{description}
% subsection 515 (end)

\subsection[Satz: Einheitskugel eines Banachraumes $X$ liegt $\w^*$-dicht in Einheitskugel von $X^{**}$]{Satz} % (fold)
\label{sub:516}
Sei $X$ ein Banachraum. Dann gilt (unter der Inklusion $\iota_X \colon X \hookrightarrow X^{**}$)
\[
	\overline{B}_X(0,1)\! \underset{\text{$\w^*$-dicht}}{\subset}\!\! \overline{B}_{X^{**}}(0,1)
\]
\minisec{Beweis}
Zu zeigen: Zu $f \in \overline{B}_{X^{**}}(0,1)$ und $\varepsilon>0$, sowie $\varphi_1, \ldots , \varphi_n \in X^*$, existiert ein $x \in \overline{B}_X(0,1)$ mit
$\abs*{f(\varphi_i)- \varphi_i(x)} < \varepsilon$ für $i=1,\ldots ,n$. Behauptung: Gegeben $f$ und $\varphi_1, \ldots , \varphi_n$ gilt
\[
	\inf_{x \in \overline{B}_X(0,1)} \sum_{i=1}^{n} \abs[\big]{f(\varphi_i)- \varphi_i(x)}^2 = 0
\]
Sei $h(x) := \sum_{i=1}^{n} \abs*{f(\varphi_i)- \varphi_i(x)}^2 $ und $\inf_{x\in \overline{B}_X(0,1)} h(x) =: \mu \ge 0$. Wähle 
$(x_k)_{k \in \mathds{N}} \subset \overline{B}_X(0,1)$ mit $\lim_{k \to \infty} h(x_k)= \mu$. Nach Übergang zu einer Teilfolge dürfen wir annehmen, dass 
$\enbrace[\big]{\varphi_i(x_k)}_{k \in \mathds{N}}$ gegen  $\gamma_i \in \mathds{K}$ konvergiert für $i=1,\ldots ,n$. Setze $\delta_i := f(\varphi_i) -\gamma_i$, dann
gilt $\mu = \sum_{i=1}^{n} \abs*{\delta_i}^2$. Für $y \in \overline{B}_X(0,1)$ und $t \in [0,1]$ gilt nun
\begin{align*}
	\mu \le h\enbrace[\Big]{\underbrace{(1-t) \cdot x_k + t \cdot y}_{\in \overline{B}_X(0,1)}} &= 
	\sum_{i=1}^{n} \abs[\Big]{f(\varphi_i)- (1-t) \cdot \varphi_i(x_k) - t \cdot \varphi_i(y)}^2 \\
	&= \sum_{i=1}^{n} \abs[\Big]{\enbrace[\big]{f(\varphi_i)- \varphi_i(x_k)} - t \cdot  \enbrace[\big]{\varphi_i(y)- \varphi_i(x_k)} }^2 \\
	&= \sum_{i=1}^{n} \abs[\big]{f(\varphi_i)- \varphi_i(x_k)}^2 + t^2 \cdot \sum_{i=1}^{n} \abs[\big]{\varphi_i(y)- \varphi_i(x_k)}^2 \marginnote{mit $z \cdot 
	\overline{z}=\abs*{z}^2$ und $z+ \overline{z}=2 \re(z)$}\\
	& \quad - 2 t \cdot \re \enbrace*{\sum_{i=1}^{n} \enbrace[\big]{\varphi_i(y)- \varphi_i(x_k)} \cdot \overline{\enbrace[\big]{f(\varphi_i)- \varphi_i(x_k)} }} \\
	\xrightarrow{k \to \infty} &\hphantom{=} \underbrace{\sum_{i=1}^{n} \abs[\big]{f(\varphi_i)- \gamma_i}^2}_{=\mu} + t^2 \cdot \sum_{i=1}^{n} \abs[\big]{\varphi_i(y)- \gamma_i}^2  
	-2 t \cdot \re \sum_{i=1}^{n} \enbrace[\big]{\varphi_i(y)- \gamma_i} \cdot \overline{\delta_i }
\end{align*}
Nach Abziehen von $\mu$ folgt für $t >0$
\begin{align*}
	&0 \le t \cdot \sum_{i=1}^{n} \abs[\big]{\varphi_i(y)- \gamma_i}^2 - 2 \cdot \re \sum_{i=1}^{n} \enbrace[\big]{\varphi_i(y)- \gamma_i} \cdot \overline{\delta_i}  \\
	\xRightarrow{\minwidthbox{t \searrow 0}{2em}} \enspace & 2 \cdot \re \sum_{i=1}^{n} \enbrace[\big]{\varphi_i(y)- \gamma_i} \cdot \overline{\delta_i} \le 0 \text{ für alle } y \in \overline{B}_X(0,1)
\end{align*}
Definiere nun $\varphi \in X^*$ durch $\varphi := \sum_{i=1}^{n} \overline{\delta _i} \cdot \varphi_i \in X^*$. Dann gilt für $y \in \overline{B}_X(0,1)$
\[
	\re \varphi(y) \le \re \sum_{i=1}^{n} \gamma_i \cdot \overline{\delta_i},
\]
also ist $\norm[X^*]{\varphi} \le \re \sum_{i=1}^{n} \gamma_i \cdot \overline{\delta_i}$. Dies folgt durch Einsetzen der Definition von $\norm{\varphi}$ und Multiplikation 
mit $\abs*{\lambda}$ für ein geeignetes $\lambda \in S^1 \subset \mathds{C}$.
Andererseits gilt aber
\[
	\varphi(x_k) = \sum_{i=1}^{n} \overline{\delta_i} \cdot \varphi_i(x_k) \xrightarrow{k \to \infty} \sum_{i=1}^{n} \overline{\delta_i} \cdot \gamma_i, 
\]
das heißt $\abs*{\sum_{i=1}^{n} \overline{\delta_i} \cdot \gamma_i} \le \norm{\varphi} \le \re \sum_{i=1}^{n} \gamma_i \cdot \overline{\delta_i}$ und somit
$\sum_{i=1}^{n} \overline{\delta_i} \cdot \gamma_i = \norm[X^*]{\varphi}$. Wir erhalten 
\begin{align*}
	\mu = \sum_{i=1}^{n} \overline{\delta_i} \cdot \delta_i = \sum_{i=1}^{n}\overline{\delta_i} \cdot \enbrace[\big]{f(\varphi_i)- \gamma_i} = f(\varphi)-\norm[X^*]{\varphi} 
	\stackrel{\norm{f}\le 1}{\le} 0 \bewende
\end{align*}
% subsection 516 (end)

\subsection[Corollar: $f \in \overline{B}_{X^{**}}(0,1)$ lässt sich auf endlich vielen $\varphi_i$ als Einsetzung auffassen]{Corollar} % (fold)
\label{sub:517}
Sei $X$ ein Banachraum, $f \in \overline{B}_{X^{**}}(0,1)$ und $\varphi_1, \ldots , \varphi_n \in X^*$. Falls $\overline{B}_X(0,1)$ $\w$-kompakt ist, so existiert 
$x \in \overline{B}_X(0,1)$ mit $f(\varphi_i)= \varphi_i(x)$ für $i=1,\ldots ,n$.
\minisec{Beweis}
Definiere $h \colon X \to \mathds{R}$ durch $h(x):= \sum_{i=1}^{n} \abs*{f(\varphi_i)- \varphi_i(x)}^2$ wie in Satz \ref{sub:516}. $h$ ist stetig auf $X$ bezüglich 
$\weakT{X}$, nimmt also auf $\overline{B}_X(0,1)$ ihr Minimum an. \bewende
% subsection 517 (end)


\subsection[Satz: Äquivalenzen zu: $X$ ist reflexiv]{Satz} % (fold)
\label{sub:518}
Sei $X$ ein Banachraum. Dann sind äquivalent: 
\begin{enumerate}[(i)]
	\item $X$ ist reflexiv.
	\item $\overline{B}_X(0,1)$ ist $\w$-kompakt.
	\item $\weakT{X^*}= \weakTstar{X^*}$.
\end{enumerate}
\minisec{Beweis}
\begin{description}
	\item[(i)$\Rightarrow$(ii):] $\overline{B}_{X^{**}}(0,1)$ ist $\w^*$-kompakt nach \ref{sub:510}. Nach \ref{sub:515} ist $X^*$ reflexiv, daher stimmen auf $X^{**}$
	die $\w^*$- und die $\w$-Topologie überein. Damit ist $ \overline{B}_{X^{**}}(0,1)$ $\w$-kompakt. Aber unter $X \cong X^{**}$ stimmen die jeweiligen $\w$-Topologien
	überein; daher ist $\overline{B}_{X}(0,1) \cong \overline{B}_{X^{**}}(0,1)$ $\w$-kompakt.
	\item[(ii)$\Rightarrow$(i):] Es gilt $(X,\weakT{X}) \subset (X^{**}, \weakTstar{X^{**}})$ in der Relativtopologie (warum?).
	$\overline{B}_X(0,1)$ ist $\w$-kompakt, also ist $\overline{B}_X(0,1) \subset \overline{B}_{X^{**}}(0,1)$ $\w^*$-abgeschlossen. Nach \ref{sub:516} ist $\overline{B}_X(0,1)$ 
	sogar $\w^*$-dicht. Also folgt $\overline{B}_X(0,1) = \overline{B}_{X^{**}}(0,1)$ und somit $X \cong X^{**}$.
	\item[(iii)$\Rightarrow$(i):] $\overline{B}_{X^*}(0,1)$ ist $\w^*$-kompakt nach Banach-Alaoglu (\ref{sub:510}). Mit (iii) folgt, dass $\overline{B}_{X^*}(0,1)$
	$\w$-kompakt ist. Da (ii)$\Rightarrow$(i) folgt, dass $X^*$ reflexiv ist und somit ist nach \ref{sub:515} auch $X$ reflexiv.
	\item[(i)$\Rightarrow$(iii):] klar. \bewende
\end{description}
% subsection 518 (end)

\subsection[Bemerkung: $X$ separabel, wenn $X^*$ separabel, Reflexiviät von Banachräumen]{Bemerkung} % (fold)
\label{sub:519}
\begin{enumerate}[a)]
	\item Sei $X$ ein normierter Raum. Falls $X^*$ separabel ist, so ist $X$ separabel.\marginnote{bzgl. der Normtopologie}
	\item Für einen Banachraum $X$ gilt: $X$ reflexiv $\iff$ $\overline{B}_X(0,1)$ ist folgenkompakt bezüglich $\weakT{X}$.
\end{enumerate}
\minisec{Beweis}
\begin{enumerate}[a)]
	\item \emph{Übung (Blatt 10, Aufgabe 2)}
	\item \begin{description}
		\item["$\Rightarrow$":] leicht.
		\item["$\Leftarrow$":] nicht so leicht. \bewende
	\end{description}
\end{enumerate}
% subsection 519 (end)
% section 5 (end)
\newpage

\section{Gleichmäßig konvexe Räume, noch einmal $L^p$} % (fold)
\label{sec:6}

\subsection[Definition und Proposition: Gleichmäßig konvexer Raum]{Definition und Proposition} % (fold)
\label{sub:61}
Ein normierter Raum $X$ heißt \Index{gleichmäßig konvex}, falls eine der folgenden äquivalenten Bedingungen erfüllt ist:
\begin{enumerate}[(i)]
	\item Für jedes $\varepsilon>0$ existiert $\delta >0$, sodass für $x,y \in X$ mit $\norm{x}, \norm{y} \le 1$ gilt:
	\[
		\norm{x-y} \ge \varepsilon \enspace\Longrightarrow \enspace\norm{\frac{1}{2} (x+y) } \le 1 -\delta 
	\]
	\item \label{61:enum:2} Für jedes $\varepsilon>0$ existiert $\delta >0$, sodass für $x,y \in X$ mit $\norm{x}, \norm{y}= 1$ gilt:
	\[
		\norm{x-y} \ge \varepsilon \enspace\Longrightarrow \enspace\norm{\frac{1}{2} (x+y)} \le 1-\delta   
	\]
	\item Für $(x_n)_\mathds{N}, (y_n)_\mathds{N} \subset X$ mit $\norm{x_n} = \norm{y_n}=1$ für alle $n \in \mathds{N}$ gilt: 
	\[
		\lim_{ n \to \infty} \norm{\frac{1}{2} (x_n+y_n)}=1 \enspace\Longrightarrow \enspace\lim_{ n \to \infty} \norm{x_n- y_n}=0  
	\]
	\item \label{61:enum:4}Für $(x_n)_\mathds{N}, (y_n)_\mathds{N} \subset X$ mit $\limsup_n \norm{x_n}, \limsup_n \norm{y_n}\le 1$ gilt:
	\[
		\lim_{ n \to \infty} \norm{\frac{1}{2} (x_n+y_n)}=1 \enspace\Longrightarrow \enspace\lim_{ n \to \infty} \norm{x_n- y_n}=0  
	\]
\end{enumerate}
\minisec{Beweis}
\begin{description}
	\item[(i)$\Rightarrow$(ii)$\Rightarrow$(iii):] trivial; für (ii)$\Rightarrow$(iii) einfach Implikation umkehren und Aussagen negieren. 
	\item[(iii)$\Rightarrow$(iv):] Aus $\lim_{ n \to \infty} \norm{\frac{1}{2} (x_n+y_n)}=1$ folgt $\lim_{n \to \infty} \norm{x_n +y_n}=2$. Wir haben
	\[
		2 = \lim_{n \to \infty} \norm{x_n +y_n} = \liminf_{n \to \infty} \norm{x_n +y_n} \le \liminf_{n \to \infty} \enbrace[\big]{\norm{x_n} + \norm{y_n}}    
	\]
	Also stimmen Limes superior und Limes inferior jeweils überein und es folgt $\lim_{ n \to \infty} \norm{x_n}=1$ und $ \lim_{ n \to \infty} \norm{y_n}=1$, woraus mit (iii)
	dann wiederum die gewünschte Implikation folgt.
	\item[(iv)$\Rightarrow$(i):] Angenommen (i) gilt nicht, d.h. $\exists \varepsilon>0$ und für jedes $\delta_n=\frac{1}{n+1}$ existieren $x_n,y_n \in X$ mit 
	$\norm{x_n}, \norm{y_n}\le 1$, $\norm{x_n-y_n}\ge \varepsilon$ und $\norm{\frac{1}{2}(x_n+y_n)} > 1- \frac{1}{n+1}$. \light zu (iv). \bewende
\end{description}
% subsection 61 (end)

\subsection[Beispiel für gleichmäßig konvexe Räume]{Beispiel} % (fold)
\label{sub:62}
$(\mathds{R}^n, \norm[p]{\cdot})$ ist gleichmäßig konvex für $1 < p < \infty$ aber nicht für $p=1, p=\infty$
\begin{figure}[H]
	\centering{
	\begin{tikzpicture}[scale=0.8]
		\coordinate (x1) at (5:2);
		\coordinate (x2) at (80:2);
		\draw[lightgray] (-2.4,0) -- (2.4,0);
		\draw[lightgray] (0,-2.4) -- (0,2.4);
		\draw[very thick, Chartreuse3] (0,0) circle(2);
		\draw[thick, dashed] (x1) -- (x2) node[midway, name=mid]{ };
		\draw[fill=DodgerBlue2] (x1) circle(0.05) node[right]{$x$};
		\draw[fill=DodgerBlue2] (x2) circle(0.05) node[above]{$y$};
		\draw[fill=DodgerBlue2] (mid) circle(0.05) node[below left]{$\frac{1}{2}(x+y)$};
	\end{tikzpicture}
	\qquad 
	\begin{tikzpicture}[scale=0.8]
		\coordinate (x1) at (1.8,0.2);
		\coordinate (x2) at (0.4,1.6);
		\draw[lightgray] (-2.4,0) -- (2.4,0);
		\draw[lightgray] (0,-2.4) -- (0,2.4);
		\draw[very thick,DarkGoldenrod2] (2,0) -- (0,2) -- (-2,0) -- (0,-2) -- cycle;
		\draw[thick, dashed] (x1) -- (x2) node[midway, name=mid]{ };
		\draw[fill=DodgerBlue2] (x1) circle(0.05) node[above right]{$x$};
		\draw[fill=DodgerBlue2] (x2) circle(0.05) node[above right]{$y$};
		\draw[fill=DodgerBlue2] (mid) circle(0.05) node[above right]{$\frac{1}{2}(x+y)$};
	\end{tikzpicture}
	\qquad 
	\begin{tikzpicture}[scale=0.8]
		\coordinate (x1) at (2,-1.1);
		\coordinate (x2) at (2,1.8);
		\draw[lightgray] (-2.4,0) -- (2.4,0);
		\draw[lightgray] (0,-2.4) -- (0,2.4);
		\draw[very thick,Firebrick3] (-2,-2) rectangle (2,2) ;
		\draw[thick, dashed] (x1) -- (x2) node[midway, name=mid]{ };
		\draw[fill=DodgerBlue2] (x1) circle(0.05) node[right]{$x$};
		\draw[fill=DodgerBlue2] (x2) circle(0.05) node[right]{$y$};
		\draw[fill=DodgerBlue2] (mid) circle(0.05) node[right]{$\frac{1}{2}(x+y)$};
	\end{tikzpicture}
	\caption[Einheitskugeln der $p$-Norm auf $\mathds{R}^2$ für $p=2$, ${\color{DarkGoldenrod3}p=1}$ und ${\color{Firebrick3}p=\infty}$]{Einheitskugeln der $p$-Norm auf 
	$\mathds{R}^2$ für $p=2$, ${\color{DarkGoldenrod2}p=1}$ und ${\color{Firebrick3}p=\infty}$. Nur für $p \not\in \set{1,\infty}$ ist (\ref{61:enum:2}) aus 
	\ref{sub:61} erfüllt. Anschaulich gesprochen bedeutet dies, dass nur in gleichmäßig konvexen Räumen Bälle auch tatsächlich \enquote{rund} oder zumindest 
	\enquote{rundlich} sind.}}
\end{figure}
% subsection 62 (end)

\subsection[Definition und Proposition: Gleichmäßig konvexe Räume sind strikt konvex]{Definition und Proposition} % (fold)
\label{sub:63}
Ein gleichmäßig konvexer normierter $\mathds{R}$-Vektorraum ist \Index{strikt konvex}, d.h. für alle $0\not= x,y \in X$ mit $\norm{x+y}=\norm{x} + \norm{y}$ existiert 
$\lambda >0$ mit $x=\lambda \cdot y$. 
\minisec{Beweis}
Seien $x,y$ wie angegeben. Ohne Einschränkungen sei $\norm{x}=1$. Wir setzen nun $x' := \frac{x+y}{\norm{x+y}}$, $a := \frac{x+x'}{2}$, $\alpha := \norm{a}$ und 
$\beta := \norm{x+y-a}$.
\begin{figure}[t]
	\centering{
	\begin{tikzpicture}[scale=3, my brace/.style={decorate, decoration={brace, amplitude=5pt, mirror, raise=3pt}, gray}]
		\coordinate (0) at (0,0);
		\coordinate (a) at (2,0.5);
		\coordinate (x) at (2,1);
		\coordinate (x') at (2,0);
		\coordinate (x+y) at (2.8,0);
		\draw[thick] (0,0)  -- (x+y);
		\draw[my brace] (0) -- (x') node[midway, below=6pt]{$\norm{x'}=1$};
		\draw[my brace] (x') --(x+y) node[midway, below=6pt]{$\norm{y}$};
		
		\draw[thick] (0) -- (x);
		\draw[my brace] (x) -- (0) node[midway, above=6pt, sloped]{$\norm{x}=1$}; % above left, yshift=0.16cm,
		
		\draw[thick] (x) -- (x');
		
		\draw[thick] (x+y) -- (x);
		\draw[my brace] (x+y) -- (x) node[midway, above=6pt,sloped]{$\norm{y}$};
		
		\draw[thick,dashed] (0) -- (a) -- (x+y);
		\draw[my brace] (0) -- (a) node[midway, below=6pt,sloped]{$\alpha$};
		\draw[my brace] (a) -- (x+y) node[midway, below=6pt, sloped]{$\beta$};
		
		\foreach \t/\pos in {0/left,x/above,x'/below,x+y/right,a/above right}{
			\draw[fill=DodgerBlue2,thick] (\t) circle (0.02) node[\pos=2pt]{$\t$};
		}
	\end{tikzpicture}
	\caption[Zeichnung zum Beweis von \ref{sub:63}]{Zeichnung zum Beweis von \ref{sub:63}. Die Tatsache, dass $\norm{x}$ und $\norm{x'}$ in dieser Zeichnung nicht gleich 
	sind, weißt schon darauf hin, dass diese Zeichnung in Wahrheit \enquote{degeneriert}.}}
\end{figure}
Es gilt dann
\[
	\norm{x+y-x'} = \enbrace*{1- \frac{1}{\norm{x+y}}} \cdot \norm{x+y} = \norm{x+y}-1 = 1 + \norm{y} -1 = \norm{y}      
\]
Also ist $\beta = \norm{x+y-a} \le \doppelstrich[\big]{\frac{x+y-x}{2}} + \doppelstrich[\big]{\frac{x+y-x'}{2}}= \frac{\norm{y}}{2} + \frac{\norm{y}}{2} = \norm{y}$. Weiter gilt
$\alpha = \norm{a} \le \frac{\doppelstrich{x}}{2} + \frac{\doppelstrich{x'}}{2} =1$ und
\begin{align*}
	1 + \norm{y} = \norm{x+y} \le \norm{a} + \norm{x+y-a} \le \alpha+ \beta\le 1 + \norm{y} 
\end{align*}
also $1 \le \alpha \le 1$. Wegen $\norm{x}=1$, $\norm{x'}=1$ und $\norm{\frac{1}{2} (x+x')}= \norm{a}=\alpha=1 > 1 -\delta$ für jedes $\delta >0$ erhalten wir aus 
gleichmäßiger Konvexität $\norm{x-x'} \le \varepsilon$ für jedes $\varepsilon>0$, also$ x=x'$. Es folgt nun
\[
	x= \frac{x+y}{\norm{x+y}} \iff \norm{x} \cdot x + \norm{y}\cdot x = x+y \iff \norm{y} \cdot x = y \bewende    
\]
% subsection 63 (end)

\subsection[Proposition über strikt konvexe Teilmenge $W$, $a \in X$ und $\inf_{y \in W} \norm{a-y}$]{Proposition} % (fold)
\label{sub:64}
Sei $X$ ein normierter $\mathds{R}$-Vektorraum, $a \in X$, $W \subset X$ konvex und abgeschlossen. 
\begin{enumerate}[(i)]
	\item Ist $W$ strikt konvex, so existiert höchstens ein $x \in W$ mit $\norm{a -x}= \inf_{y \in W} \norm{a-y}$.
	\item Ist $X$ vollständig und gleichmäßig konvex, so existiert genau ein $x \in W$ mit $\norm{a-x}= \inf_{y \in W} \norm{a-y}$
\end{enumerate}
\minisec{Beweis}
Wir dürfen ohne Einschränkungen $a=0$ annehmen. Für $a \in W$ ist nichts zu beweisen. Für $a \not\in W$ gilt $\inf_{y \in W} \norm{a-y} >0$, denn $W$ ist abgeschlossen.
Ohne Einschränkungen dürfen wir außerdem $\inf_{y \in W} \norm{a-y}=1$ annehmen. 
\begin{enumerate}[(i)]
	\item Sei $x,x' \in W$ mit $\norm{a-x}= \norm{a-x'}= \inf_{y \in W} \norm{a-y}$, also $\norm{x}=\norm{x'}=1$. Dann gilt 
	\[
		1= \inf_{y \in W} \norm{a-y} \le \doppelstrich[\bigg]{\underbrace{\frac{1}{2} (x+x')}_{\in W}} \le \frac{1}{2} \norm{x} + \frac{1}{2} \norm{x'} =1   
	\]
	Es folgt aus strikter Konvexität, dass $x=\lambda \cdot x'$ für ein $\lambda \ge 0$. Wegen $\norm{x}=\norm{x'}=1$ folgt $\lambda=1$ und $x=x'$. 
	\item Wähle eine Folge $(x_n)_{\mathds{N}} \subset W$ mit 
	\[
		\lim_{ n \to \infty} \norm{x_n} = \lim_{ n \to \infty} \norm{a-x_n} = \inf_{y \in W} \norm{a-y} = 1 \marginnote{\oE{} wieder $a=0$ und $\inf\limits_{y \in W} \norm{a-y}=1$}
	\]
	Behauptung: Diese Folge ist Cauchy. $W$ ist konvex, also folgt $\frac{1}{2} (x_n+x_m) \in W$. Es gilt dann 
	\begin{equation*}
		1 \le \norm{\frac{1}{2}(x_n+x_m)} \le \frac{1}{2} \enbrace[\big]{\norm{x_n}+ \norm{x_m}} \enspace\text{ für } n,m \in \mathds{N} \label{eq:64} \tag{\#}
	\end{equation*}
	Angenommen $(x_n)_\mathds{N}$ sei nicht Cauchy. Dann existiert $\varepsilon>0$, so dass gilt: Für jedes $N \in \mathds{N}$ existieren $n,m >N$ mit 
	$\norm{x_n +x_m} >\varepsilon$. Wir können daher Teilfolgen $(x_{n_k})_{k \in \mathds{N}}$, $(x_{m_k})_{k \in \mathds{N}}$ bilden mit 
	$\liminf_k \norm{x_{n_k} - x_{m_k}} \ge \varepsilon$. Es gilt aber $\lim_{ k \to \infty} \norm{x_{n_k}} = \lim_{ k \to \infty} \norm{x_{m_k}}=1$ und wegen \eqref{eq:64}
	gilt $\lim_{ k \to \infty} \norm{\frac{1}{2} (x_{n_k} + x_{m_k})} = 1$ \light zu \hyperref[61:enum:4]{\ref*{sub:61} (\ref*{61:enum:4})}. 
	Die Folge ist also Cauchy und konvergiert damit gegen ein $x \in W$, da $W$ vollständig ist. Nach (i) ist $x$ eindeutig. \bewende
\end{enumerate}
% subsection 64 (end)

\subsection[Bemerkung über Folge mit $\limsup_n \norm{x_n}\le 1$ in einem gleichmäßig konvexen Raum]{Bemerkung} % (fold)
\label{sub:65}
Der Beweis von (ii) zeigt auch: Sei $X$ gleichmäßig konvex und $(x_n)_\mathds{N} \subset X$ eine Folge mit $\limsup_n \norm{x_n}\le 1$ und 
$\lim_{ n,m \to \infty} \norm{\frac{1}{2} (x_n+x_m)}=1$. Dann gilt $\lim_{ n \to \infty} \norm{x_n}=1$ und $(x_n)_{\mathds{N}}$ ist Cauchy.
% subsection 65 (end)

\subsection{Proposition: Jensensche Ungleichung} % (fold)
\label{sub:66}
Für $\alpha,\beta \ge 0$, $p \ge r >0$ gilt\index{Jensen-Ungleichung} 
\[
	(\alpha^p + \beta^p)^{\frac{1}{p}} \le \enbrace*{\alpha^r + \beta^r}^{\frac{1}{r}}.
\]
\minisec{Beweis}
Setze $A := \enbrace*{\alpha^r +\beta^r}^{\frac{1}{r}}$, wobei wir o.B.d.A. annehmen können, dass $A \not= 0$ ist. Dann gilt 
\[
	1=\frac{1}{A^r} \cdot \enbrace*{\alpha^r+\beta^r} = \enbrace[\bigg]{\underbracket[0.6pt][1pt]{\frac{\alpha}{A}}_{\le 1}}^r + 
	\enbrace[\bigg]{\underbracket[0.6pt][1pt]{\frac{\beta}{A}}_{\le 1}}^r
	\ge \enbrace[\bigg]{\frac{\alpha}{A} }^p + \enbrace[\bigg]{\frac{\beta}{A} }^p    
\]
Es folgt $\enbrace*{\frac{1}{A^p} \cdot \enbrace*{\alpha^p + \beta^p}}^{\frac{1}{p} } \le 1 = \enbrace*{\frac{1}{A^r} \enbrace*{\alpha^r+ \beta^r}}^{\frac{1}{r} }$. 
Kürzen von $\frac{1}{A}$ liefert die Behauptung. \bewende
% subsection 66 (end)

\subsection[Proposition: Zwei Abschätzungen für die $p$-Norm in $L^p(\mu)$]{Proposition} % (fold)
\label{sub:67}
Sei $(X,\Sigma,\mu)$ ein Maßraum, $1 < p,q < \infty$ mit $\frac{1}{p} +\frac{1}{q}=1$ und $f,g \in L^p(\mu)$.
\begin{enumerate}[(i)]
	\item Falls $p\ge 2$, so gilt $\norm[p]{f+g}^p + \norm[p]{f-g}^p \le 2^{p-1} \enbrace*{\norm[p]{f}^p + \norm[p]{g}^p}$.
	\item Falls $p < 2$, so gilt $\norm[p]{f+g}^q + \norm[p]{f-g}^q \le 2 \cdot \enbrace*{\norm[p]{f}^p + \norm[p]{g}^p}^{q-1}$.
\end{enumerate}
\minisec{Beweis}
\begin{enumerate}[(i)]
	\item Nach der \hyperref[sub:66]{Jensenschen Ungleichung (\ref*{sub:66})} gilt für $\gamma,\delta  \in \mathds{R}$
	\begin{equation*}
		\enbrace[\Big]{\abs*{\gamma+\delta }^p + \abs*{\gamma-\delta }^p}^{\frac{1}{p}} \le \enbrace*{\abs*{\gamma+\delta }^2 + \abs*{\gamma-\delta}^2}^{\frac{1}{2}}
		= \sqrt{2} \cdot \enbrace*{\gamma^2 + \delta^2}^{\frac{1}{2}} \label{eq:67} \tag{*}
	\end{equation*}
	Wir betrachten nun zunächst den Fall $p>2$. Setze $r := \frac{p}{p-2}$. Dann gilt $\frac{1}{\frac{p}{2}} + \frac{1}{r}=1$. Die \hyperref[sub:45]{Höldersche Ungleichung 
	(\ref*{sub:45})} für $\mathds{R}^2$ liefert für $(\gamma^2,\delta^2)$ und $(1,1)$
	\[
		\gamma^2 \cdot 1 + \delta^2 \cdot 1 \le \enbrace*{\enbrace{\gamma^2}^{\frac{p}{2} } + \enbrace*{\delta^2}^{\frac{p}{2}} }^{\frac{2}{p}} 
		\cdot (1^r+ 1^r)^{\frac{1}{r}} = \enbrace[\big]{\abs*{\gamma}^p + \abs*{\delta}^p}^{\frac{2}{p}} \cdot 2^{\frac{p-2}{p}} 
	\]
	Es folgt 
	\begin{align*}
		\sqrt{2} \cdot \enbrace*{\gamma^2+ \delta^2}^{\frac{1}{2}} \le 2^{\frac{1}{2}} \cdot \enbrace[\big]{\abs*{\gamma}^p + \abs*{\delta }^p}^{\frac{1}{p}} \cdot 2^{\frac{p-2}{2p} } = 2^{\frac{p-1}{p}} \cdot \enbrace[\big]{\abs*{\gamma}^p +\abs*{\delta}^p}^{\frac{1}{p} } 
	\end{align*}
	Mit \eqref{eq:67} folgt insgesamt $\abs*{\gamma+\delta}^p + \abs*{\gamma-\delta}^p \le 2^{p-1} \cdot  \enbrace[\big]{\abs*{\gamma}^p + \abs*{\delta}^p}$. 
	Integration liefert nun (i).
	Für $p=2$ gilt Gleichheit in \eqref{eq:67} und auch hier liefert Integration das gewünschte Ergebnis.
	\item \emph{Übung!} \bewende
\end{enumerate}
% subsection 67 (end)

\subsection[Satz: Für $1 <p <\infty$ ist $L^p(\mu)$ gleichmäßig konvex]{Satz} % (fold)
\label{sub:68}
Sei $(X,\Sigma,\mu)$ ein Maßraum und $1 <p < \infty$. Dann ist $L^p(\mu)$ gleichmäßig konvex.
\minisec{Beweis}
Seien $(f_n)_\mathds{N}, (g_n)_\mathds{N} \subset L^P(\mu)$ Folgen mit $\norm[p]{f_n}= \norm[p]{g_n}= 1 $ für $n \in \mathds{N}$ und 
$\lim_{ n \to \infty} \norm[p]{\frac{1}{2} (f_n+g_n)}=1$. Wir zeigen, dass \ref{sub:61}(iii) gilt, also $\norm[p]{f_n-g_n} \xrightarrow{n \to \infty} 0$. Nach \ref{sub:67}
gilt 
\[
	\underbrace{\norm[p]{f_n+g_n}^p}_{\xrightarrow{n \to \infty} 2^p} + \norm[p]{f_n-g_n}^p \le 2^{p} 
\]
falls $p\ge 2$. Für $p < 2$ erhalten wir mit $q>0$, sodass $\frac{1}{p} +\frac{1}{q}=1$
\[
	\underbrace{\norm[p]{f_n+g_n}^q }_{\xrightarrow{n \to \infty} 2^q}+ \norm[p]{f_n-g_n}^q \le 2^q
\]
Also folgt $\norm[p]{f_n -g_n}^p \xrightarrow{n \to \infty} 0$ bzw. $\norm[p]{f_n-g_n}^q \xrightarrow{n \to \infty} 0$. Also gilt 
$\norm[p]{f_n -g_n} \xrightarrow{n \to \infty} 0$. \bewende
% subsection 68 (end)

\subsection[Satz: Gleichmäßig konvexe Banachräume sind reflexiv]{Satz} % (fold)
\label{sub:69}
Sei $X$ ein gleichmäßig konvexer Banachraum. Dann ist $X$ reflexiv.
\minisec{Beweis}
Sei $f \in X^{**}$ mit $\norm[X^{**}]{f}=1$. Zu zeigen: $f \in \iota(X)$. Wir wählen $\varphi_n \in X^*$ mit $\norm[X^*]{\varphi_n}=1$ und $f(\varphi_n) > 1- \frac{1}{n+1}$
für $n \in \mathds{N}$. Nach dem Beweis von \ref{sub:516} gilt für $n \in \mathds{N}$
\[
	\inf_{x \in \overline{B}_X(0,1) } \sum_{i=0}^{n} \,\abs[\big]{f(\varphi_i) - \varphi_i(x)}^2 =0
\]
Also gibt es eine Folge $(x_n)_{n \in \mathds{N}} \subset X$, sodass $\norm[X]{x_n}=1$ und für $0 \le m \le n$\marginnote{Frag mich nicht, wie man auf die zweite 
Abschätzung kommt \ldots }
\[
	\abs*{f(\varphi_m)- \varphi_m(x_n)} < \frac{1}{2(n+1)} \quad \text{also auch } \quad 1- \frac{3}{2(m+1)}\le \varphi_m(x_n) \le 1
\]
Behauptung: $(x_n)_\mathds{N}$ ist Cauchy. Es gilt für $n \ge m$
\begin{align*}
	\underbrace{1- \frac{3}{2(m+1)}  + 1 - \frac{3}{2(m+1)}}_{\xrightarrow{n,m \to \infty}\, 2} \le \varphi_m(x_m) + \varphi_m(x_n) = \varphi_m(x_m+x_n) &\le \norm{\varphi_m} \cdot \norm[X]{x_m+x_n} \\[-1.2em]
	&\le \norm[X]{x_m} + \norm[X]{x_n} = 2  
\end{align*}
Also gilt $ \norm[X]{\frac{1}{2} (x_m+x_n)} \xrightarrow{n,m \to \infty} 1$. Mit Bemerkung \ref{sub:65} folgt nun, dass $(x_n)_\mathds{N}$ Cauchy ist. Sei 
$\overline{x} := \lim_{ n \to \infty} x_n$. Es gilt $\norm[X]{\overline{x}}=1$. Behauptung: $\iota_X(\overline{x})=f$. Für $n \ge m$ gilt
\begin{align*}
	\abs[\Big]{f(\varphi_m) - \underbrace{\varphi_m(x_n)}_{\xrightarrow{n \to \infty} \varphi_m(\overline{x} )}} < \frac{1}{2(n+1)}  
\end{align*}
$\varphi_m$ ist stetig, also ist $f(\varphi_m)=\varphi_m(\overline{x})= \iota(\overline{x})(\varphi_m)$ für jedes $m \in\mathds{N}$. Sei $\tilde{x} \in X$ ein weiterer Punkt
mit $\norm{\tilde{x}}=1$ und $f(\varphi_m) = \varphi_m(\tilde{x})$, $m \in \mathds{N}$. Das obige Argument für die Folge 
$(\overline{x}, \tilde{x}, \overline{x}, \tilde{x}, \ldots)$ liefert: $(\overline{x}, \tilde{x}, \overline{x}, \tilde{x}, \ldots)$ ist Cauchy und somit
$\overline{x}=\tilde{x}$.

Noch zu zeigen: $f(\varphi)= \varphi(\overline{x})$ für alle $\varphi \in X^*$. Das obige Argument für $(\varphi, \varphi_1, \varphi_2, \ldots )$ an Stelle von
$(\varphi_0, \varphi_1, \varphi_2,\ldots )$ liefert eine Folge $(y_n)_\mathds{N} \subset X$ mit $\norm{y_n}=1$, $(y_n)_\mathds{N}$ ist Cauchy; 
Dann erfüllt $\overline{y} := \lim_{ n \to \infty} y_n$ die Gleichung $f(\varphi_m)=\varphi_m(\overline{y})$ für alle $m \in \mathds{N}$. Also gilt auch 
$f(\varphi)=\varphi(\overline{y})$. Die Eindeutigkeit von $\overline{x}$ liefert $ \overline{y}=\overline{x}$. Damit folgt 
$\iota(\overline{x})(\varphi)= \varphi(\overline{x})= \varphi(\overline{y})= f(\varphi)$ für jedes $\varphi \in X^*$, also ist $\iota(\overline{x})=f$. \bewende
% subsection 69 (end)

\subsection[Corollar: $L^p(\mu)$ ist für $1<p<\infty$ reflexiv.]{Corollar} % (fold)
\label{sub:610}
Für einen Maßraum $(X,\Sigma, \mu)$ und $1 < p< \infty$ ist $L^p(\mu)$ reflexiv.
\minisec{Beweis}
Kombiniere die Aussagen aus \ref{sub:68} und \ref{sub:69}. \bewende
% subsection 610 (end)

\subsection[Satz: Isometrischer Isomorphismus $\iota_p \colon L^p(\mu) \to L^q(\mu)^*$]{Satz} % (fold)
\label{sub:611}
Sei $(X,\Sigma,\mu)$ ein Maßraum und $\frac{1}{p} + \frac{1}{q}=1$. Dann ist die Abbildung $\iota_p \colon L^p(\mu) \to L^q(\mu)^*$ gegeben durch 
\[
	\iota_p(f)(g) := \int \!f g \,\mathd\mu
\] 
ein isometrischer Isomorphismus.
\minisec{Beweis}
$\iota_p$ ist eine lineare Isometrie nach Proposition \ref{sub:410}, wir müssen also nur Surjektivität zeigen. $\iota_p \enbrace*{L^p(\mu)}$ ist vollständig, also 
abgeschlossen in $L^q(\mu)^*$. Falls $\iota_p(L^p(\mu)) \subsetneq L^q(\mu)^*$, so existiert nach \hyperref[sub:227]{Hahn-Banach} ein $0 \not= \psi \in L^q(\mu)^{**}$ mit
$\psi \enbrace[\big]{\iota_p(L^p(\mu))} =0$. $L^q(\mu)$ ist reflexiv nach \ref{sub:610}, also existiert $g \in L^q(\mu)$ mit $\iota(g)=\psi$, d.h. 
$\varphi(g)= \iota(g)(\varphi)=\psi(\varphi)$ für alle $\varphi \in L^q(\mu)^*$. Für jedes $f\in L^p(\mu)$ gilt dann
\[
	0 = \psi \enbrace[\big]{\iota_p(f)} = \iota_p(f)(g) = \int\! f g \, \mathd\mu \enspace \Longrightarrow \enspace g=0 \enspace\light
\]
Also kann ein solches $\psi$ nicht existieren und es folgt die Surjektivität von $\iota_p$. \bewende
% subsection 611 (end)

\subsection[Bemerkung zur Gültigkeit von Satz \ref{sub:611} für $p\in \set{1,\infty}$]{Bemerkung} % (fold)
\label{sub:612}
\begin{itemize}
	\item Weiter kann man zeigen, dass die kanonische Abbildung\marginnote{Ich meine irgendwo gelesen zu haben, dass dafür das Maß aber $\sigma$-endlich sein muss} 
	\[
		\iota_\infty \colon L^\infty(\mu) \to L^1(\mu)^* \enspace,  \quad \iota_\infty(f)(g) = \int\! fg \, \mathd\mu
	\]
	ein isometrischer Isomorphismus ist. \hfill \emph{(ohne Beweis)}
	\item Die Isometrie $\iota_1 \colon L^1(\mu) \to L^\infty(\mu)^*$ ist jedoch \emph{nicht} surjektiv! \hfill \emph{(Übung, Blatt 10, Aufgabe 1)}
\end{itemize}
% subsection 612 (end)

% section 6 (end)
\newpage
\section{Hilberträume und selbstadjungierte Operatoren} % (fold)
\label{sec:7}

\subsection[Definition: Hermitesche Form, Skalarprodukt]{Definition} % (fold)
\label{sub:71}
Sei $X$ ein $\mathds{K}$-Vektorraum. Eine \Index{hermitesche Form} auf $X$ ist eine Abbildung $\skal*{\cdot}{\cdot} \colon X \times X \to \mathds{K}$ mit
\begin{align*}
	\skal*{x}{y+y'} &= \skal*{x}{y} + \skal*{x}{y'} \marginnote{linear im zweiten Eintrag}\\
	\skal*{x}{\alpha \cdot y} &= \alpha \cdot \skal*{x}{y} \\
	\skal*{x}{y} &= \overline{\skal*{y}{x}} \qquad \text{ für } x,y,y' \in X, \alpha \in \mathds{K}       
\end{align*}
$\skal*{\cdot}{\cdot } $ heißt \bet{positiv semidefinit}\index{hermitesche Form!positiv semidefinit}, falls $\skal*{x}{x}\ge 0$ für $x \in X$ und 
\bet{positiv definit}\index{hermitesche Form!positiv definit}, falls 
$\skal*{x}{x} >0$ für $0 \not= x \in X$. In diesem Fall heißt $\skal*{\cdot }{\cdot }$ auch \Index{Skalarprodukt}.  
% subsection 71 (end)

\subsection[Bemerkung: Einfache Eigenschaften von hermiteschen Formen]{Bemerkung} % (fold)
\label{sub:72}
Für eine hermitesche Form $\skal*{\cdot}{\cdot}$ und $x,x',y \in X$, $\alpha \in \mathds{K}$ gilt 
\begin{align*}
	\skal*{x+x'}{y} = \skal*{x}{y} + \skal*{x'}{y} \quad , \quad \qquad \marginnote{konjugiert linear im ersten Eintrag}
	\skal*{\alpha \cdot x}{y} = \overline{\alpha} \cdot \skal*{x}{y} \qquad \text{und} \qquad 
	\skal*{x}{x} \in \mathds{R} 
\end{align*}
% subsection 72 (end)

\subsection[Beispiele für hermitesche Formen]{Beispiele} % (fold)
\label{sub:73}
\begin{enumerate}[(i)]
	\item Auf $\mathds{C}^n$ definiert $\skal*{\underline{x}}{\underline{y}} := \sum_{i=1}^{n} \overline{x}_i \cdot y_i$ ein Skalarprodukt.
	\item Auf $L^2(\mathds{R})$ definiert $\skal*{f}{g} := \int\! f  g \,\mathd\lambda$ ein Skalarprodukt.
	Auf\marginnote{wohldefiniert nach Hölder}
	\[
		L^2_\mathds{C}(\mathds{R}) := \set[f \colon \mathds{R} \to \mathds{C}]{\re f \text{ und } \im f \text{ messbar}, \int \! \bar{f} f\, \mathd\lambda < \infty } / \mathcal{N}
	\]
	definiert $\skal*{f}{g} := \int\! \bar{f} g \, \mathd\lambda$ ein Skalarprodukt.
	\item Auf $\ell^2_{\color{gray}\mathds{C}}(\mathds{N})$ definieren wir ein Skalarprodukt durch $\skal*{(a_n)}{(b_n)} := \sum_{n \in \mathds{N}} \overline{a}_n b_n$.
\end{enumerate}
% subsection 73 (end)

\subsection{Proposition (Cauchy-Schwarz-Ungleichung)} % (fold)
\label{sub:74}
Sei $X$ ein $\mathds{K}$-Vektorraum mit einer positiv semidefiniten hermiteschen Form. Dann gilt für $x,y \in X$ \index{Cauchy-Schwarz-Ungleichung}
\[
	\abs[\big]{\skal*{x}{y}}^2 \le \skal*{x}{x} \cdot \skal*{y}{y}
\]
\minisec{Beweis}
Für $\lambda \in \mathds{K}$, $x,y \in X$ gilt: 
\begin{align*}
	0 \le \skal*{x+\lambda y}{x + \lambda y} = \skal*{x}{x} + \lambda \cdot \skal*{x}{y} + \overline{\lambda} \cdot \skal*{y}{x} 
	+ \lambda \cdot \overline{\lambda} \cdot \skal*{y}{y}  
\end{align*}
Falls $\skal*{y}{y} \not=0$, so setze $\lambda := - \frac{\skal*{y}{x}}{\skal*{y}{y}}$. Es folgt 
\begin{align*}
	0 \le \skal*{x}{x} - \frac{\skal*{y}{x} \cdot \skal*{x}{y}}{\skal*{y}{y}} + \overline{\lambda} \cdot \enbrace*{\skal*{y}{x}  
	- \frac{\skal*{y}{x}}{\skal*{y}{y}} \skal*{y}{y}   } = \skal*{x}{x} - \frac{\abs*{\skal*{x}{y}}^2 }{\skal*{y}{y}} 
\end{align*}
Falls $\skal*{x}{x} \not= 0$, so vertausche die Rollen von $x$ und $y$. Falls $\skal*{x}{x}=\skal*{y}{y}=0$ gilt, so setzen wir $\lambda := - \skal*{y}{x}$ und erhalten
\[
	0 \le - \skal*{y}{x} \cdot \skal*{x}{y} - \overline{\skal*{y}{x}} \cdot \skal*{y}{x} = -2 \cdot \abs*{\skal*{x}{y}}^2 
\]
Also muss in diesem Fall bereits $\abs*{\skal*{x}{y}}^2 =0$ und somit die gewünschte (Un-)Gleichung gelten. \bewende
% subsection 74 (end)

\subsection[Proposition: Ein Skalarprodukt definiert eine Norm]{Proposition} % (fold)
\label{sub:75}
Sei $X$ ein $\mathds{K}$-Vektorraum mit Skalarprodukt. Dann definiert $x \mapsto \norm{x} := \skal*{x}{x}^{\frac{1}{2}}$ eine Norm auf $X$.
\minisec{Beweis}
$\skal*{\cdot}{\cdot}$ ist positiv definit, also ist auch $\norm{\cdot}$ positiv definit. Weiter gilt
\begin{align*}
	\norm{\lambda \cdot x} = \skal*{\lambda \cdot x}{\lambda \cdot x}^{\frac{1}{2}} \,&= \enbrace*{\overline{\lambda} \lambda \cdot \skal*{x}{x}}^{\frac{1}{2}} 
	= \abs*{\lambda} \cdot \norm{x}    \\
	\intertext{Die Dreiecksungleichung gilt, da}
	\norm{x+y}^2 = \skal*{x+y}{x+y} &= \skal*{x}{x} + \skal*{x}{y} + \skal*{y}{x} + \skal*{y}{y} \\
	&\stackrel{\mathclap{\mbox{\scriptsize \hyperref[sub:74]{C.S.}}}}{\le} \skal*{x}{x} + 2 \cdot \enbrace[\big]{\skal*{x}{x} \cdot \skal*{y}{y}}^{\frac{1}{2}}
	+ \skal*{y}{y}  \\
	&= \norm{x}^2 + 2 \cdot \norm{x} \cdot \norm{y} + \norm{y}^2 = \enbrace[\big]{\norm{x} + \norm{y}}^2    
\end{align*}
Durch Wurzelziehen folg $\norm{x+y} \le \norm{x} + \norm{y}$. \bewende
% subsection 75 (end)

\subsection[Bemerkungen über Cauchy-Schwarz und die Stetigkeit von $\skal*{\cdot}{\cdot}$]{Bemerkung} % (fold)
\label{sub:76}
\begin{enumerate}[(i)]
	\item Die Cauchy-Schwarz-Ungleichung schreibt sich auch als $\abs*{\skal*{x}{y}} \le \norm{x} \cdot \norm{y}$.
	\item Die Abbildung $\skal*{\cdot}{\cdot} \colon X \times X \to \mathds{K}$ ist stetig:
	\begin{align*}
		\abs[\big]{\skal*{x}{y}- \skal*{x'}{y'}} = \abs[\big]{\skal*{x}{y} -\skal*{x'}{y} + \skal*{x'}{y} -\skal*{x'}{y'}} &= \abs[\big]{\skal*{x-x'}{y} +\skal*{x'}{y'-y}}\\
		&\le \abs[\big]{\skal*{x-x'}{y}} + \abs[\big]{\skal*{x'}{y'-y}} \\
		&\stackrel{\mathclap{\mbox{\scriptsize \hyperref[sub:74]{C.S.}}}}{\le} \norm{x-x'} \cdot \norm{y} + \norm{x'} \cdot \norm{y'-y}    
	\end{align*}
	% \ldots \le \norm{x-x'} \cdot \norm{y} + \norm{x'} \cdot \norm{y'-y}.   
\end{enumerate}
% subsection 76 (end)

\subsection[Proposition: Zusammenhang Norm $\leftrightarrow$ Skalarprodukt mit dem Parallelogrammgesetz]{Proposition} % (fold)
\label{sub:77}
\begin{figure}[b]
	\centering{
	\begin{tikzpicture}
		\coordinate (0) at (0,0);
		\coordinate (x) at (2,1.4);
		\coordinate (y) at (3,0);
		\coordinate (x+y) at ($(x)+(y)$);
		
		\draw[thick, gray, dashed] (0) -- (x+y);
		\draw[thick, gray, dashed] (y) -- (x);
		\draw[thick] (0,0) -- (x) -- (x+y) -- (y) -- cycle;
		\foreach \x/\pos in {0/below left,x/above left,y/below right,x+y/above right}{
			\filldraw (\x) circle[radius=0.04] node[\pos]{$\x$};
		}
	\end{tikzpicture}
	\caption[Die Parallelogrammgleichung in $\mathds{C}\cong \mathds{R}^2$]{Die Parallelogrammgleichung in $\mathds{C}\cong \mathds{R}^2$. Die gestrichelten Linien sind
	zusammen genauso lang wie die durchgezogenen. In $\mathds{C}$ lässt sich die Gleichung sehr einfach mittels $\abs*{z}^2= z \cdot \overline{z}$ beweisen. Ein andere 
	einfache Möglichkeit ist der Kosinussatz aus der Schule; \hrefsym{https://de.wikipedia.org/wiki/Parallelogrammgleichung}{Wikipedia-Link}.}
	}
\end{figure}
Für einen normierten Raum $X$ sind äquivalent:
\begin{enumerate}[(i)]
	\item Es gilt das \Index{Parallelogrammgesetz}: $\norm{x+y}^2 + \norm{x-y}^2 = 2 \enbrace*{\norm{x}^2 + \norm{y}^2}$ für alle $x,y \in X$.
	\item \[
		\skal*{x}{y} := \begin{cases}
			\frac{1}{4} \enbrace*{\norm{x+y}^2 - \norm{-x+y}^2 + i \cdot \norm{i \cdot x +y}^2 - i \cdot \norm{-i \cdot x+ y}^2}, &\text{ falls }\mathds{K}=\mathds{C}\\
			\frac{1}{4} \enbrace*{\norm{x+y}^2 - \norm{x-y}^2}, &\text{ falls }\mathds{K}=\mathds{R}
		\end{cases}
	\]
	definiert ein Skalarprodukt auf $X$. In diesem Fall gilt
	$\skal*{x}{x} = \norm{x}^2$ für alle $x \in X$; insbesondere ist $\skal*{\cdot}{\cdot}$ stetig auf $X \times X$.
\end{enumerate}
\minisec{Beweis}
\begin{description}
	\item[(ii)$\Rightarrow$(i):] Wie man leicht nachrechnet gilt $\norm{x}^2 = \skal*{x}{x}$ für $x \in X$. (i) folgt ebenfalls durch eine einfache Rechnung.
	\item[(i)$\Rightarrow$(ii):] $\skal*{\cdot}{\cdot} \colon X \times X \to X$ ist stetig (klar). Linearität (für $\mathds{K}=\mathds{C}$): Es gilt
	\begin{gather*}
		\skal*{x}{y+y'} = \frac{1}{4} \enbrace*{\norm{x+y+y'}^2 - \norm{-x+y+y'}^2 + i  \norm{i x + y+ y'}^2 - i \norm{- ix +y+y'}^2} \label{eq:77:1}\tag{*} \\ 
		\begin{split}
			\skal*{x}{y} + \skal*{x}{y'} = \frac{1}{4} \Big( &\norm{x+y}^2 - \norm{-x+y}^2 + i  \norm{ix +y}^2 - i  \norm{-ix +y}^2 + \\
			&\norm{x+y'}^2 - \norm{-x+y'} + i  \norm{ix + y'} - i  \norm{-ix +y'} \Big)
		\end{split}
		\label{eq:77:2}\tag{**}
	\end{gather*}
	Vergleich der Imaginärteile: Es gilt nach (i) sowohl
	\begin{align*}
		\norm{x+y+y'}^2 &= - \norm{x+y-y'}^2 + 2 \norm{x+y}^2 + 2 \norm{y'}  \\
		\shortintertext{als auch}
		\norm{x+y+y'}^2 &= - \norm{x-y+y'}^2 + 2 \norm{x+y'} + 2 \norm{y}^2 
	\end{align*}
	Wir multiplizieren beide Gleichungen mit $\frac{1}{2}$, addieren diese und erhalten
	\[
		\norm{x+y+y'}^2= - \frac{1}{2} \enbrace*{\norm{x+y-y'}^2 + \norm{x-y+y'}^2} + \norm{x+y}^2 + \norm{x+y'}^2 + \norm{y'}^2 + \norm{y}^2         
	\]
	Wir substituieren $x$ mit $-x$ und ziehen die Gleichungen voneinander ab
	\[
		\norm{x+y+y'}^2 - \norm{-x+y+y'}^2 = \norm{x+y}^2 + \norm{x+y'}^2 -\norm{-x+y}^2 - \norm{-x+y'}^2
	\]
	Also stimmen die Realteile von \eqref{eq:77:1} und \eqref{eq:77:2} überein. Der Beweis für die Imaginärteile erfolgt analog. Es gilt also
	$\skal*{x}{y} + \skal*{x}{y'}= \skal*{x}{y+y'}$. Daraus folgt
	\begin{align*}
		\skal*{x}{\alpha \cdot y} &= \alpha \cdot \skal*{x}{y} && \text{ für $\alpha \in \mathds{N}\setminus \set{0} $} \\
		\Longrightarrow \skal*{x}{\alpha \cdot y} &= \alpha \cdot \skal*{x}{y} && \text{ für $\alpha \in \mathds{Z}$ ($\skal*{x}{0}=0$ ist trivial)}
	\end{align*}
	Es gilt nun für $\alpha=\frac{m}{n} \in \mathds{Q}$
	\[
		\skal*{x}{\alpha \cdot y} = \frac{n}{n} \cdot \skal*{x}{\frac{m}{n} \cdot y } = \frac{m}{n} \cdot \skal*{x}{\frac{n}{n}\cdot y } = \alpha \cdot \skal*{x}{y}  
	\]
	Mit der Stetigkeit von $\skal*{\cdot}{\cdot}$ folgt nun $\skal*{x}{\alpha \cdot y} = \alpha \cdot \skal*{x}{y}$ auch für $\alpha \in \mathds{R}$.
	$\skal*{x}{i \cdot y} = i \cdot \skal*{x}{y}$ ist klar und damit ist $\skal*{x}{\alpha \cdot y }= \alpha \cdot \skal*{x}{y}$ für $\alpha \in \mathds{C}$. Also ist
	$\skal*{\cdot}{\cdot}$ linear in der zweiten Variablen. $\skal*{x}{y} = \overline{\skal*{y}{x}}$ ist trivial. \bewende
\end{description}
% subsection 77 (end)

\subsection[Bemerkung: Polarisierungsidentität]{Bemerkung} % (fold)
\label{sub:78}
Ist $X$ ein $\mathds{C}$-Vektorraum und $q \colon X \times X \to \mathds{C}$ eine \Index{Sesquilinearform} (d.h. $q$ ist linear in der zweiten Variable und konjugiert 
linear in der ersten), dann ist $q$ durch die Abbildung $n_q \colon X \to \mathds{C}$, $n_q(x) := q(x,x)$ eindeutig bestimmt durch die \Index{Polarisierungsidentität}
\[
	q(x,y) = \frac{1}{4} \enbrace[\Big]{n_q(x+y) - n_q(-x+y) + i \cdot n_q(i x +y) - i \cdot n_q(-ix +y)}.  
\]
% subsection 78 (end)

\subsection[Definition: Prä-Hilbertraum und Hilbertraum]{Definition} % (fold)
\label{sub:79}
Ein $\mathds{K}$-Vektorraum $\mathcal{H}$ heißt \Index{Prä-Hilbertraum}, falls $\mathcal{H}$ ein Skalarprodukt besitzt. $\mathcal{H}$ heißt \Index{Hilbertraum}, falls
$\mathcal{H}$ vollständig ist bezüglich der durch $\skal*{\cdot}{\cdot}$ induzierten Norm.
% subsection 79 (end)

\subsection[Proposition: Die Vervollständigung eines Prä-Hilbertraumes ist ein Hilbertraum]{Proposition} % (fold)
\label{sub:710}
Sei $\mathcal{H}$ ein Prä-Hilbertraum und $\overline{\mathcal{H}}^{\norm{\cdot}}$ die Vervollständigung bezüglich $\norm{x} = \skal*{x}{x}^{\frac{1}{2}}$. Dann ist
$\overline{\mathcal{H}}^{\norm{\cdot}}$ in kanonischer Weise ein Hilbertraum.
\minisec{Beweis}
$\enbrace{\mathcal{H}, \norm{\cdot}}\hookrightarrow\enbrace{\overline{\mathcal{H}}^{\scriptscriptstyle\norm{\cdot}}, 
\norm[\scriptscriptstyle\overline{\mathcal{H}}^{\norm{\cdot}}]{\cdot}}$ ist isometrisch mit dichtem Bild nach \hyperref[sub:15]{Satz \ref*{sub:15}}. $\norm{\cdot}$ erfüllt 
das Parallelogrammgesetz und daher auch $\norm[\scriptscriptstyle\overline{\mathcal{H}}^{\norm{\cdot}}]{\cdot}$. Mit \ref{sub:77} folgt, dass 
$\overline{\mathcal{H}}^{\scriptscriptstyle\norm{\cdot}}$ ein Skalarprodukt besitzt, welches die Norm 
$\norm[\scriptscriptstyle\overline{\mathcal{H}}^{\norm{\cdot}}]{\cdot}$ induziert und auf $\mathcal{H}$ mit $\skal*{\cdot}{\cdot}$ übereinstimmt. Das Skalarprodukt auf 
$\overline{\mathcal{H}}^{\scriptscriptstyle\norm{\cdot}}$ ist wegen Stetigkeit hierdurch eindeutig bestimmt. \bewende
% subsection 710 (end)

\subsection[Proposition: Isometrischer Isomorphismus $\kappa \colon \mathcal{H} \to \mathcal{H}^*$]{Proposition} % (fold)
\label{sub:711}
Sei $\mathcal{H}$ ein Hilbertraum und $\kappa \colon \mathcal{H} \to \mathcal{H}^*$ definiert durch $\kappa(x)(y) := \skal*{x}{y}$, $x,y \in \mathcal{H}$. Dann gilt:
\marginnote{Diese Aussage wird manchmal als Rieszscher Darstellungssatz bezeichnet}
\begin{enumerate}[(i)]
	\item $\kappa$ ist \Index{konjugiert linear}, das heißt $\kappa(\alpha x+ \alpha' x') = \overline{\alpha} \kappa(x) + \overline{\alpha'}\kappa(x')$ mit 
	$x,x' \in \mathcal{H}$, $\alpha, \alpha' \in \mathds{K}$.
	\item $\kappa$ ist isometrisch, also insbesondere injektiv.
	\item $\kappa$ ist surjektiv.
\end{enumerate}
\minisec{Beweis}
Es gilt $\kappa(x) \in \mathcal{H}^*$, da $\kappa(x) \colon \mathcal{H} \to \mathds{K}$ offensichtlich linear ist. Weiter ist $\kappa(x)$ stetig, da
\begin{align*}
	\abs*{\kappa(x)(y)}= \abs*{\skal*{x}{y}} \stackrel{\mathclap{\mbox{\scriptsize \hyperref[sub:74]{C.S.}}}}{\le} \norm{x} \cdot \norm{y} 
	\Longrightarrow \norm[\mathcal{H}^*]{\kappa(x)} \le \norm{x}  
\end{align*}
\begin{enumerate}[(i)]
	\item klar.
	\item Es gilt $\kappa(x) \enbrace*{\frac{1}{\norm{x}}x }= \frac{1}{\norm{x}} \skal*{x}{x} = \norm{x}$. Daraus folgt $\norm[\mathcal{H}^*]{\kappa(x)} \ge \norm{x}$, also
	auch $\norm[\mathcal{H}^*]{\kappa(x)}= \norm{x}$, da wir die andere Abschätzung eben gezeigt haben.
	\item Sei $\varphi \in \mathcal{H}^*$. Ohne Einschränkungen gelte $\norm[\mathcal{H}^*]{\varphi}=1$. 
	Behauptung: Es gibt ein $\overline{x} \in \mathcal{H}$ mit $\norm{\overline{x}}=1 $ und
	$\varphi(\overline{x} )=1$. 
	
	Wähle dazu $(x_n)_\mathds{N} \subset \mathcal{H}$ mit $\norm{x_n}=1$ und $0\le \varphi(x_n) \xrightarrow{n \to \infty} 1$.
	Zu $1 >\varepsilon>0$ existiert ein $N \in \mathds{N}$ mit $\varphi(x_n) > 1- \frac{\varepsilon}{8} $ für $n \ge N$, also $\varphi(x_n +x_m) > 2- \frac{\varepsilon}{4}$.
	Aus dem Parallelogrammgesetz folgt nun für $n,m \ge N$
	\begin{align*}
		\norm{x_n -x_m}^2 = 2 \cdot \norm{x_n}^2 + 2 \cdot \norm{x_m}^2 - \norm{x_n +x_m}^2 \le 4 - \enbrace*{2-\frac{\varepsilon}{4} }^2 \le \varepsilon 
	\end{align*}
	Es folgt, dass $(x_n)_\mathds{N}$ eine Cauchyfolge ist. $\overline{x} := \lim_{ n \to \infty} x_n$ erfüllt die Behauptung.
	
	Wir zeigen nun, dass $\overline{x}$ ein Urbild von $\varphi$ unter $\kappa$ ist, das heißt für $y \in \mathcal{H}$ ist $\varphi(y)= \kappa(\overline{x})(y)$. Sei 
	$\mathds{K}=\mathds{R}$. Für $\lambda>0$ gilt wegen der Dreiecksungleichung und $\norm[\mathcal{H}*]{\varphi}\le 1$
	\begin{align*}
		-\frac{1}{\lambda} \enbrace[\big]{\norm{\overline{x} - \lambda \cdot y } - \norm{\overline{x}}} \le -\frac{1}{\lambda} \enbrace[\big]{\varphi \enbrace*{\overline{x}- \lambda 
		\cdot y} - \varphi(\overline{x})} = \varphi(y) \\
		\shortintertext{und}
		\varphi(y) = \frac{1}{\lambda} \enbrace[\Big]{\varphi(\overline{x}+\lambda \cdot y) - \underbrace{\varphi(\overline{x})}_{=1}} 
		\le \frac{1}{\lambda} \cdot \enbrace[\Big]{\norm{\overline{x} + \lambda \cdot y } - \underbrace{\norm{\overline{x}}}_{=1} }    \\
		\shortintertext{also insgesamt}
		 \underbrace{-\frac{1}{\lambda} \enbrace[\Big]{\norm{\overline{x} - \lambda \cdot y } - \norm{\overline{x}}} }_{\xrightarrow{\lambda \to 0} \skal*{\overline{x}}{y}}
		 \le \varphi(y) 
		 \le \underbrace{\frac{1}{\lambda} \cdot \enbrace[\Big]{\norm{\overline{x} +\lambda\cdot y} - \norm{\overline{x}}}}_{\xrightarrow{\lambda \to 0} \skal*{\overline{x}}{y}}
	\end{align*}
	nach l'Hospital. Im Detail:
	\begin{align*}
		\norm{\overline{x} \pm \lambda y} - \norm{\overline{x}} = \skal*{\overline{x} \pm \lambda y }{\overline{x} \pm \lambda y }^{\frac{1}{2}} - \norm{\overline{x}}
		= \enbrace*{\skal*{\overline{x}}{\overline{x}} \pm 2 \cdot \skal*{\overline{x}}{\lambda y} + \lambda^2 \cdot \skal*{y}{y}}^{\frac{1}{2}} - 
		\skal*{\overline{x}}{\overline{x}}^{\frac{1}{2}}
	\end{align*}
	ist in $\lambda=0$ differenzierbar nach $\lambda$ mit 
	\begin{align*}
		\diffd{}{\lambda}(\ldots) &= \enbrace*{2 \skal*{\overline{x}}{y} \pm 2 \lambda \norm{y}^2} \cdot  
		\frac{1}{2} \enbrace[\Big]{\skal*{\overline{x}}{\overline{x}} \pm 2 \cdot \skal*{\overline{x}}{\lambda y} + \lambda^2 \cdot \skal*{y}{y}}^{-\frac{1}{2}} \\
		&\xrightarrow{\lambda \to 0} 2 \cdot \skal*{\overline{x}}{y} \cdot \frac{1}{2} \cdot \frac{1}{\norm{\overline{x}}} = \skal*{\overline{x}}{y}
	\end{align*}
	Wir erhalten $\varphi(y)= \skal*{\overline{x}}{y} = \kappa(\overline{x})(y)$, also $\varphi=\kappa(\overline{x})$. 
	
	Für $\mathds{K}=\mathds{C}$ stellen wir fest, dass $\mathcal{H}$ ein reeller Hilbertraum ist mit $\skal*{\cdot}{\cdot}_\mathds{R} := \re \enbrace*{\skal*{\cdot}{\cdot}}$. 
	Es gilt dann $\varphi(y) = \rho(y) - i \cdot \rho(i \cdot y)$ mit $\rho(\cdot ) := \re \enbrace*{\varphi(\cdot )}$, da folgendes gilt
	\[
		\im \enbrace[\big]{\varphi(y)} = - \re \enbrace[\big]{i \cdot \varphi(y)} = - \re \enbrace[\big]{\varphi(i \cdot y)} = - \rho (i \cdot y)   
	\]
	Analog zum reellen Fall ist dann $\rho(y) = \skal*{\overline{x}}{y}_\mathds{R} = \re \enbrace*{\skal*{\overline{x}}{y}}$ für ein $\overline{x} \in \mathcal{H}$. Also gilt
	\begin{align*}
		\varphi(y)= \rho(y)- i \cdot \rho(i y)= \skal*{\overline{x}}{y}_\mathds{R} - i \cdot \skal*{\overline{x}}{i y}_\mathds{R}
		=\re\enbrace[\big]{\skal*{\overline{x}}{y}} - i \cdot \im \enbrace[\big]{-\skal*{\overline{x}}{y}} = \skal*{\overline{x}}{y} \bewende
	\end{align*}
\end{enumerate}
% subsection 711 (end)

\subsection[Corollar: Hilberträume sind reflexiv]{Corollar} % (fold)
\label{sub:712}
Hilberträume sind reflexiv.
\minisec{Beweis}
Das Diagramm
\[
	\begin{tikzcd}[row sep=large, column sep=large]
		\mathcal{H} \rar["\iota_{\mathcal{H}}"] \dar["\kappa"] & \mathcal{H}^{**} \dar["\kappa^\tr"]\\
		\mathcal{H}^* \rar["\varphi \mapsto \overline{\varphi}"] & \mathcal{H}^*
	\end{tikzcd}
\]
kommutiert, da folgendes gilt:
\begin{align*}
	\kappa^\tr \circ \iota_{\mathcal{H}}(x)(y) = \kappa^\tr \enbrace*{\iota_{\mathcal{H}}(x)}(y)  =\iota_{\mathcal{H}}(x) \circ \kappa (y) 
	= \kappa(y)(x) = \skal*{y}{x} = \overline{\skal*{x}{y}} = \overline{\kappa(x)(y)} 
\end{align*}
Die Abbildungen $\kappa, \kappa^\tr$ und $(\varphi \mapsto \overline{\varphi})$ sind surjektive Isometrien,\marginnote{$\kappa^\tr$ nach Aufgabe 2, Blatt 6} also ist auch 
$\iota_{\mathcal{H}} \colon \mathcal{H} \to \mathcal{H}^{**}$ eine surjektive Isometrie.\bewende
% subsection 712 (end)

\subsection[Definition: Orthogonal, Orthonormalsystem und Hilbertraumbasis]{Definition} % (fold)
\label{sub:713}
Sei $\mathcal{H}$ ein Hilbertraum.
\begin{enumerate}[(i)]
	\item $x,y \in \mathcal{H}$ heißen \Index{orthogonal}, $x\perp y$, falls $\skal*{x}{y}=0$.
	\item $A,B  \subset \mathcal{H}$ heißen \Index{orthogonal}, falls $x \perp y$ für alle $x \in A$, $y \in B$. 
	\item Für $A \subset \mathcal{H}$ heißt $A^\bot := \set[x \in \mathcal{H}]{\skal*{x}{y}=0 \text{ für alle } y \in A}$ \Index{orthogonales Komplement} von $A$.
	\item $\set{x_i}_{I} \subset \mathcal{H}$ heißt \Index{Orthonormalsystem}, falls $\skal*{x_i}{x_j}= \delta_{ij}$ für $i,j \in I$.
	\item $\set{x_i}_{I} \subset \mathcal{H}$ heißt \Index{Hilbertraumbasis} (bzw. Basis), falls es ein maximales Orthonormalsystem ist. 
\end{enumerate}
% subsection 713 (end)

\subsection[Bemerkung: Satz von Pythagoras, Vergleich Hilbertraumbasis mit Vektorbasis]{Bemerkung} % (fold)
\label{sub:714}
\begin{enumerate}[(i)]
	\item Für $x \perp y \in \mathcal{H}$ gilt der \Index{Satz des Pythagoras}: $\norm{x+y}^2 = \norm{x}^2  + \norm{y}^2$.
	\item Wir werden sehen, dass jeder Hilbertraum eine Hilbertraumbasis besitzt.
	\item Im Allgemeinen ist eine Hilbertraumbasis \emph{keine} Vektorraumbasis (\Index{Hamelbasis}).
\end{enumerate}
% subsection 714 (end)

\subsection[Proposition: Für einen abgeschlossenen Unterraum $\mathcal{H}_0$ gilt $\mathcal{H}= \mathcal{H}_0 \oplus \mathcal{H}_0^\bot$]{Proposition} % (fold)
\label{sub:715}
Sei $\mathcal{H}$ ein Hilbertraum, $\mathcal{H}_0 \subset \mathcal{H}$ ein abgeschlossener Unterraum. Dann gilt $\mathcal{H}_0 \cap \mathcal{H}_0^\bot = \set{0}$ und 
$\mathcal{H}_0 + \mathcal{H}_0^\bot = \mathcal{H}$. Wir schreiben in diesem Fall auch $\mathcal{H} = \mathcal{H}_0 \oplus \mathcal{H}_0^\bot$ (\Index{Direkte Summe}).
\minisec{Beweis}
$\mathcal{H}_0 \cap \mathcal{H}_0^\bot= \set{0}$ ist klar. Sei $x \in \mathcal{H}$. Dann ist $\kappa(x) \in \mathcal{H}^*$ gegeben durch 
$\kappa(x)(y)=\skal*{x}{y}$ und es ist $\kappa(x)|_{\mathcal{H}_0} \in \mathcal{H}_0^*$. Nun ist $\mathcal{H}_0 \subset \mathcal{H}$ abgeschlossen und somit selbst ein 
Hilbertraum. Sei $\kappa_0 \colon \mathcal{H}_0 \to \mathcal{H}_0^*$ der konjugiert lineare, isometrische Isomorphismus aus \ref{sub:711}. Wegen der Surjektivität von 
$\kappa_0$ existiert $x_0 \in \mathcal{H}_0$ mit $\kappa_0(x_0)= \kappa(x)|_{\mathcal{H}_0}$. Das heißt für $y \in \mathcal{H}_0$ gilt 
\begin{align*}
	\skal*{x_0}{y} = \kappa_0(x_0)(y) = \kappa(x)\big|_{\mathcal{H}_0}(y) = \kappa(x)(y) = \skal*{x}{y}
\end{align*}
Daraus folgt $\skal*{x-x_0}{y}=0$ für jedes $y \in \mathcal{H}_0$, also ist $x_1 := x-x_0 \in \mathcal{H}_0^\bot$ und $x=x_0+x_1$. \bewende
% subsection 715 (end)

\subsection[Definition: Summierbare Familie (in einem Hilbertraum)]{Definition} % (fold)
\label{sub:716}
Sei $\mathcal{H}$ ein Hilbertraum. Eine Familie $\set{x_i}_{i \in I} \subset \mathcal{H}$ heißt \Index{summierbar} zu $x \in \mathcal{H}$, falls gilt: 
Zu jedem $\varepsilon>0$ existiert $\overline{J} \finSub I$ endlich, sodass für alle $\overline{J} \subset J \finSub I$ endlich gilt 
\[
	\norm{\sum_{i \in J}x_i - x } < \varepsilon.
\]
Wir schreiben dann auch $x= \sum_{i \in I} x_i$. (Warum kann es höchstens ein solches $x$ geben?) Formulierung mit Netzen: 
\[
	\enbrace*{\sum_{i \in J} x_i}_{\!\!\set[J]{J \finSub I \text{ endlich}}} \hspace{-4em} \longrightarrow \enspace x \qquad \text{ oder} \qquad \sum_{i \in J}x_i \xrightarrow{J \finSub I} x 
\]
Dabei ist $\set[J]{J \finSub I \text{ endlich}}$ gerichtet bezüglich Inklusion.
% subsection 716 (end)

\subsection[Proposition: Charakterisierungen von Summierbarkeit]{Proposition} % (fold)
\label{sub:717}
Sei $\mathcal{H}$ ein Hilbertraum und $\set{x_i}_{i \in I} \subset \mathcal{H}$ eine Familie in $\mathcal{H}$. 
\begin{enumerate}[(i)]
	\item $\set{x_i}_{i \in I}$ ist genau dann summierbar, wenn gilt: Zu jedem $\varepsilon>0$ existiert $\overline{J} \finSub I$ endlich, sodass 
	$\norm{\sum_{i \in J'}x_i}< \varepsilon$ für jedes $J' \finSub I$ endlich mit $J' \cap \overline{J} = \emptyset$.
	\item $\set{x_i}_{i \in I}$ ist genau dann summierbar zu $x \in \mathcal{H}$, wenn gilt: Es sind höchstens abzählbar viele der $x_i \not=0$ und für jede Abzählung 
	$x_{i_0}, x_{i_1}, \ldots $ dieser $x_i$ gilt 
	\[
		x= \lim_{n \to \infty} \sum_{k=0}^{n} x_{i_k}.
	\]
\end{enumerate}
\minisec{Beweis}
\begin{enumerate}[(i)]
	\item Angenommen $\sum_{i\in I} x_i \in \mathcal{H}$ existiert. Sei $\varepsilon>0$. Dann existiert $\overline{J}\finSub I$, sodass für $\overline{J}\subset J\finSub I$ 
	gilt $\norm{\sum_{i \in J} x_i - \sum_{i \in I} x_i} < \frac{\varepsilon}{2}$. Für $J' \finSub I$ mit $J' \cap \overline{J}=\emptyset$ gilt dann
	\begin{align*}
		\norm{\sum_{i \in J'} x_i} = \norm{\sum_{i \in J' \cup \overline{J}} x_i - \sum_{i \in \overline{J}} x_i}  \le \norm{\sum_{i \in J' \cup \overline{J}}x_i - \sum_{i \in I} x_i} + \norm{\sum_{i \in I} x_i - \sum_{i \in \overline{J}}x_i}< \varepsilon  
	\end{align*}
	
	Zur Rückrichtung: Für $n \in \mathds{N}$ existiert $J_n \finSub I$ endlich, sodass für $J' \finSub I$ mit $J' \cap J_n = \emptyset$ gilt 
	$\norm{\sum_{i \in J'} x_i}< \frac{1}{n+1}$. Aber dann ist 
	\begin{align*}
		\enbrace*{\sum_{i \in J_0 \cup J_1 \cup \ldots \cup J_n} \hspace{-1em} x_i}_{n \in \mathds{N}} \subset \mathcal{H}
	\end{align*}
	eine Cauchyfolge. Für den Limes $x$ gilt $x= \sum_{i \in I} x_i$.
	
	Sei dazu $\varepsilon>0$. Wähle $n \in \mathds{N}$, sodass $\frac{1}{n+1}<\frac{\varepsilon}{2}$ und außerdem 
	$\norm{x - \sum_{i \in J_0 \cup \ldots \cup J_n} x_i}< \frac{\varepsilon}{2}$ gilt. Setze nun $\overline{J}:= J_n$. Dann gilt für 
	$\overline{J} \subset J \finSub I$
	\[
		\norm{x - \sum_{i \in J}x_i} \le \norm{x- \sum_{i \in J_0 \cup \ldots \cup J_n} \hspace{-1em} x_i } 
		+ \norm{\sum_{i \in J_0 \cup \ldots \cup J_n}\hspace{-1em} x_i - \sum_{i \in J} x_i}
		< \frac{\varepsilon}{2} + \frac{1}{n+1} < \varepsilon  
	\]
	\item Angenommen $\set{x_i}_{i \in I}$ ist summierbar. Seien die $J_n$ wie oben. Dann ist $\bigcup_{n \in \mathds{N}} J_n$ abzählbar. Falls 
	$I \ni i \not\in \bigcup_{n \in \mathds{N}} J_n$, so gilt $\norm{x_i} < \frac{1}{n+1}$ für jedes $n \in \mathds{N}$, also $x_i=0$.
	
	Die Rückrichtung ist klar. (sonst noch einmal an Analysis I. erinnern \ldots ) \bewende
\end{enumerate}
% subsection 717 (end)

\subsection[Proposition: Linearität von unendlichen Summen und Verträglichkeit Skalarprodukt]{Proposition} % (fold)
\label{sub:718}
Sei $\mathcal{H}$ ein Hilbertraum, $z \in \mathcal{H}$, $\alpha, \beta \in \mathds{K}$, $\set{x_i}_{i \in I}, \set{y_i}_{i \in I}$ summierbare Familien. Dann gilt
\begin{align*}
	\alpha \cdot \sum_{i \in I} x_i + \beta \cdot \sum_{i \in I} y_i = \sum_{i \in I} \enbrace*{\alpha \cdot x_i + \beta \cdot y_i} 
	\qquad \text{ und } \qquad 
	\skal*{\sum_{i \in I} x_i}{z} = \sum_{i \in I} \skal*{x_i}{z}
\end{align*}
\minisec{Beweis}
\emph{Übung!} \bewende
% subsection 718 (end)

\subsection[Proposition: Kriterium für die Summierbarkeit orthogonaler Vektoren]{Proposition} % (fold)
\label{sub:719}
Sei $\set{x_i}_{i \in I}\subset \mathcal{H}$ eine Familie paarweise orthogonaler Vektoren. Dann ist $\set{x_i}_{i \in I}$ summierbar genau dann, wenn 
$\sum_{i \in I} \norm{x_i}^2$ endlich ist. In diesem Fall gilt
\[
	\norm{\sum_{i \in I}x_i}^2 = \sum_{i \in I} \norm{x_i}^2  
\]
\minisec{Beweis}
Falls $\set{x_i}_{i \in I}$ summierbar ist, so gilt
\begin{align*}
	\norm{\sum_{i \in I} x_i}^2 =  
	\skal*{\sum_{i \in I}x_i}{\sum_{j \in I} x_j} \stackrel{\mbox{\scriptsize\ref{sub:718}}}{=} \sum_{i \in I} \skal*{x_i}{\sum_{j \in I} x_j}
	\stackrel{\mbox{\scriptsize\ref{sub:718}}}{=} \sum_{i \in I} \sum_{j \in I} \skal*{x_i}{x_j} = \sum_{i \in I} \norm{x_i}^2  
\end{align*}
Äquivalenz: Übung mit 
\[
	\norm{\sum_{i \in I}x_i}^2 \xleftarrow{J \finSub I}\norm{\sum_{i \in J} x_i}^2 = \sum_{i \in J} \norm{x_i}^2 \xrightarrow{J \finSub I} \sum_{i \in I} \norm{x_i}^2  
\]
für $J \finSub I$ endlich und \ref{sub:717}. \bewende
% subsection 719 (end)

\subsection[Proposition: Besselsche Ungleichung und Parselvalsche Gleichung]{Proposition} % (fold)
\label{sub:720}
Sei $\set{x_i}_{i \in I} \subset \mathcal{H}$ ein Orthonormalsystem und $x \in \mathcal{H}$. 
\begin{enumerate}[(i)]
	\item Es gilt $\sum_{i \in I} \abs*{\skal*{x_i}{x}}^2 \le \norm{x}^2$
	(\Index{Besselsche Ungleichung}\footnote{nach Friedrich Wilhelm Bessel, \url{http://de.wikipedia.org/wiki/Friedrich_Wilhelm_Bessel}}).
	\item Es gilt $\sum_{i \in I} \abs*{\skal*{x_i}{x}}^2 = \norm{x}^2$ 
	(\Index{Parselvalsche Gleichung}\footnote{nach Marc-Antoine Parseval, \url{http://de.wikipedia.org/wiki/Marc-Antoine_Parseval}}) 
	genau dann, wenn 
	\[
		x= \sum_{i \in I} \skal*{x_i}{x} \cdot x_i.
	\]
\end{enumerate}
\minisec{Beweis}
\begin{enumerate}[(i)]
	\item Für $J \finSub I$ endlich gilt
	\begin{align*}
		\hspace{-1ex} 0 \le\norm{ x - \sum_{i \in J} \skal*{x_i}{x} x_i}^2 &= \skal*{x}{x} - \sum_{i \in J} \skal[\big]{\skal*{x_i}{x}  x_i}{x} - 
		\sum_{i \in J} \skal[\big]{x}{\skal*{x_i}{x} x_i} \\
		&\hphantom{= } + \sum_{i \in J} \sum_{j \in J} \skal[\big]{\skal*{x_i}{x} x_i}{\skal*{x_j}{x} x_j} \\
		&= \skal*{x}{x} - \sum_{i \in J} \overline{\skal*{x_i}{x}} \skal*{x_i}{x} - \sum_{i \in J} \skal*{x_i}{x} \overline{\skal*{x_i}{x}}
		+ \sum_{i \in J} \overline{\skal*{x_i}{x}} \skal*{x_i}{x}   
	\end{align*}
	Also $\sum_{i \in J} \abs*{\skal*{x_i}{x}}^2 \le \norm{x}^2 < \infty$ und somit folgt $\sum_{i \in I} \abs*{\skal*{x_i}{x}}^2 \le \norm{x}^2$.
	\item Die Rechnung aus (i) zeigt auch: 
	\[
		\sum_{i \in J} \abs*{\skal*{x_i}{x}}^2 \xrightarrow{J \finSub I} \norm{x}^2  \iff \sum_{i \in J} \skal*{x_i}{x} \cdot x_i \xrightarrow{J \finSub I}  x 
		\bewende
	\]
\end{enumerate}
% subsection 720 (end)

\subsection[Definition und Proposition: Summe von Hilberträumen]{Definition und Proposition} % (fold)
\label{sub:721}
Seien $\mathcal{H}_i$ für $i \in I$ Hilberträume. Dann ist
\[
	\bigoplus_{i \in I} \mathcal{H}_i := \set[(x_i)_{i \in I}]{x_i \in \mathcal{H}_i, \sum_{i \in I} \norm[\mathcal{H}_i]{x_i}^2 < \infty} 
\]
ein Hilbertraum mit $\skal*{(x_i)_{i \in I}}{(y_i)_{i \in I}} := \sum_{i \in I} \skal*{x_i}{y_i}_{\mathcal{H}_i}$.
\minisec{Beweis}
\emph{Übung, Aufgabe 1 von Blatt 12} \bewende
% subsection 721 (end)

\subsection[Satz über Äquivalenzen zu: $\set{x_i}_{i \in I}$ ist eine Hilbertraumbasis]{Satz} % (fold)
\label{sub:722}
Sei $\mathcal{H}$ ein Hilbertraum und $\set{x_i}_{i \in I} \subset \mathcal{H}$ ein Orthonormalsystem. Dann sind äquivalent: 
\begin{enumerate}[(i)]
	\item $\set{x_i}_{i \in I}$ ist eine Hilbertraumbasis. 
	\item Falls $x \perp x_i$ für alle $i \in I$, so gilt $x=0$.
	\item Es gibt einen isometrischen Isomorphismus $\bigoplus_{i \in I} \mathcal{H}_i \cong \mathcal{H}$ gegeben durch 
	\(
		\enbrace*{\lambda_i \cdot x_i}_{i \in I} \mapsto \sum_{i \in I} \lambda_i \cdot x_i 
	\)
	wobei $\mathcal{H}_i = \mathds{K} \cdot x_i$ der von $x_i$ erzeugte Unterhilbertraum ist.
	\item Für jedes $x \in \mathcal{H}$ gilt $x= \sum_{i \in I} \skal*{x_i}{x} \cdot x_i$.
	\item Für alle $x,y \in \mathcal{H}$ gilt $\skal*{x}{y} = \sum_{i \in I} \skal*{x}{x_i} \cdot \skal*{x_i}{y}$.
	\item Für alle $x \in \mathcal{H}$ gilt die Parsevalsche Gleichung $\norm{x}^2 = \sum \abs*{\skal*{x_i}{x}}^2$.
\end{enumerate}
\minisec{Beweis}
\begin{description}
	\item[(i)$\Rightarrow$(ii):] Falls $0 \not= x \perp x_i$ für alle $i \in I$, so ist $\set{x_i}_{i \in I} \cup \set{\frac{1}{\norm{x}} \cdot x}$ ein Orthonormalsystem. 
	\light
	\item[(ii)$\Rightarrow$(iii):] $\gamma \colon \bigoplus_{i \in I} \mathcal{H}_i \to \mathcal{H}$ gegeben durch 
	$(\lambda_i \cdot x_i)_I \mapsto \sum \lambda_i \cdot x_i$ ist eine wohldefinierte lineare Isometrie nach \ref{sub:718},\ref{sub:719} und \ref{sub:721}. Außerdem erhält 
	$\gamma$ das Skalarprodukt. $\gamma \enbrace*{\bigoplus_{i \in I} \mathcal{H}_i}$ ist vollständig,\footnote{Das Bild eines vollständigen Raumes unter eine stetigen 
	Abbildung $f \colon (X,d) \to (X'.d')$ mit $d(x,y) \le d'\enbrace[\big]{f(x),f(y)}$ ist vollständig.}
	also abgeschlossen in $\mathcal{H}$. Falls $\gamma \enbrace*{\bigoplus_{i\in I} \mathcal{H}_o} \not= \mathcal{H}$, so ist nach \ref{sub:715} 
	$\gamma \enbrace*{\bigoplus_{i \in I} \mathcal{H}_i}^\bot \not= \set{0}$. \light zu (ii). 
	\item[(iii)$\Rightarrow$(iv):] Für $x \in \mathcal{H}$ existieren $\lambda_i \in \mathds{K}$,  sodass 
	$x= \gamma \enbrace[\big]{(\lambda_i \cdot x_i)_{i \in I}} = \sum_{i \in I} \lambda_i \cdot x_i$ gilt. Es gilt 
	\begin{align*}
		\skal*{x_j}{x} = \skal*{x_j}{\sum_{i \in I} \lambda_i \cdot x_i} = \sum_{i \in I} \skal*{x_j}{\lambda_i \cdot x_i} = \sum_{i\in I} \lambda_i \cdot \delta_{ij} = \lambda_j
	\end{align*}
	Damit folgt (iv).
	\item[(iv)$\Rightarrow$(v):] Es gilt
	\[
		\skal*{x}{y} = \skal*{\sum_{i \in I}\skal*{x_i}{x} x_i}{\sum_{j \in I} \skal*{x_j}{y} x_j} = 
		\sum_{i \in I} \sum_{j \in I} \overline{\skal*{x_i}{x}} \skal*{x_j}{y} \skal*{x_i}{x_j} = \sum_{i \in I} \skal*{x}{x_i} \skal*{x_i}{y}
	\]
	\item[(v)$\Rightarrow$(vi):] Klar.
	\item[(vi)$\Rightarrow$(i):] Falls $x \perp x_i$ für alle $i \in I$, so gilt $\norm{x}^2= \sum_{i \in I} \abs*{\skal*{x_i}{x}}^2 =0$. Also ist $x=0$ und damit ist das 
	Orthonormalsystem maximal. \bewende
\end{description}
% subsection 722 (end)

\subsection[Bemerkungen zu Hilbertraumbasen und Mächtigkeit deren Mächtigkeit]{Bemerkung} % (fold)
\label{sub:723}
\begin{enumerate}[(i)]
	\item Jeder Hilbertraum besitzt eine Basis (Lemma von Zorn).
	\item Wegen \ref{sub:722} (iii) ist ein Hilbertraum bis auf isometrischen Isomorphismus durch die Mächtigkeit seiner Basis eindeutig bestimmt.
	\item Jeder separable Hilbertraum ist isometrisch isomorph zu $\mathds{K}^n$ für ein $n \in \mathds{N}$ oder zu $\ell^2(\mathds{N})$.
	
	Dies folgt mit Aufgabe 2 von Blatt 12: Jeder separable Hilbertraum hat eine höchstens abzählbare Hilbertraumbasis. 
\end{enumerate}
% subsection 723 (end)

\subsection[Beispiele für Basen von Hilberträumen]{Beispiele} % (fold)
\label{sub:724}
\begin{enumerate}[(i)]
	\item $\set{e_i}_{i \in \mathds{N}}$ bzw. $\set{e_i}_{i \in \mathds{Z}}$ bilden Basen von $\ell^2(\mathds{N})$ bzw. $\ell^2(\mathds{Z})$, wobei 
	$e_i(j)= \delta_{ij}$ ist.
	\item $\enbrace*{e_n}_{n \in \mathds{N}}$, wobei $e_n(t)=\exp(2 \pi i n t)$ ist, definiert eine Basis von $L^2([0,1])$.
\end{enumerate}
% subsection 724 (end)

\subsection[Definition und Proposition: Der zu $T \in \mathcal{L}(\mathcal{H}_1, \mathcal{H}_2)$ adjungierte Operator $T^*$]{Definition und Proposition} % (fold)
\label{sub:725}
Seien $\mathcal{H}_1$, $\mathcal{H}_2$ Hilberträume und $T \in \mathcal{L}(\mathcal{H}_1, \mathcal{H}_2)$. Dann existiert genau ein Operator 
$T^* \in \mathcal{L}(\mathcal{H}_2, \mathcal{H}_1)$, sodass für alle $x \in \mathcal{H}_1$, $y \in \mathcal{H}_2$ gilt 
\begin{equation*}
	\skal*{T^* y}{x}_{\mathcal{H}_1} = \skal*{y}{T x}_{\mathcal{H}_2} \label{eq:725} \tag{\#}
\end{equation*}
Die Abbildung ${ }^*  \colon \mathcal{L}(\mathcal{H}_1, \mathcal{H}_2) \to \mathcal{L}(\mathcal{H}_2, \mathcal{H}_1)$ ist konjugiert linear und isometrisch. Weiter gilt
$T^{**} = T$. Ist $\mathcal{H}_3$ ein weiterer Hilbertraum und $S \in \mathcal{L}(\mathcal{H}_2, \mathcal{H}_3)$, so gilt
\[
	(S \circ T)^* = T^* \circ S^*
\]
$T^*$ heißt der zu $T$ \bet{adjungierte Operator}\index{adjungierter Operator}.
\minisec{Beweis}
Sei $\kappa_i \colon \mathcal{H}_i \to \mathcal{H}_i^*$ wie in \ref{sub:711}. Wir haben 
\begin{align*}
	& \skal*{T^* y}{x}_{\mathcal{H}_1} = \skal*{y}{T x}_{\mathcal{H}_2} & \tag*{[\textbf{\#}]} \\
	\iff & \kappa_1(T^* y)(x) =\kappa_2(y)(T x) &\text{ für } x \in \mathcal{H}_1, y \in \mathcal{H}_2 \\
	\iff & \kappa_1(T^* y) =\kappa_2(y)\circ T & \text{ für } y \in \mathcal{H}_2\\
	\iff & \kappa_1(T^* y) = T^\tr \circ \kappa_2(y) &\text{ für } y \in \mathcal{H}_2\\
	\iff & T^* y = \kappa_1 ^{-1} \circ T^\tr \circ \kappa_2(y) &\text{ für } y \in \mathcal{H}_2
\end{align*}
Also definieren wir $T^* := \kappa_1 ^{-1} \circ T^\tr \circ \kappa_2 \in \mathcal{L}(\mathcal{H}_2, \mathcal{H}_1)$. Durch \eqref{eq:725} ist $T^*$ eindeutig bestimmt.
$\kappa_1 ^{-1}$ und $\kappa_2$ sind Isometrien; da außerdem $\norm{T^\tr}= \norm{T}$, gilt $\norm{T^*}=\norm{T}$ und somit ist $T \mapsto T^*$ isometrisch.
$\kappa_1 ^{-1}$ ist konjugiert linear, also auch $T \mapsto T^*$. Weiter gilt
\begin{align*}
	(S T)^* = \kappa_1 ^{-1} (S T)^\tr \kappa_3 = \kappa_1 ^{-1} T^\tr S^\tr \kappa_3 = \kappa_1 ^{-1} T^\tr \kappa_2 \kappa_2 ^{-1} S^\tr \kappa_3 = T^* S^*
\end{align*}
Des Weiteren erfüllt $T^{**} = (T^*)^* \in \mathcal{L}(\mathcal{H}_1, \mathcal{H}_2)$ für alle $x \in \mathcal{H}_1, y \in \mathcal{H}_2$ die Gleichung
\[
	\skal*{T^{**} x}{y}_{\mathcal{H}_2} = \skal*{x}{T^* y}_{\mathcal{H}_1}
\]
Ebenso gilt $\skal*{T x}{y}_{\mathcal{H}_2}= \overline{\skal*{y}{T_x}}_{\mathcal{H}_2} = \overline{\skal*{T^* y}{x}}_{\mathcal{H}_1} = \skal*{x}{T^* y}_{\mathcal{H}_1}$ für
alle $x \in \mathcal{H}_1$, $y \in \mathcal{H}_2$. Folglich muss $T^{**} x = T x$ gelten.\bewende
% subsection 725 (end)

\subsection[Proposition: Darstellung der Operatornorm durch $\inf$ und $\sup$ für Hilberträume]{Proposition} % (fold)
\label{sub:726}
Für $T \in \mathcal{L}(\mathcal{H}_1, \mathcal{H}_2)$ gilt: 
\begin{align*}
	\norm{T} &= \inf \set[\lambda \in \mathds{R}]{ \abs*{\skal*{y}{T x}_{\mathcal{H}_2}} \le \lambda \cdot \norm{y} \cdot \norm{x} \,\forall x \in \mathcal{H}_1, y \in 
	\mathcal{H}_2\rule{0cm}{1em}}  \\
	&= \sup \set[\abs*{\skal*{y}{T x}}_{\mathcal{H}_2}]{ x \in \overline{B}_{\mathcal{H}_1}(0,1), y \in \overline{B}_{\mathcal{H}_2}(0,1) \rule{0cm}{1em}} 
\end{align*}
\minisec{Beweis}
Sei $\gamma$ die rechte Seite der oberen Gleichung.
Es gilt $\abs*{\skal*{y}{Tx}_{\mathcal{H}_2}} \le \norm{T} \cdot \norm{y} \cdot \norm{x}$ für $x \in \mathcal{H}_1$, $y \in \mathcal{H}_2$, also ist 
$\gamma \le \norm{T}$. Weiter gilt 
\[
	\norm{T x}^2 = \abs*{\skal*{T x}{T x}} \le \gamma \cdot \norm{T x} \cdot \norm{x} \le \gamma \cdot \norm{T} \cdot \norm{x}^2
\]
also ist $\norm{T x} \le \sqrt{\gamma} \cdot \sqrt{\norm{T}} \cdot \norm{x}$ für $x \in \mathcal{H}_1$ und es folgt $\norm{T} \le \sqrt{\gamma} \cdot \sqrt{\norm{T}}$. Also
gilt $\sqrt{\norm{T}} \le \sqrt{\gamma}$, woraus $\norm{T} \le \gamma$ folgt und somit insgesamt $\gamma = \norm{T}$.
Die zweite Gleichheit ist klar. \bewende
% subsection 726 (end)

\subsection[Proposition: Abschätzung von $\abs*{\skal*{y}{T x} + \skal*{T y}{x}}$]{Proposition} % (fold)
\label{sub:727}
Sei $\mathcal{H}$ ein $\mathds{K}$-Hilbertraum, $T\in \mathcal{L}(\mathcal{H})$ und $\beta\ge 0$. Falls $\abs*{\skal*{x}{T x}}\le\beta\norm{x}^2$ für alle $x\in\mathcal{H}$,
so gilt für $x,y \in \mathcal{H}$
\begin{align*}
	\abs*{\skal*{y}{T x} + \skal*{T y}{x}} &\le 2 \beta \cdot \norm{x} \cdot \norm{y} \\
	\shortintertext{Falls $\mathds{K}=\mathds{C}$ ist, gilt sogar}
	\abs*{\skal*{y}{T x}} + \abs*{\skal*{T y}{x}} &\le 2 \beta \cdot \norm{x} \cdot \norm{y}
\end{align*}
In diesem Fall gilt also $T=0 \iff \skal*{x}{T x}=0$ für alle $x \in \mathcal{H}$.
\minisec{Beweis}
Für alle $x,y \in \mathcal{H} \setminus \set{0} $ gilt (für $0$ ist die Ungleichung trivial)
\begin{align*}
	2 \abs[\big]{\skal*{y}{T x}+ \skal*{T y}{x}} = \abs[\big]{\skal*{x+y}{T(x+y)} - \skal*{x-y}{T(x-y)}} &\le \beta \norm{x+y}^2 + \beta \norm{x-y}^2 \\
	&\le 2 \beta \enbrace*{\norm{x}^2 + \norm{y}^2}
\end{align*}
Ersetze $x$ durch $\enbrace*{\frac{\norm{y}}{\norm{x}} }^{\frac{1}{2}} \!\cdot x$ und $y$ durch $\enbrace*{\frac{\norm{x}}{\norm{y}}}^{\frac{1}{2}}\! \cdot y$. Dann folgt
\begin{equation*}
	\bcancel{2}\cdot  \abs*{\skal*{y}{T x} + \skal*{T y}{x}} \le \bcancel{2} \cdot \beta \cdot \enbrace[\Big]{\norm{x} \cdot \norm{y} + \norm{x} \cdot \norm{y}} 
	= \bcancel{2} \cdot 2 \cdot \beta \cdot \norm{x} \cdot \norm{y} \label{eq:727} \tag{*}
\end{equation*}
Damit ist die Aussage für $\mathds{K}=\mathds{R}$ gezeigt. Falls $\mathds{K}=\mathds{C}$ ist, so finden wir $\rho, \sigma \in \mathds{R}_+$, sodass\todo{Das stimmt noch 
nicht so ganz. Man bräuchte noch $e^{-i \sigma} \skal*{x}{T y} \in \mathds{R}$ oder Ähnliches, um die Konjugation loszuwerden. Solche $\rho,\sigma$ muss es aber nicht 
unbedingt geben \ldots}
\begin{align*}
	\abs*{\skal*{y}{T x}} + \abs*{\skal*{T y}{x}} &= \abs*{ e^{i(\rho +\sigma)} \cdot \skal*{y}{T x} + e^{i(\rho-\sigma)} \cdot {\skal*{x}{T y}}} \\
	&= \abs[\Big]{e^{i \rho} \cdot \enbrace*{\skal*{y}{T \enbrace*{e^{i \sigma} \cdot x} } + \skal*{e^{i \sigma} \cdot x}{T y} } } \\
	&= \abs*{\skal*{y}{T \enbrace*{e^{i \sigma} \cdot x} } + \overline{\skal*{Ty}{e^{i \sigma} \cdot x}}} \\
	\ldots & \stackrel{\mathclap{\mbox{\scriptsize\eqref{eq:727}}}}{\le} 2 \cdot \beta \cdot \norm{x} \cdot \norm{y}  \bewende
\end{align*}
% subsection 727 (end)

\subsection[Definition: Selbstadjungierter Operator]{Definition} % (fold)
\label{sub:728}
$T \in \mathcal{L}(\mathcal{H})$ heißt \Index{selbstadjungiert}, falls $T=T^*$. Wir setzen
\[
	\mathcal{L}(\mathcal{H})_\sa := \set[T \in \mathcal{L}(\mathcal{H})]{T \text{ selbstadjungiert}} 
\]
% subsection 728 (end)

\subsection[Bemerkung: Operatornorm von selbstadjungierten Operatoren als ein Infimum]{Bemerkung} % (fold)
\label{sub:729}
Für $T$ selbstadjungiert gilt 
\[
	\norm{T}= \inf \set[\lambda \in \mathds{R}]{\abs*{\skal*{x}{T x}} \le \lambda \norm{x}^2 \text{ für alle } x \in \mathcal{H}}  
\]
\minisec{Beweis}
Sei $\beta$ das Infimum auf der rechten Seite. Dann gilt für $x,y \in \mathcal{H}$
\begin{align*}
	2 \cdot \abs*{\skal*{y}{T x}} = \abs*{\skal*{y}{T x} + \skal*{T y}{x}} \stackrel{\mbox{\scriptsize \ref{sub:727}}}{\le} 2 \cdot \beta \cdot \norm{x} \cdot \norm{y} 
\end{align*}
Nach \ref{sub:726} folgt nun $\norm{T} \le \beta \le \norm{T}$. \bewende
% subsection 729 (end)

\subsection[Proposition: Charakterisierung von selbstadjungiert im komplexen Fall]{Proposition} % (fold)
\label{sub:730}
Falls $\mathds{K}=\mathds{C}$ und $T \in \mathcal{L}(\mathcal{H})$, so gilt
\[
	T \text{ selbstadjungiert } \iff \skal*{x}{T x} \in \mathds{R} \text{ für alle } x \in \mathcal{H}
\]
\minisec{Beweis}
Die Hinrichtung folgt aus $\skal*{x}{T x} = \skal*{T x}{x} = \overline{\skal*{x}{T x}}$ für $x \in \mathcal{H}$. Für die Rückrichtung betrachte
$\skal*{x}{T x} = \skal*{T x}{x} = \skal*{x}{T^* x}$. Dann ist $\skal*{x}{(T-T^*)x}=0$ für $x \in \mathcal{H}$. Es folgt dann $T- T^*=0$. \bewende 
% subsection 730 (end)

\subsection[Definition und Proposition: Partielle Ordnung auf den selbstadjungierten Operatoren]{Definition und Proposition} % (fold)
\label{sub:731}
Seien $R,S,T \in \mathcal{L}(\mathcal{H})$ selbstadjungiert. Wir schreiben 
\begin{itemize}
	\item $T \ge 0$, falls $\skal*{x}{T x} \ge 0$ für alle $x \in \mathcal{H}$ gilt.
	\item $T \ge S$, falls $T-S \ge 0$
\end{itemize}
\enquote{$\ge$} ist eine partielle Ordnung auf $\mathcal{L}(\mathcal{H})_\sa$. Das heißt 
\[
	T \ge T \qquad T \ge S, S \ge T \Longrightarrow S=T \qquad T \ge S, S \ge R \Longrightarrow T \ge R\marginnote{folgt mit \ref{sub:729}}
\]
Es gilt $-\norm{T} \cdot \id_{\mathcal{H}} \le T  \le \norm{T} \cdot \id_{\mathcal{H}}$. Falls $T \ge 0$ ist, so gilt 
\begin{equation*}
	\abs*{\skal*{y}{T x}}^2 \le  \skal*{x}{T x} \cdot \skal*{y}{T y} \label{eq:731} \tag{*}
\end{equation*}
insbesondere gilt $\skal*{x}{T x}=0 \iff T x=0$
\minisec{Beweis}
\emph{Übung, siehe Anhang \ref{sub:bew_731}.}\bewende
% subsection 731 (end)

\subsection[Satz: Konvergenz eines monoton wachsenden, beschränkten Netzes in $\mathcal{L}(\mathcal{H})_\sa$]{Satz} % (fold)
\label{sub:732}
Sei $(T_j)_{j \in I} \subset \mathcal{L}(\mathcal{H})_\sa$ ein monoton wachsendes und beschränktes Netz selbstadjungierter Operatoren. Dann konvergiert $(T_j)_{j \in I}$
punktweise gegen ein $T \in \mathcal{L}(\mathcal{H})_\sa$, das heißt $T_j x$ konvergiert gegen $T x$ in Norm für jedes $x \in \mathcal{H}$. 

\noindent Wir schreiben auch $T_j \xrightarrow{\so} T$, d.h. $(T_j)_{j \in I}$ konvergiert gegen $T$ in der \bet{starken Operatortopologie}\index{starke Operatortopologie}.
\minisec{Beweis}
Nach Polarisierung (vergleiche \hyperref[sub:78]{Bemerkung \ref*{sub:78}}) gilt für $\mathds{K}=\mathds{C}$ \todo{Beweis vervollständigen/korrigieren}
\[
	\skal[\big]{x}{T_j y} = \frac{1}{4} \enbrace[\Big]{\skal[\big]{x+y}{T_j(x+y)} + \skal[\big]{x-y}{T_j(x-y)} + i \skal[\big]{i x +y}{T_j(ix +y)} 
	- i \skal[\big]{i x -y}{T_j(ix-y)}}
\]
Die rechte Seite konvergiert (warum?), also auch die linke Seite. $\Rightarrow$ Für jedes $x \in \mathcal{H}$ konvergiert 
$\skal*{x}{T_j(\cdot)} \colon \mathcal{H} \to \mathds{C}$ gegen ein $\varphi_x \colon \mathcal{H} \to \mathds{C}$. Aber $\varphi_x \in \mathcal{H}^*$ (warum?). Es gilt
\begin{equation*}
	\norm[\mathcal{H}^*]{\varphi_x} \le \limsup_j \norm{T_j} \cdot \norm{x} < \infty. 
\end{equation*}
Die Abbildung $x \mapsto \kappa ^{-1}(\varphi_x)$ ist linear (warum?). Definiere 
$T \colon \mathcal{H} \to \mathcal{H}$ durch $T(x) := \kappa ^{-1}(\varphi_x)$. Es gilt $\norm{T(x)} = \norm{\varphi_x}$, also $\norm{T}\le\limsup_j \norm{T_j} \le \infty$.
Damit ist $T \in \mathcal{L}(\mathcal{H})$. Außerdem gilt für $x,y \in \mathcal{H}$
\begin{align*}
	\skal*{T x}{y} = \kappa(T x)(y) = \varphi_x(y) = \lim_j \skal[\big]{x}{T_j y} = \lim_j \skal*{T_j x}{y} = \overline{\lim_j \skal*{y}{T_j x}} 
	= \overline{\varphi_y(x)} &= \overline{\skal*{T y}{x}}\\  &= \skal*{x}{T y}  
\end{align*}
Also ist $T$ auch selbstadjungiert. Es gilt für $\lim_j \skal*{x}{T_j y} = \skal*{x}{T y}$ für $x,y \in \mathcal{H}$, das heißt $T_j \xrightarrow{\mathrm{w.o.}} T$
($T_j \to T$ in der schwachen Operatortopologie). Für $j \ge j'$ gilt $T_j - T_{j'} \ge 0$, also nach \ref{sub:731} für $x \in \mathcal{H}$
\begin{align*}
	\skal[\Big]{\enbrace*{T_j - T_{j'}}(x)}{\enbrace*{T_j - T_{j'}}(x)} &\le \skal[\Big]{x}{\enbrace*{T_j - T_{j'}}(x) } \cdot 
	\skal*{\enbrace*{T_j - T_{j'}}(x)}{\enbrace*{T_j - T_{j'}}^2(x) } \\
	&\le \skal[\Big]{x}{\enbrace*{T_j - T_{j'}}(x)} \cdot \enbrace*{2 \cdot \limsup_j \norm{T_j} }^3 \cdot \norm{x}^2 <  \varepsilon
\end{align*}
falls $j'$ groß genug ist. Also $\norm{T_j(x-)- T_{j'}(x)}< \varepsilon$ falls $j \ge j'$ groß. Damit folgt dann $T_j(x) \to T(x)$ in Norm (warum?). \bewende
% subsection 732 (end)
% section 7 (end)
\newpage

\section{Kompakte Operatoren und ein Spektralsatz} % (fold)
\label{sec:8}

\subsection[Definition: Spektrum eines Bachnachraumes]{Definition} % (fold)
\label{sub:81}
Sei $X$ ein $\mathds{K}$-Banachraum und $T \in \mathcal{L}(X)$. Wir definieren das \Index{Spektrum} von $T$ als
\[
	\sigma(T) := \set[\lambda \in \mathds{K}]{T- \lambda \cdot \id_X \text{ ist nicht invertierbar}} \subset \mathds{K} \marginnote{auch $\spec(T)$} 
\]
Die \Index{Resolventenmenge} ist $\res(T) :=  \rho(T) := \mathds{K} \setminus \sigma(T)$. Wir schreiben $\resolv(T,\lambda) := (T- \lambda \cdot \id_X) ^{-1}$ für
die \Index{Resolvente} von $T$ in $\lambda \in \rho(T)$. 
% subsection 81 (end)

\subsection[Bemerkung zum Zusammenhang von Spektralwerten und Eigenwerten]{Bemerkung} % (fold)
\label{sub:82}
\begin{enumerate}[(i)]
	\item Jeder Eigenwert von $T$ ist ein Spektralwert.
	\item Für $X$ endlichdimensional gilt auch die Umkehrung, für beliebige $X$ jedoch nicht: Sei $X=\ell^2(\mathds{N})$, dann ist $S \in \mathcal{L}\enbrace[\big]{\ell^2(\mathds{N})}$
	gegeben durch $S(x_0, x_i, \ldots) := (0,x_0, x_1, \ldots)$ (\Index{unilateraler Shift}) linear, stetig und sogar isometrisch, aber \emph{nicht} surjektiv. Dann ist
	$0 \in \sigma(S)$, aber $0$ ist kein Eigenwert, da $S$ injektiv ist.
\end{enumerate}
% subsection 82 (end)

\subsection[Proposition: Bijektivität von Operatoren in einer Umgebung eines bijektiven Operators]{Proposition} % (fold)
\label{sub:83}
Seien $X,Y$ Banachräume und $T \in \mathcal{L}(X,Y)$ bijektiv (also injektiv). Falls für $S \in \mathcal{L}(X,Y)$ gilt 
\[
	\norm[\mathcal{L}(X,Y)]{T-S} < \norm[\mathcal{L}(Y,X)]{T^{-1}}^{-1}, 
\]
so ist auch $S$ bijektiv.
\minisec{Beweis}

Es gilt $S= T \enbrace[\big]{\id_X - T ^{-1}(T-S)}$. Setze $C := \norm{T ^{-1}} \cdot \norm{T-S} <1$.\marginnote{Ausführlicher Beweis im Anhang \ref{sub:bew_83}}
Für $k \le l$ gilt dann 
\begin{align*}
	\norm{\sum_{n=k}^{l} \enbrace[\big]{T^{-1}(T-S)}^n } \le \sum_{n=k}^{l} \norm{\enbrace[\big]{T ^{-1}(T-S)}^n} \le \sum_{n=k}^{l} \norm{T ^{-1}}^n \cdot \norm{T-S}^n
	= \sum_{n=k}^{l} C^n      
\end{align*}
also ist $\sum_{n=0}^{\infty} \enbrace[\big]{T ^{-1}(T-S)}^n$ konvergent in $\mathcal{L}(X,X)$. Weiter ist 
$\enbrace*{\sum_{n=0}^{\infty} \enbrace[\big]{T ^{-1}(T-S)}^n} T ^{-1} \in \mathcal{L}(Y,X)$ das Inverse von $S$ (warum?). \bewende
% subsection 83 (end)

\subsection[Corollar: Die Resolventenmenge von $T \in \mathcal{L}(X)$ ist offen in $\mathds{K}$]{Corollar} % (fold)
\label{sub:84}
Für $T \in \mathcal{L}(X)$ ist die Resolventenmenge offen in $\mathds{K}$.
% subsection 84 (end)

\subsection[Satz: Die Resolventenabbildung ist lokal durch eine Potenzreihe gegeben]{Satz} % (fold)
\label{sub:85}
Sei $X$ ein Banachraum und $T \in \mathcal{L}(X)$. Dann ist die \Index{Resolventenabbildung} $R(T,\cdot ) \colon \rho(T) \to \mathcal{L}(X)$, $\lambda \mapsto R(T,\lambda)$
lokal durch eine Potenzreihe mit Koeffizienten in $\mathcal{L}(X)$ gegeben. Mit anderen Worten $R(T,\cdot )$ ist lokal analytisch\index{analytisch}.
\minisec{Beweis}
Sei $\lambda_0 \in \rho(T)$. Setze $S_0 := T- \lambda_0 \cdot \id_X$ und $S_\lambda := T- \lambda \cdot \id_X$ für $\lambda \in \rho(T)$. Sei $\lambda \in \rho(T)$ mit 
\[
	\norm{S_0 - S_\lambda} = \abs*{\lambda-\lambda_0}< \norm{S_0 ^{-1}} ^{-1}
\]
Nach Proposition \ref{sub:83} ist $S_\lambda$ invertierbar und es gilt 
\[
	R(T,\lambda) = S_\lambda ^{-1} = \sum_{n=0}^{\infty} \enbrace*{S_0^{-1} (S_0-S_\lambda)}^n S_0^{-1} = \sum_{n=0}^{\infty} (\lambda-\lambda_0)^n \cdot S_0^{-(n+1)}
	= \sum_{n=0}^{\infty} (\lambda-\lambda_0)^n R(T,\lambda_0)^{n+1}
\]
Durch Abschätzung mit der geometrischen Reihe folgt die Beschränktheit. \bewende
% subsection 85 (end)

\subsection[Satz: Für $\mathds{K}=\mathds{C}$ ist $\sigma(T)$ nichtleer, kompakt und ${\sigma(T) \subset \set[z \in \mathds{C}]{\abs*{z} \le \norm{T}}}$]{Satz} % (fold)
\label{sub:86}
Sei $X$ ein $\mathds{C}$-Banachraum und $T \in \mathcal{L}(X)$. Dann ist $\sigma(T)$ nichtleer, kompakt und es gilt 
\[
	\sigma(T) \subset \set[z \in \mathds{C}]{\abs*{z} \le \norm{T}} 
\]
\minisec{Beweis}
Wir wissen bereits, dass $\sigma(T)$ abgeschlossen in $\mathds{C}$ ist. Für $\lambda\not=0$ ist $S_\lambda := -\lambda \cdot \id_X$ invertierbar und es gilt
$\cramped{\norm{S_\lambda ^{-1}}^{-1} =\abs*{\lambda}}$. Falls $\cramped{\norm{S_\lambda - (T-\lambda \cdot \id_X)}=\norm{T} < \abs*{\lambda}= \norm{S_\lambda ^{-1}} ^{-1}}$, so 
ist nach \ref{sub:83} $T- \lambda \cdot \id_X$ invertierbar, also ist $\lambda \not\in \sigma(T)$. Damit folgt 
$\sigma(T) \subset \set[z \in \mathds{C}]{\abs*{z}\le \norm{T}}$ und die Kompaktheit von $\sigma(T)$.

Angenommen $\sigma(T)= \emptyset$. $R(T,\lambda)$ ist stetig auf $\set[z \in \mathds{C}]{\abs*{z} \le 2 \cdot \norm{T}}$, also beschränkt. Weiter gilt für 
$\abs*{\lambda} \ge 2 \cdot \norm{T}$
\[
	R(T,\lambda) = \enbrace*{T- \lambda \cdot \id_X}^{-1} = -\lambda ^{-1} \cdot \enbrace*{- \frac{T}{\lambda} + \id_X}^{-1} 
	= -\lambda ^{-1} \cdot \sum_{n=0}^{\infty} \enbrace*{\frac{T}{\lambda}}^n 
\]
also $\norm{R(T,\lambda)} \le \abs*{\lambda}^{-1}\cdot \sum_{n=0}^{\infty} \norm{\frac{T}{\lambda}}^n\le \frac{1}{2 \norm{T}} \cdot 2 = \frac{1}{\norm{T}}$. Damit ist 
$R(T,\lambda)$ auf $\mathds{C}$ beschränkt. Für $x \in X$ und $\varphi \in X^*$ gilt: 
\[
	\lambda \mapsto \varphi \enbrace[\big]{R(T,\lambda)(x)} {\color{Grey0} \enspace = \sum_{n=0}^{\infty} (\lambda-\lambda_0)^n 
	\underbrace{\varphi\enbrace*{R(T,\lambda_0)^{n+1}(x)} }_{\in \mathds{C}}}
\]
ist lokal analytisch (auf ganz $\mathds{C}$), also ganz und beschränkt. Nach dem Satz von Liouville\footnote{$f$ ganz $\leadsto f(z)=\sum_{n=0}^{\infty}a_n \cdot z^n$ mit 
$a_k = \frac{f^{(k)}(0)}{k!} \stackrel{\text{Cauchy}}{=} \frac{1}{2 \pi i} \cdot \oint_{C_r} \frac{f(\zeta)}{\zeta^{k+1}} \mathd\zeta$, wobei $C_r$ der Kreis mit Radius $r$ 
ist. Dann folgt $\abs*{f(z)} \le M$, also $\abs*{a_k} \le \frac{1}{r^k}$, $r>0$, $k \ge 1$ $\Rightarrow a_k=0, k \ge 1$. Siehe auch
\url{https://de.wikipedia.org/wiki/Satz_von_Liouville_(Funktionentheorie)}} folgt
\begin{align*}
	&&\varphi \enbrace[\big]{R(T,\lambda)(x)} &\equiv \varphi \enbrace[\big]{R(T,0)(x)} \\  
	\Longrightarrow  && R(T,\lambda)(x) &\equiv R(T,0)(x) 	\quad , x \in X \\
	\Longrightarrow  && \enbrace*{T- \lambda \cdot \id_X}^{-1} = R(T,\lambda) &\equiv R(T,0) = T ^{-1} \light \bewende
\end{align*}
% subsection 86 (end)

\subsection[Definition und Proposition: Kompakte Operatoren]{Definition und Proposition} % (fold)
\label{sub:87}
Sei $X$ ein Banachraum. $T \in \mathcal{L}(X)$ heißt \bet{kompakt}\index{kompakter Operator}, falls eine der folgenden äquivalenten Bedingungen erfüllt ist:
\begin{enumerate}[(i)]
	\item Das Bild jeder beschränkten Menge $M$ unter $T$ ist relativ kompakt, d.h. $\overline{T(M)} \subset X$ kompakt.
	\item $\overline{T \enbrace*{B_X(0,1)}}$ ist kompakt. 
	\item Ist $(x_n)_\mathds{N} \subset X$ beschränkt, so enthält $\enbrace*{T(x_n)}_{n \in \mathds{N}}$ eine konvergente Teilfolge.
\end{enumerate}
Wir schreiben $\mathcal{K}(X)$ für die Menge der kompakten Operatoren auf $X$.
\minisec{Beweis}
\begin{description}
	\item[(i)$\Leftrightarrow$(ii):] Klar, da $T$ linear ist.
	\item[(i)$\Leftrightarrow$(iii):] Für metrische Räume ist kompakt äquivalent zu folgenkompakt. \bewende
\end{description}
% subsection 87 (end)

\subsection[Bemerkung zu kompakten Operatoren]{Bemerkung} % (fold)
\label{sub:88}
\begin{enumerate}[(i)]
	\item Sei $X$ ein Banachraum. Dann gilt \hfill \emph{(Übung!)}\marginnote{Je nach Vorgehen, hat man in Aufgabe 1 von Blatt 8 schon
	$\overline{B}_r(0) \subseteq X$ kompakt $\iff X$ endlichdimensional gezeigt}
	\[
		\overline{B}_X(0,1) \text{ ist kompakt } \iff \id_X \text{ ist kompakt } \iff X \text{ ist endlichdimensional}
	\]
	\item Entsprechend kann man $\mathcal{K}(X,Y)$ definieren.
\end{enumerate}
% subsection 88 (end)

\subsection[Proposition: Die Menge der kompakten Operatoren ist ein abgeschlossenes Ideal in $\mathcal{L}(X)$]{Proposition} % (fold)
\label{sub:89}
Sei $X$ ein Banachraum. Dann ist $\mathcal{K}(X) \lhd \mathcal{L}(X)$ ein abgeschlossenes Ideal.
\minisec{Beweis}
\begin{description}
	\item[$\mathcal{K}(X) \subset \mathcal{L}(X)$ ist ein linearer Unterraum:] $T,T' \in \mathcal{K}(X)$, $\alpha \in \mathds{K}$. $\alpha \cdot T \in \mathcal{K}(X)$ ist 
	klar. Wir müssen nur $T+T' \in \mathcal{K}(X)$ zeigen: Sei $(x_n)_\mathds{N} \subset X$ beschränkt. Da $T$ kompakt ist, existiert eine Teilfolge 
	$(x_{n_k})_{k \in \mathds{N}}$, sodass $(T x_{n_k})_{k \in \mathds{N}}$ konvergiert. Da $T'$ kompakt ist, gibt es wiederum eine Teilfolge 
	$\enbrace[\big]{\cramped{x_{n_{k_l}}}}_{l \in \mathds{N}}$, sodass $\enbrace{T x_{n_{k_l}}}_{l \in \mathds{N}}$ konvergiert. Dann muss auch 
	$\enbrace[\big]{(T+T')\enbrace[\big]{x_{n_{k_l}}}}_{l \in \mathds{N}}$ konvergieren.
	\item[$\mathcal{K}(X)$ ist Ideal:] Sei $T \in \mathcal{K}(X)$, $S \in \mathcal{L}(X)$. Dann gilt auch $TS, ST \in \mathcal{K}(X)$, wie man sich leicht überlegt. (Stetige 
	Abbildungen bilden Kompakta auf Kompakta ab)
	\item[$\mathcal{K}(X)$ ist abgeschlossen:] Zu $T \in \overline{\mathcal{K}(X)}$ existiert eine Folge $(T_k)_{k \in \mathds{N}} \subset \mathcal{K}(X)$ mit
	$\norm{T-T_k} \to 0$. Sei $(x_n)_{n \in \mathds{N}} \subset X$ beschränkt. Wähle eine Teilfolge $(x_n^\ssbrace{0})_{n \in \mathds{N}}$, sodass 
	$(T_0 x_n^\ssbrace{0})_{n \in \mathds{N}}$ konvergiert und in einem $\frac{1}{1}$-Ball liegt. Wähle nun wiederum eine Teilfolge $(x_n^\ssbrace{1})_{n \in \mathds{N}}$ von
	$(x_n^\ssbrace{0})_{n \in \mathds{N}}$ mit: $(T_1 x_n^\ssbrace{1})_{n \in \mathds{N}}$ konvergiert und liegt in einem $\frac{1}{2}$-Ball. Induktiv folgt, dass
	für $(x_n^\ssbrace{n})_{n \in \mathds{N}}$ gilt: $(x_n^\ssbrace{n})_{n \in \mathds{N}}$ ist Teilfolge von $(x_n)_{n \in \mathds{N}}$ und 
	$(Tx_n^\ssbrace{n})_{n \in \mathds{N}}$ konvergiert. \bewende
\end{description}
% subsection 89 (end)

\subsection[Satz: Diagonalisierbarkeit selbstadjungierter, kompakter Operatoren]{Satz} % (fold)
\label{sub:810}
Sei $T \in \mathcal{K}(\mathcal{H})$ ein selbstadjungierter, kompakter Operator auf dem Hilbertraum $\mathcal{H}$. Dann ist $T$ diagonalisierbar. Genauer gilt:
\begin{enumerate}[(i)]
	\item $\sigma(T) \subset \mathds{R}$ ist abzählbar und der einzige mögliche Häufungspunkt ist $0$.
	\item Jedes $\lambda \in \sigma(T)$ ist ein Eigenwert. Die Eigenräume zu verschiedenen Eigenwerten sind\marginnote{Letzteres gilt allgemein für selbstadjungierte 
	Operatoren} orthogonal.
	\item Seien $\tilde{\lambda}_0, \tilde{\lambda}_1, \ldots $ die unterschiedlichen Spektralwerte (d.h. Eigenwerte \emph{ohne} Vielfachheiten) und 
	$\tilde{X}_0,\tilde{X}_1,\ldots$ die zugehörigen Eigenräume. Dann ist $\tilde{X}_i$ für jedes $\tilde{\lambda}_i\not=0$ endlichdimensional und es gilt 
	$\mathcal{H}= \bigoplus_i \tilde{X}_i$.
	\item Seien $\lambda_0, \lambda_1,\ldots $ die nichtverschwindenden Eigenwerte, gezählt \emph{mit} Vielfachheiten. Dann existiert ein Orthonormalsystem 
	$\set{e_i}_{i \in \mathds{N}}$ von zugehörigen Eigenvektoren und für jedes solche Orthonormalsystem gilt für $x \in \mathcal{H}$
	\[
		T x = \sum_{i=0}^{\infty} \lambda_i \cdot \skal*{e_i}{x} \cdot e_i 
	\]
	Falls $T\ge 0$, kann man die $\lambda_i$ absteigend wählen und es gilt $\lambda_i \to 0$ (falls es unendlich viele $\lambda_i\not=0$ gibt).
\end{enumerate}
\minisec{Beweis (für $T \ge 0$)}
Für $x \in \mathcal{H}$ setze $\mu(x) := \frac{\skal*{x}{T x}}{\skal*{x}{x}}\ge 0$ und \todo{Beweis durchsehen \ldots}
\[
	\overline{\mu} := \sup_{x \in \mathcal{H}} \mu(x) = \sup_{x \in \overline{B}(0,1)} \skal*{x}{T x} \le \norm{T} 
\]
Wir nehmen $\mu >0$ an, denn andernfalls ist $T=0$ nach Bemerkung \ref{sub:729}. Wähle $(x_k)_{k \in \mathds{N}} \subset \mathcal{H}$ mit $\norm{x_k}=1$ und 
$\mu(x_k)= \skal*{x_k}{T x_k} \to \overline{\mu}$. Da $T$ kompakt ist, dürfen wir (eventuell nach Übergang zu einer Teilfolge) annehmen, dass $T x_k \to \overline{y}$ für
ein $\overline{y}\in \mathcal{H}$.
Nach \ref{sub:510} ist $\overline{B}_{\mathcal{H}^*}(0,1)$ $\w^*$-kompakt, also gilt (wieder eventuell nach Übergang zu einer Teilfolge) 
\[
	\kappa(x_k) \xrightarrow{\w^*} \overline{\varphi} \in \mathcal{H}^*
\]
Setze nun $\overline{x} := \kappa ^{-1}\enbrace*{\overline{\varphi}}$. Für jedes $x \in \mathcal{H}$ gilt  
$\skal*{x_k}{T x} = \kappa(x_k)(T x) \to \overline{\varphi}(T x) = \skal*{\overline{x}}{T x}$. Also insbesondere
\[
	0 = \lim_{k\to \infty} \skal*{x_k -\overline{x}}{T x} = \lim_{ k \to \infty} \skal*{T x_k - T \overline{x}}{x}= \skal*{\lim_{ k \to \infty} T x_k - T \overline{x} }{x}
	= \skal*{\overline{y} - T \overline{x}}{x}
\]
Damit folgt $\overline{y}=T \overline{x}$. Es gilt $\norm{\overline{x}}=1$ und\footnote{\enquote{teilweiser Limes} mit: 
$\abs*{\skal*{x_k}{T x_k} - \skal*{x_k}{\overline{y}}}=\abs*{\skal*{x_k}{T x_k - \overline{y}}} \le \norm{x_k} \cdot \norm{T x_k - \overline{y}} \to 0$}
\[
	\overline{\mu} = \lim_{ k \to \infty} \skal*{x_k}{T x_k} = \lim_{ k \to \infty}  \skal*{x_k}{\overline{y}} = \skal*{\overline{x}}{\overline{y}} = 
	\skal*{\overline{x}}{T \overline{x}} = \mu(\overline{x})
\]
das heißt $\overline{\mu}$ ist das Maximum von $\mu \colon \mathcal{H} \to \mathds{R}$. Für $z \in \mathcal{H}$ definiere $f_z \colon \mathds{R} \to \mathds{R}$ durch
$f_z(t) := \mu(\overline{x}+ t \cdot z)$. Dann ist $f_z$ in $t=0$ differenzierbar und es gilt
\[
	0 = f_z'(0) = \frac{2 \cdot \skal*{z}{T \overline{x}} \cdot \norm{\overline{x}}^2 - 2 \cdot \skal*{\overline{x}}{T \overline{x}} \cdot 
	\skal*{z}{\overline{x}}}{\norm{\overline{x}}^4} 
\]
Also ist $\skal*{z}{T \overline{x} - \overline{\mu} \cdot \overline{x}} =0$ für alle $z \in \mathcal{H}$. Es folgt $T \overline{x}= \overline{\mu} \cdot \overline{x}$.
Setze nun $\lambda_0 := \overline{\mu}$, $e_0:= \overline{x}$. Definiere $\mathcal{H}_1 := \set{e_0}^\bot$. Dann ist $\mathcal{H}_1 \subset \mathcal{H}$ ein Untervektorraum.
Für $x \in \mathcal{H}_1$ gilt $\skal*{e_0}{Tx} = \skal*{T e_0}{x}= \lambda_0 \cdot \skal*{e_0}{x}=0$. Also ist $Tx \in \mathcal{H}_1$. Wir können also 
\[
	T_1 := T\big|_{\mathcal{H}_1} \in \mathcal{L}(\mathcal{H}_1)
\]
definieren. Tatsächlich gilt $0 \le T_1 = T_1^* \in \mathcal{K}(\mathcal{H}_1)$. Induktion liefert Eigenwerte $\lambda_0 \ge \lambda_1 \ge \lambda_2 \ge \ldots>0$ zu
normierten Eigenvektoren $e_0, e_1, e_2, \ldots$ (Die Folge ist endlich falls ein $\lambda_i=0$ oder $\dim \mathcal{H} < \infty$). Die Eigenvektoren sind paarweise orthogonal
und für jedes $X_k := \Span \set{e_0, \ldots ,e_k}$ und $x \in X_k^\bot$ gilt $\mu(x) \le \lambda_k$.

(Noch zu zeigen: $\lambda_k \to 0$)

Für $\tilde{\mathcal{H}} := \overline{\bigcup_{k=0}^\infty X_k}$ und $x \in \tilde{\mathcal{H}}^\bot$ gilt $\mu(x) \le \lambda_k$, $k \in \mathds{N}$, also $\mu(x)=0$.
Damit folgt $\skal*{x}{T x}=0$ und wegen $T|_{\tilde{\mathcal{H}}^\bot} \in \mathcal{K} \enbrace[\big]{\tilde{\mathcal{H}}^\bot}$ selbstadjungiert gilt nach Bemerkung 
\ref{sub:729} $T|_{\tilde{\mathcal{H}}^\bot} =0$, das heißt $\tilde{\mathcal{H}}^\bot \subset \ker T$.

Die $e_k$ bilden eine Orthonormalbasis von $\tilde{\mathcal{H}}$: Für $x \in \mathcal{H}$ existieren $\tilde{x} \in \tilde{\mathcal{H}}$ und $\tilde{x}^\bot \in \tilde{\mathcal{H}}^\bot$ mit $x= \tilde{x} + \tilde{x}^\bot$. Es gilt dann 
\[
	T x = T \tilde{x} + T(\tilde{x}^\bot) = T \tilde{x} = T \enbrace*{\sum_{k=0}^{\infty} \skal*{e_k}{\tilde{x}}} \cdot e_k = \sum_{k=0}^{\infty} \skal*{e_k}{x} T e_k
	= \sum_{k=0}^{\infty} \lambda_k \skal*{e_k}{x} \cdot e_k 
\]
Also gilt (iv). Damit folgt auch (ii). (i) folgt aus (iv) und (ii). Da $T$ kompakt ist, existiert eine konvergente Teilfolge $(T e_{k_l})_{l \in \mathds{N}}$. Es gilt
\[
	0=\lim_{l,m \to \infty} \norm{T e_{k_l} - T e_{k_n}} = \lim_{l,m \to \infty} \norm{\lambda_{k_l} \cdot e_{k_l} - \lambda_{k_m} e_{k_m}} = 
	\lim_{l,m \to \infty}  \sqrt{\lambda_{k_l}^2 + \lambda_{k_m}^2}  
\]
Damit folgt $\lambda_{k_l} \xrightarrow{l \to \infty} 0$. Damit müssen wegen der Absteigenden Folge auch $\lambda_k \xrightarrow{k \to \infty} 0$. \bewende
% subsection 810 (end)

\subsection[Corollar: Approximierbarkeit kompakter Operatoren und $\mathcal{K}(\mathcal{H})$ ist selbstadjungiert]{Corollar} % (fold)
\label{sub:811}
\begin{enumerate}[(i)]
	\item $\mathcal{K}(\mathcal{H})= \overline{\mathcal{F}(\mathcal{H})}^{\norm{\cdot}}$, wobei 
	$\mathcal{F}(\mathcal{H}) := \set[T \in \mathcal{L}(\mathcal{H})]{T(\mathcal{H}) \text{ endlichdimensional}}$.\marginnote{Kompakte Operatoren lassen sich also gut 
	approximieren}
	\item $\mathcal{K}(\mathcal{H})$ ist selbstadjungiert, das heißt $T \in \mathcal{K}(\mathcal{H}) \iff T^* \in \mathcal{K}(\mathcal{H})$.
\end{enumerate}
\minisec{Beweis}
\emph{Übung! (Blatt 14)} \bewende
% subsection 811 (end)
% section 8 (end)
\newpage

\section{Fredholm-Operatoren. Ein Index} % (fold)
\label{sec:9}

\subsection[Definition: Fredholm-Operator und Fredholm-Index]{Definition} % (fold)
\label{sub:91}
Sei $\mathcal{H}$ ein Hilbertraum. Ein Operator $T \in \mathcal{L}(\mathcal{H})$ heißt \Index{Fredholm-Operator}\footnote{nach Erik Ivar Fredholm, siehe 
\url{https://de.wikipedia.org/wiki/Erik_Ivar_Fredholm}}, falls gilt:
\begin{enumerate}[(i)]
	\item $\ker T$ ist endlichdimensional.
	\item $\im T \subset \mathcal{H}$ ist abgeschlossen.
	\item $\nicefrac{\mathcal{H}}{\im T}$ ist endlichdimensional (endliche Kodimension).
\end{enumerate}
In diesem Fall definieren wir den \Index{Fredholm-Index} von $T$ als 
\[
	\idx(T) := \dim(\ker T) - \dim (\nicefrac{\mathcal{H}}{\im T}) \in \mathds{Z} 
\]
Wir schreiben $\Fred(\mathcal{H}) := \set[T \in \mathcal{L}(\mathcal{H})]{T \text{ ist Fredholm}}$.
% subsection 91 (end)

\subsection{Satz (Fredholm-Riesz)} % (fold)
\label{sub:92}
Sei $\mathcal{H}$ ein Hilbertraum und $S \in \mathcal{K}(\mathcal{H})$. Dann ist $T := \id_\mathcal{H}-S$ Fredholm.
\minisec{Beweis}
\begin{enumerate}[(i)]
	\item $\ker T \subset \mathcal{H}$ ist abgeschlossen, also ein Unterhilbertraum. Sei $P := \mathrm{proj}_{\ker T}$ die orthogonale Projektion auf $\ker T$, das heißt 
	für $\mathcal{H} \ni x = x_0 + x_1$ mit $x_0 \in \ker T$ und $x_1 \in (\ker T)^\bot$ ist $P(x)=x_0$. Es ist $P \in \mathcal{L}(\mathcal{H})$ (warum?).
	Dann ist 
	\[
		P=\id_\mathcal{H} \circ P \stackrel{T \circ P=0}{=} S \circ P \in \mathcal{K}(\mathcal{H}) \marginnote{$\mathcal{K}(\mathcal{H})$ ist Ideal}
	\]
	Also ist $P|_{\ker T} = \id_{\ker T} \in \mathcal{K}(\ker T)$. Damit ist $\ker T$ endlichdimensional nach \ref{sub:88}.
	\item Es ist $T|_{(\ker T)^\bot} \colon (\ker T)^\bot \to \im T$ ist bijektiv und stetig. Zu zeigen ist, dass $\enbrace*{T|_{(\ker T)^\bot}} ^{-1}$ stetig ist, denn dann
	ist $\im T$ vollständig, also auch abgeschlossen. 
	
	Falls $\enbrace*{T|_{(\ker T)^\bot}}^{-1}$ nicht stetig ist, so existiert eine Folge $(x_n)_n \subset (\ker T)^\bot$
	mit $\norm{x_n}=1$ und $T x_n \to 0$. Nach Übergang zu einer Teilfolge dürfen wir $S x_n \to \overline{y}$ für ein $\overline{y} \in \mathcal{H}$ annehmen. Da
	$T x_n \to 0$, folgt $x_n = \id_x x_n \to \overline{y}$. Aber dann ist auch $\overline{y} \in (\ker T)^\bot$, $\norm{\overline{y}}=1$ und $\overline{y} \in \ker T$. 
	\light
	\item Es ist 
	\begin{align*}
		(\nicefrac{\mathcal{H}}{\im T})^* \cong\enbrace*{(\im T)^\bot}^* = \kappa \enbrace*{(\im T)^\bot} 
		&= \set[\skal*{y}{\cdot}]{\skal*{y}{T x}=0 \text{ für alle }x \in \mathcal{H}} \\
		&= \set[\skal*{y}{\cdot }]{\skal*{T^*y}{x}=0 \text{ für alle }x \in \mathcal{H}} \\
		&= \set[\skal*{y}{\cdot}]{y \in \ker T^*} \\
		&= \kappa \enbrace*{\ker T^*}  \cong \ker T^* = \ker(\id_\mathcal{H}- S^*)
	\end{align*}
	Dies ist endlichdimensional nach (i), da $S^* \in \mathcal{K}(\mathcal{H})$ nach \ref{sub:811}.\bewende \todo{RevChap9}
\end{enumerate}
% subsection 92 (end)

\subsection[Satz: $\Fred(\mathcal{H}) \subset \mathcal{L}(\mathcal{H})$ ist offen und der Fredholm-Index $\idx$ ist stetig]{Satz} % (fold)
\label{sub:93}
$\Fred(\mathcal{H}) \subset \mathcal{L}(\mathcal{H})$ ist offen bezüglich der Normtopologie. Die Abbildung $\idx \colon \Fred(\mathcal{H}) \to \mathds{Z}$ ist stetig.
\minisec{Beweis}
Sei $T \in \Fred(\mathcal{H})$. Wir haben $\ker T \oplus (\ker T)^\bot = \mathcal{H} = \im T \oplus (\im T)^\bot$ abgeschlossen.
\begin{enumerate}[(i)]
	\item Für $S \in \mathcal{L}(\mathcal{H})$ definiere $\tilde{S} \colon \tilde{H} := (\ker T)^\bot \oplus (\im T)^\bot \to \mathcal{H}$ durch
	$\tilde{S}(v,w) := S(v)+w$. Dann ist $\tilde{S}$ stetig und die Abbildung 
	\[
		\tilde{\minwidthbox{ }{5pt}} \colon \mathcal{L}(\mathcal{H}) \longrightarrow \mathcal{L}(\tilde{\mathcal{H}}, \mathcal{H}) , \quad S \longmapsto \tilde{S}
	\]
	stetig. $\tilde{T}$ ist bijektiv und stetig. Nach dem \hyperref[sub:39]{Satz von Inversen Operator} ist $\tilde{T}$ also ein Homöomorphismus. Mit \ref{sub:83} folgt,
	dass $\tilde{T}$ eine Umgebung von Homöomorphismen $\tilde{U}\subset\mathcal{L}(\tilde{\mathcal{H}},\mathcal{H})$ besitzt. Also besitzt $T$ eine Umgebung 
	$U \subset \mathcal{L}(\mathcal{H})$, sodass für alle $S \in U$ der Operator $\tilde{S}$ ein Homöomorphismus ist. Für jedes $S \in U$ gilt:
	\begin{enumerate}[1)]
		\item $S\enbrace*{(\ker T)^\bot} \subset \mathcal{H}$ ist abgeschlossen, denn $\tilde{S}\enbrace*{(\ker T)^\bot}= (\tilde{S}^{-1})^{-1}\enbrace*{(\ker T)^\bot}$, 
		$(\ker T)^\bot$ ist abgeschlossen und $S\enbrace*{(\ker T)^\bot} = \tilde{S} \enbrace*{(\ker T)^\bot}$
		\item Es gilt
		\[
			\dim \nicefrac{\mathcal{H}}{S\enbrace*{(\ker T)^\bot}} = \dim \nicefrac{\mathcal{H}}{\tilde{S}\enbrace*{(\ker T)^\bot}} = \dim  
			\nicefrac{\tilde{\mathcal{H}}}{(\ker T)^\bot} = \dim (\im T)^\bot
		\]
		\item Es gilt $(\ker S) \cap (\ker T)^\bot = \set{0}$: Es ist $S|_{(\ker T)^\bot} = \tilde{S}|_{(\ker T)^\bot}$, aber $\tilde{S}$ ist injektiv, da $S \in U$. 
		Insbesondere $\dim (\ker S) \le \dim (\ker T) < \infty$.
		\item $\im S \subseteq \mathcal{H}$ ist abgeschlossen: $S \enbrace*{(\ker T)^\bot} \subset S(\mathcal{H})=\im S$. Nach 1) ist 
		$S \enbrace*{(\ker T)\bot} \subset \mathcal{H}$ abgeschlossen. Es gilt
		\[
			\dim \enbrace*{\nicefrac{S(\mathcal{H})}{S \enbrace*{(\ker T)\bot} }} \le \dim \enbrace*{\nicefrac{\mathcal{H}}{S \enbrace*{(\ker T)^\bot} }}  
			= \dim (\im T)^\bot < \infty
		\]
		Also ist $S(\mathcal{H})= S \enbrace*{(\ker T)^\bot} + V$ für ein $V \subset \mathcal{H}$ endlichdimensional und damit ist $S(\mathcal{H}) \subset \mathcal{H}$
		abgeschlossen.
		\item Es ist $\dim \nicefrac{\mathcal{H}}{S(\mathcal{H})} \le \dim  \nicefrac{\mathcal{H}}{S \enbrace*{(\ker T)^\bot}}<\infty$.
	\end{enumerate}
	Aus 3), 4) und 5) folgt nun, dass $S \subset U$ ein Fredholm-Operator ist.
	\item Sei $S \in U$. Dann ist $(\ker S) \cap (\ker T)^\bot \stackrel{\text{3)}}{=} \set{0}$, also $\ker S \times (\ker T)^\bot \times Z \cong \mathcal{H}$ als Vektorräume
	für ein $Z \subset \mathcal{H}$ endlichdimensional. Dann gilt
	\[
		\dim (\ker S) + \dim Z =\dim (\ker T)
	\]
	Wir wissen bereits, dass $\dim \enbrace*{\nicefrac{\mathcal{H}}{S \enbrace*{(\ker T)^\bot} }}=\dim (\im T)^\bot =\dim \enbrace*{\nicefrac{\mathcal{H}}{\im T}}$ gilt.
	Es ist $\im S \cong S \enbrace*{(\ker T)^\bot} \times S(Z)$, also
	\[
		\dim \enbrace*{\nicefrac{\mathcal{H}}{\im S}} = \dim \enbrace*{\nicefrac{\mathcal{H}}{S \enbrace*{(\ker T)^\bot}}} - \dim \enbrace*{S(Z)}
		= \underbrace{\dim \enbrace*{\nicefrac{\mathcal{H}}{\im T}} - \dim (\ker T)}_{=\idx T} + \dim (\ker S)
	\]
	Also ist $\idx S = \idx T$. \bewende
\end{enumerate}
% subsection 93 (end)

\subsection{Corollar} % (fold)
\label{sub:94}
Sei $K \in \mathcal{K}(\mathcal{H})$. Dann gilt $\idx(\id_\mathcal{H}- K)=0$.
\minisec{Beweis}
Die Abbildung $t \mapsto \id_\mathcal{H} - t \cdot K$ ist stetig, also ist $\idx \enbrace*{\id_\mathcal{H}- t \cdot K}=\idx \id_\mathcal{H}=0$. \bewende
% subsection 94 (end)

\subsection{Beispiele} % (fold)
\label{sub:95}
\begin{enumerate}[(i)]
	\item Sei $S \in \mathcal{L}\enbrace*{\ell^2(\mathds{N})}$ der unilaterale Shift\index{unilateraler Shift}, d.h. 
	$S \enbrace*{(\alpha_i)_i}= (0, \alpha_0, \alpha_1, \ldots )$. $S$ ist offensichtlich isometrisch, also auch injektiv und somit ist $\dim (\ker S)=0$. Weiter ist
	$\dim \enbrace*{\nicefrac{\ell^2(\mathds{N})}{\im S}} =1$. Damit ist $S$ ein Fredholm-Operator mit $\idx S = -1$.
	\item Für $k \in C \enbrace*{[0,1] \times [0,1]}$ sei $K \in \mathcal{L} \enbrace*{L^2([0,1])}$ gegeben durch
	\[
		K(f)(t) := \int_{[0,1]}\hspace{-1em} k(s,t)\cdot f(s)\mathd s
	\]
	Es ist $K \in \mathcal{K} \enbrace*{L^2([0,1])}$. Dann ist $\ker(\id-K)$ der Lösungsraum der Integralgleichung $f(t)= \int \!k(s,t)\cdot f(s) \mathd s$.
	\item Sei $M$ ein $C^\infty$-Mannigfaltigkeit, $\mu$ ein verträgliches Borelmaß. Dann ist $C^\infty(M) \subset L^2(M)$ dicht. Sei $\D \colon C^\infty(M) \to C^\infty(M)$
	ein geeigneter Differentialoperator. $\D$ ist im Allgemeinen nicht beschränkt. Aber dann ist der Operator 
	\[
		\tilde{\D} := \frac{\D}{\sqrt{1+ \D^* \D}} 
	\]
	beschränkt und dicht definiert und lässt sich auf ganz $L^2(M)$ fortsetzen. Unter geeigneten Bedingungen ist $\tilde{\D}$ Fredholm. $\ker \tilde{\D}$ bzw. 
	$\coker \tilde{\D}$ entsprechen Lösungsräumen von partielle Differentialgleichungen.
\end{enumerate}
% subsection 95 (end)
% section 9 (end)
\newpage

\section{Ausblick} % (fold)
\label{sec:10}

\subsection{Frage} % (fold)
\label{sub:101}
Sei $T \in \mathcal{L}(\mathcal{H})$. ¿Existiert ein nicht trivialer Unterraum $X \subset \mathcal{H}$ mit $T(X) \subset X$? Das gilt nicht für Operatoren auf Banachräumen 
(Per Enflo, \url{https://de.wikipedia.org/wiki/Per_Enflo}).
% subsection 101 (end)

\subsection{Beispiel} % (fold)
\label{sub:102}
\begin{enumerate}[(i)]
	\item Sie $G$ eine abzählbare diskrete Gruppe. Sei $u \colon G \to \mathcal{L}\enbrace*{\ell^2(G)}$ gegeben durch $u_g(f)(h) := f (g ^{-1} h)$ für 
	$g \in G, f \in \ell^2(G), h \in G$. Betrachte 
	\begin{align*}
		C_r^*(G) &:= \overline{\mathrm{Alg}(u_g : g \in G)}^{\norm{\cdot}} \subset \mathcal{L}\enbrace*{\ell^2(G)} \\
		\mathrm{VN}(G) &:= \overline{\mathrm{Alg}(u_g : g \in G)}^{\mathrm{w.o.t.}} \subset \mathcal{L}\enbrace*{\ell^2(\mathds{N})} 
	\end{align*}
	Was ist die Struktur dieser Algebra? ¿$\mathrm{VN}(\mathds{F}_m) \cong \mathrm{VN}(\mathds{F}_n)$?
	\item Sei $\Omega$ ein kompakter Hausdorffraum, $\alpha \colon \Omega \to \Omega$ ein Homöomorphismus. $\leadsto$ Wirkung von $\mathds{Z}$ auf $C(\Omega)$.
	$(\Omega,\alpha)$ heißt dynamischer System. $\leadsto C(\Omega) \rtimes \mathds{Z}$. Zum Beispiel
	$\Omega =S^1$, $\alpha$ Rotation um $2 \pi \Theta$. Nun ist interessant, ob $\Theta \in \mathds{Q}$ oder $\Theta \not\in \mathds{Q}$. Damit ergibt sich die 
	Rotationsalgebra $A_\Theta$, die sich den Winkel $\Theta$ \enquote{merkt}.
\end{enumerate}
% subsection 102 (end)
% section 10 (end)

\cleardoubleoddemptypage
\appendix
\section{Anhang} % (fold)
\label{sec:anhang}

\subsection{Vierecksungleichung} % (fold)
\label{sub:vier_ungl}
Sei $(X,d)$ ein metrischer Raum und $x,y,u,v \in X$. Dann gilt
\[
	\abs[\big]{d(x,y) - d(u,v)} \le d(x,u) + d(y,v) 
\]
\minisec{Beweis}
Einerseits gilt nach der Dreiecksungleichung
\[
	d(x,y) \le d(x,u) + d(u,v) + d(v,y) \quad \Longrightarrow \quad d(x,y) - d(u,v) \le d(x,u) + d(y,v)
\]
Andererseits aber auch
\[
	d(u,v) \le d(u,x) + d(x,y) + d(y,v) \quad \Longrightarrow \quad d(u,v) - d(x,y) \le d(x,u) + d(y,v)
\]
Insgesamt folgt also die Behauptung. \bewende
% subsection vier_ungl (end)

\subsection{Abschluss einer konvexen Menge ist konvex} % (fold)
\label{sub:ab_konvex}
Sei $X$ ein topologischer Vektorraum, von dem wir der Einfachheit halber annehmen, dass die Topologie von einer Norm induziert wird. Sei $M \subset X$ konvex. Dann ist auch 
$\overline{M}$ konvex.
\minisec{Beweis}
Seien $a,b \in \overline{M}$, $\lambda \in [0,1]$. Dann existieren für jedes $\varepsilon>0$ Elemente $a',b' \in M$ mit $\norm{a-a'}< \varepsilon$ und 
$\norm{b-b'}< \varepsilon$. Dann gilt
\begin{align*}
	\norm{\enbrace[\big]{(1-\lambda)\cdot a +\lambda \cdot b} -\enbrace[\big]{(1-\lambda)\cdot a'+\lambda \cdot b'}} &= \norm{(1-\lambda)\cdot (a-a') +\lambda\cdot (b-b')}\\
	&\le \abs*{1-\lambda} \cdot \norm{a-a'} + \abs*{\lambda} \cdot \norm{b-b'}  \\
	&\le \abs*{1- \lambda + \lambda} \cdot \varepsilon  = \varepsilon 
\end{align*}
Da $\varepsilon>0$ beliebig war, folgt somit $(1-\lambda) \cdot a + \lambda \cdot b \in \overline{M}$. \bewende
% subsection ab_konvex (end)

\subsection{Beweis von Aufgabe 2 von Blatt 7} % (fold)
\label{sub:blatt7_aufg2}
\emph{Dies ist eine Teilaussage im Beweis des \hyperref[sub:411]{Satzes von Riesz-Fischer (\ref*{sub:411})}.}\\
Es sei $(X,\Sigma,\mu)$ ein Maßraum, $p \in [1,\infty)$ und $(f_n)_n \subset \mathcal{L}^p(\mu)$ eine Folge $p$-integrierbarer Funktionen.
\minisec{Behauptung}
Es existieren messbare Mengen $E_l \subset X$ mit $\mu(E_l)< \infty$, $l \in \mathds{N}$, sodass für $E := \bigcup_{l \in \mathds{N}}E_l$ gilt $\chi_E \cdot f_n = f_n$ fast
überall für $n \in \mathds{N}$.
\minisec{Beweis}
Sei $n \in \mathds{N}$. Für $l \in \mathds{N}$ definieren wir
\(
	E_{l,n} := f_n^{-1}\enbrace[\Big]{\enbrace*{-\infty,-\frac{1}{l}} \cup \enbrace*{\frac{1}{l},\infty} } 
\).
Da $\enbrace*{-\infty,-\frac{1}{l}}$ und $\enbrace*{\frac{1}{l},\infty}$ Borelmengen sind und $f_n$ messbar ist, ist $E_{l,n}\subset X$ messbar, also $E_{l,n} \in \Sigma$.
Wir zeigen, dass $\mu(E_{l,n})<\infty$ gilt: Für $x \in E_{l,n}$ gilt $\abs*{f_n(x)} > \frac{1}{l} \Rightarrow \abs*{f_n(x)}^p > \frac{1}{l^p}$. Es folgt mit der Monotonie 
des Integrals
\begin{align*}
	\int_{E_{l,n}} \!\! \frac{1}{l^p} \mathd\mu < \int_{E_{l,n}} \hspace{-1em}\abs*{f_n}^p\mathd\mu < \infty
\end{align*}
da $f_n \in \mathcal{L}^p(\mu)$. Da $\frac{1}{l^p}<\infty $, folgt also $\mu(E_{l,n})<\infty$. Setze nun $E := \bigcup_{l,n} E_{l,n}$. Dies ist eine abzählbare Vereinigung.
Es bleibt $\chi_E \cdot f_n = f_n$ fast überall zu zeigen. Sei also $x \in X$ beliebig. Wenn $f_n(x)=0$, dann gilt die Gleichung trivialerweise. Andernfalls existiert ein 
$l \in \mathds{N}$ mit $\abs*{f_n(x)} > \frac{1}{l}$, also $x \in E_{l,n} \subset E$. Wenn wir für $f_n$ den Wert $\infty$ zulassen, dann gilt die Gleichung auf der Nullmenge
$\set[x \in X]{f_n(x)=\infty}$ nicht. Also gilt die Gleichung fast überall. \bewende
% subsection blatt7_aufg2 (end)

\subsection{Beweis der Aussagen aus \ref{sub:731}} % (fold)
\label{sub:bew_731}
Wir zeigen zunächst $T \le \norm{T} \cdot \id_{\mathcal{H}}$
\[
	\skal*{x}{\norm{T} \cdot x - T x} = \norm{T}\cdot \skal*{x}{x} - \skal*{x}{T x} \stackrel{\mbox{\scriptsize \ref{sub:729}}}{\ge} \norm{T} \cdot \norm{x}^2 - 
	\norm{T} \cdot \norm{x}^2 = 0  
\]
Genauso folgt $-\norm{T} \cdot \id_{\mathcal{H}} \le T$ aus der Abschätzung $\abs*{\skal*{x}{Tx}} \le \norm{T} \cdot \norm{x}^2$ (\ref{sub:729})
\[
	\skal[\big]{x}{T x - (- \norm{T}\cdot x)} = \skal*{x}{T x} + \norm{T}\cdot \skal*{x}{x} \ge 0 
\]
Die Abbildung $\skal*{\cdot}{\cdot}_T \colon  (y,x) \mapsto \skal*{y}{T x}$ ist positiv-semidefinit hermitesch, da $T$ selbstadjungiert ist und $T\ge 0$ gilt. Also können 
wir die \hyperref[sub:74]{Cauchy-Schwarz-Ungleichung} anwenden:
\[
	\abs*{\skal*{y}{Tx}}^2=\abs*{\skal*{y}{x}_T}^2 \le \skal*{y}{y}_T \cdot \skal*{x}{x}_T = \skal*{y}{T y} \cdot \skal*{x}{T x}
\]
Damit ist auch die letzte Aussage bewiesen. \bewende
% subsection bew_731 (end)

\subsection{Ausführlicher Beweis zu \ref{sub:83} mit Neumann-Reihen (Aufgabe 2 von Blatt 5)} % (fold)
\label{sub:bew_83}
Seien $X,Y$ Banachräume und $T \in \mathcal{L}(X,Y)$ bijektiv (also injektiv). Falls für $S \in \mathcal{L}(X,Y)$ gilt 
\[
	\norm[\mathcal{L}(X,Y)]{T-S} < \norm[\mathcal{L}(Y,X)]{T^{-1}}^{-1}, 
\]
so ist auch $S$ bijektiv.
\minisec{Über Neumann-Reihen}
Für $T \in \mathcal{L}(X,X)$ ist die \Index{Neumann-Reihe} definiert als 
\[
	\sum_{n=0}^{\infty} T^n
\]
Sie hat unter anderem die folgenden wichtigen Eigenschaften\footnote{siehe auch \url{https://de.wikipedia.org/wiki/Neumann-Reihe}}
\begin{enumerate}[(i)]
	\item Wenn $\sum_{n=0}^{\infty} T^n$ bezüglich $\norm{\cdot}$ konvergiert, so ist $\id_X - T$ invertierbar mit $(\id_X-T)^{-1}= \sum_{n=0}^{\infty}T^n$
	\item Die Reihe konvergiert, falls $(X,\norm{\cdot})$ ein Banachraum ist und $\norm{T}<1$ gilt.
\end{enumerate}
Ersteres sieht man leicht, denn wenn die Summe konvergiert, dann gilt
\[
	\lim_{k \to \infty} \enbrace*{\id_X -T} \enbrace*{\sum_{n=0}^{k} T^n} = \lim_{k \to \infty} \enbrace*{\sum_{n=0}^{k} T^n - \sum_{n=0}^{k} T^{n+1}} 
	= \lim_{k \to \infty} \enbrace*{\id_X - T^{k+1}} = \id_X
\]
Für die zweite Aussage zeigt man, dass $\sum_{n=0}^{\infty}\norm{T^n}$ konvergiert:
\[
	\sum_{n=0}^{k} \norm{T^n} \le \sum_{n=0}^{k} \norm{T}^n \le \sum_{n=0}^{\infty} \norm{T}^n \stackrel{\norm{T}<1}{=} \frac{1}{1- \norm{T}}< \infty    
\]
Jetzt muss man nur noch zeigen, dass in Banachräumen aus absoluter Konvergenz normale Konvergenz folgt, um das Argument abzuschließen.
\minisec{Beweis der der eigentlichen Aussage}
Es gilt $S=T-(T-S)=T \enbrace*{\id_X - T ^{-1}(T-S)}$. Wir erhalten nun
\begin{align*}
	\norm{T ^{-1}(T-S)} = \sup_{x \in X} \norm[X]{T ^{-1} \enbrace*{T x- S x} } \le \sup_{x \in X} \norm{T ^{-1}} \cdot \norm[Y]{T x- S x}
	= \norm{T ^{-1}} \cdot \norm{T-S} < 1  
\end{align*}
Also ist $\id_X - T ^{-1}(T-S)$ nach den obigen Eigenschaften der Neumann-Reihe invertierbar. Da $T$ invertierbar ist, ist $S= T \enbrace*{\id_X - T ^{-1}(T-S)}$ als Komposition 
invertierbarer Abbildungen auch invertierbar mit $S ^{-1} = \enbrace*{\sum_{n=0}^\infty \enbrace*{T ^{-1}(T-S)}^n} T ^{-1}$. \bewende
% subsection ausfuhrlicher_beweis_zu_ref_sub_83 (end)

% section anhang (end)
\cleardoubleoddemptypage
\pagenumbering{Alph}
\setcounter{page}{1}
\printindex
\listoffigures
\todototoc
\listoftodos[To-do's und andere Baustellen]
\end{document}
