\input{../!config/Tazdr/extra_files/Praeambel.tex}

\newcommand{\vorlesung}{Modellreduktion und partielle Differentialgleichungen}
\newcommand{\Prof}{Dr. Smetana}
\newcommand{\subt}{Mitschrift der Tafelnotizen}

\input{../!config/Tazdr/extra_files/headings.tex}


\numberwithin{equation}{section}
\numberwithin{figure}{section}

\begin{document}
\maketitle
\thispagestyle{empty}
\cleardoubleoddemptypage

\thispagestyle{empty}
\vspace*{\fill}
\begin{center}
	Hierbei handelt es sich um eine \subt von \textbf{\Prof}, WWU Münster, aus der Vorlesung \textbf{\vorlesung} im Wintersemester 2015/16. 
	Dies ist kein Skript der Vorlesung und keine eigene Arbeit des Autors.\\
	\vspace{2cm}
	Für Fehler in der Mitschrift wird keine Haftung übernommen. 
	Hinweise auf Fehler sind gerne gesehen, hierfür kann man mich in der Uni ansprechen oder alternativ eine e-Mail an: \textit{tobias.wedemeier@gmx.de}\\
	Auch ist eine Mitarbeit über Github möglich.\\
	\vspace{2cm}
	Wenn Teile aus der Vorlesung selber fehlen, können diese gerne an meine e-Mail versandt werden. 
	Ich werde diese dann einarbeiten.\\
\end{center}
\vspace*{\fill}
\cleardoubleoddemptypage

\pagenumbering{Roman}

\tableofcontents
\cleardoubleoddemptypage %sorgt dafür, dass alles folgende erst auf der nächsten freien "rechten" Seite steht
\pagenumbering{arabic}

\section{Einleitung und Motivation}

\subsection{Parameterabhängige PDGL}
\label{sub:para_pdgl}
Sei $\Omega\subset \R^d$ ein polygonales, beschränktes Gebiet. Zu einem Parametervektor $\mu \in P\subset \R^d$ aus einer Menge von 'erlaubten' Parametern ist eine Funktion, z.B. 'Temperatur'
\[
u(\mu): \Omega\to \R
\]
gesucht, so dass $-\nabla\big(\kappa(\mu)\nabla u(\mu)\big) = q(\mu)$ in $\Omega$, wobei $u(\mu) = 0$ auf $\partial\Omega$, mit $\kappa(\mu):\Omega\to \R$ dem 'Wärmeleitkoeffizient' und $q(\mu)$ eine 'Wärmequelle', z.B. $q(\mu)=1$.
%grafik 1
Weiter kann eine Ausgabe erwünscht sein, z.B.
\[
s(\mu) = \frac{1}{\abs{\Omega_s}} \int_{\Omega_s} u(x;\mu)\dint x,
\]
die mittlere Temperatur auf $\Omega_s$.
%grafik 2
%sub end

\subsection{Definition (schwache Formulierung in Hilberträumen)}
\label{sub:def_hilbert}
Sei $X$ ein reeller Hilbertraum.
Zu $\mu\in P$ ist gesucht ein $u(\mu)\in X$ und eine Ausgabe $s(\mu)\in \R$, so dass
\[
b\big(u(\mu),v;\mu\big) = f(v;\mu), ~ s(\mu) = l\big(u(\mu);\mu\big)~\forall v\in X
\]
für eine Bilinearform $b(\cdot,\cdot;\mu): X\times X \to \R$ und linearen Funktionalen $f(\cdot;\mu),l(\cdot;\mu):X\to \R$.

Die schwache Formulierung für Beispiel 1.1 lautet:
\[
X:= H_0^1(\Omega)= \left\lbrace f\in L^2(\Omega)~:\difff{ }{x_i}f\in L^2(\Omega),~f|_{\partial \Omega = 0} \right\rbrace
\]
Dann kann man die Bilinearform über
\[
b\big(u(\mu),v;\mu\big) := \int\limits_{\Omega} \kappa(\mu)\nabla u(\mu)\nabla v \dint x;~ f(v;\mu) := \int\limits_{\Omega} q(\mu) v \dint x
\]
ausdrücken und 
\[
s(\mu) = \frac{1}{\abs{\Omega_s}} \int\limits_{\Omega_s} u(x;\mu)\dint x =: l\big(u(\mu);\mu\big)
\]
ABER: Für sehr wenige PDGL's können wir die Lösung analytisch bestimmen.
Daher sind wir an einer numerische Approximation interessiert.
Ein weit verbreitetes Diskretisierungsverfahren ist die Finite Elemente Methode.
Diese Methode basiert auf obiger schwacher Formulierung.
%sub end

\subsection{Definition (hochdimensionales, diskretes Modell)}
\label{sub:def_hochdim}
Sei $X_h\subseteq X$ mit $\dim(X_h) =N_h < \infty$.
Der Index $h$ bezeichnet hier die Gitterweite.
Zu $\mu\in P$ ist gesucht ein $u_h(\mu)\in X_h$ und eine Ausgabe $s_h(\mu)\in \R$, so dass
\begin{align}
b\big(u_h(\mu),v_h;\mu\big) = f(v_h;\mu), ~ s_h(\mu)= l_h\big(v_h(\mu);\mu\big)~\forall v_h\in X_h.
\end{align}
Anwendungen für die Standarddiskretisierungsverfahren sehr teuer oder zu teuer sind:
\minisec{many-query context}
\begin{itemize}
	\item Parameterstudien
	\item Design
	\item Parameteridentifikation / inverse Probleme
	\item Optimierung
	\item Statistische Analyse
\end{itemize}
\minisec{schnelle Simulationsantwort}
\begin{itemize}
	\item Echtzeit-Steuerung technischer Geräte
	\item interaktive Benutzeroberflächen
\end{itemize}
%sub end

\subsection{Parameterabhängige Lösungsmenge}
\label{sub:para_menge}
Sei $M :=\{u(\mu)~:~\mu\in P\}\subseteq P$ für $P\in \R^p$ ist die durch $\mu$ parametrisierte Lösungsmenge.
$X$ ist im Allgemeinen unendlichdimensional.
%grafik
$\Rightarrow$ Motivation für die Suche nach einem 'niedrigdimensionalen' Teilraum $X_N\subseteq X$ zur Approximation von $M$ und einer Approximation $u_N(\mu) \approx u(\mu),~ u_N\in X_N$.
Eine Möglichkeit eine reduzierte Basis zu generieren besteht darin geschickt Parameterwerte $\mu_1\dt{,}\mu_N\in P$ zu wählen und den Raum als $X_N := \spann\{u(\mu_1)\dt{,}u(\mu_N)\}$ zu definieren.
Eine Lösung $u(\mu_i)$ für einen Parameterwert $\mu\in P$ wird auch \bet{Snapshot} genannt.

\subsection{Beispiel}
\label{sub:beispiel_1}
Gesucht ist $u(\cdot;\mu) \in C^2([0,1])$ mit $(1+\mu)u'' = 1$ auf $(0,1)$ und $u(0)=u(1)=1$ für den Parameter $\mu\in P:= [0,1]\subseteq \R$.\\
\bet{Snapshots:}\\
$\mu_1 = 0 \Rightarrow u_1 := u(\cdot;\mu_1) = \frac{1}{2} x^2- \frac{1}{2}x +1$, $\mu_2 = 1 \Rightarrow u_2 := u(\cdot;\mu_2) = \frac{1}{4} x^2- \frac{1}{4}x +1$ und $X_N := \spann\{u_1,u_2\}$.
Dann ist die reduzierte Lösung $u_N(\mu)\in X_N$ gegeben durch
\[
u_N(\mu) = \alpha_1(\mu)u_1 +\alpha_2(\mu) u_2,
\]
mit $\alpha_1 = \frac{2}{\mu+1}-1$ und $\alpha_2 = 2-\frac{2}{\mu-1}$.
Diese erfüllt folgende Fehleraussage und ist somit exakt:
\[
\norm{u_N(\mu)-u(\mu)}_\infty = \sup\limits_{x\in[0,1]} \abs{u_N(x;\mu)-u(x;\mu)} = 0
\]
Da $\alpha_1 + \alpha_2 = 1$ und $0\le \alpha_1,\alpha_2\le 1$ ist $M$ die Menge der Konvexkombinationen von $u_1$ und $u_2$.
%sub end

\subsection{Definition (reduziertes Modell)}
\label{sub:reduziertes_modell}
Sei $X_N\subseteq X$ ein reduzierter Basisraum mit $\dim(X_N)<\infty$.
Zu $\mu\in P$ ist gesucht ein $u_N(\mu)\in X_N$ und eine Ausgabe $s_N(\mu)\in \R$, so dass
\begin{align}
b\big(u_N(\mu),v_N;\mu\big) = f(v_N;\mu),~ s_N(\mu) = l_N\big(u_N(\mu);\mu\big) ~ \forall v_N\in X_N
\end{align}
%sub end

\subsection{Bemerkung (Begrifflichkeiten)}
\label{sub:bem_begrifflichkeit}
Zusammengefasst unterscheiden wir zwischen den folgenden drei Modellen:
\begin{enumerate}[1)]
	\item Eine partielle DGL ist ein \bet{analytisches Modell}, welches die analytische Lösung $u(\mu)\in X$ in einem (typischerweise) $\infty$-dimensionalen Funktionenraum charakterisiert.
	\item Ein \bet{hochdimensionales, diskretes Modell} ist ein Berechnungsverfahren zur Bestimmung einer Näherung $u_h(\mu)\in X_h$, wobei $X_h$ ein hochdimensionaler Funktionenraum ist.
	Beispiele sind \bet{Finite Elemente} oder \bet{Finite Volumenräume} und typischerweise hat $X_h$ eine Dimension von mindestens $10^5$.
	\item Ein \bet{reduziertes Modell} ist ein Berechnungsverfahren zur Bestimmung einer Näherung $u_N(\mu)\in X_N$ in einem sehr problemangepassten und daher niedrigdimensionalen Raum von typischerweise $\dim X_N \leq 100$.
	\item \bet{Modellreduktion} beschäftigt sich mit Methoden der Erzeugung von reduzierten Modellen aus hochdimensionalen, diskreten (oder auch analytischen) Modellen und Untersuchungen ihrer Eigenschaften.
\end{enumerate}

\subsection{Organisation der Vorlesung}
\label{sub:org}
\begin{itemize}
	\item[Zentrale Fragen:]
	\item \bet{Reduzierte Basis:} Wie kann ein möglichst kompakter Teilraum konstruiert werden?
	\item \bet{Reduziertes Modell:} Existenz von reduzierten Lösungen $u_N(\mu)$?
	Wie kann eine reduzierte Lösung $u_N(\mu)$ berechnet werden?
	\item \bet{Effizienz:} Wie kann $u_N(\mu)$ schnell berechnet werden?
	\item \bet{Stabilität:} Wie kann die Stabilität des reduzierten Modells für wachsendes $N$ garantiert werden?
	\item\bet{Approximationsgüte:} Warum können wir erwarten, dass eine relativ kleine Anzahl von Basisfunktionen ausreicht?
	\item \bet{Fehlerschätzer:} Kann der Fehler des reduzierten zum vollen Modell beschränkt werden?
	\item \bet{Effektivität:} Kann garantiert werden, dass der Fehlerschätzer den Fehler nicht beliebig überschätzt?
\end{itemize}

\minisec{Vorläufige Gliederung (bis Weihnachten)}
\begin{enumerate}[1)]
	\item Einleitung / Motivation
	\item Grundlagen:
	\begin{itemize}
		\item Kurze Einführung in lineare Funktionalanalysis
		\item Kurze Einführung in Finite Elemente
	\end{itemize}
	\item Reduzierte Basis Methoden für lineare, koerzive Probleme
	\begin{itemize}
		\item Reduzierte Basis Verfahren
		\item Offline-/ Online-Zerlegung
		\item Fehlerschätzer
		\item Basisgenerierung
	\end{itemize}
\end{enumerate}
%sub end

%sec end

\subsection{Literatur}
\label{sub:lit}

\begin{enumerate}
\item RB Methoden:
\begin{itemize}
\item B. Haasdonk: Skript zur Vorlesung Reduzierte-Basis-Methoden, Preprint IANS, Uni Stuttgart, 2011.
\item A. T. Patera and G. Rozza: Reduced Basis Approximation and A Posteriori Error Estimation for Parameterized Partial Differential Equations , Version 1.0, Copyright MIT 2006, to appear in (tentative rubric) MIT Pappalardo Graduate Monographs in Mechanical Engineering.
\item A. Quarteroni, A. Manzoni, and F. Negri: Reduced Basis Methods for Partial Differential Equations. An Introduction, UNITEXT Series, Springer, 2015.
\item A. Pinkus: n-widths in approximation theory, Springer, 1985.
\end{itemize}
\item Funktionalanalysis
\begin{itemize}
\item H. W. Alt: Lineare Funktionalanalysis, Springer, 2006. 
\item D. Werner: Funktionalanalysis, Springer, 2007.
\end{itemize}
\item Partielle Differentialgleichungen/ Finite Elemente
\begin{itemize}
\item L. C. Evans: Partial Differential Equations, AMS, 2002.
\item M. Ohlberger: Skript zur Vorlesung Numerik partieller Differentialgleichungen I, WWU, 2012.
\item G. Dziuk: Theorie und Numerik partieller Differentialgleichungen, de Gruyter, 2010.
\item D. Braess: Finite Elemente, Springer 2003.
\item A. Ern and J.-L. Guermond: Theory and practice of finite elements, Springer, 2004.
\item S. C. Brenner and L. R. Scott: The mathematical theory of finite element methods, Springer, 2008.
\end{itemize}
\end{enumerate}

\section{Grundlagen}

\subsection{Lineare Funktionalanalysis in Hilberträumen}

\subsubsection{Lineare Operatoren}

\ssect{2.1 Definition}{(Hilbertraum)}
Sei $X$ ein reeller Vektorraum mit $(\cdot,\cdot):X\times X\to \R$ ein Skalarprodukt und induzierter Norm $\norm{x}:= \sqrt{(x,x)}$. Falls $X$ vollständig bzgl. $\norm{.}$ , ist $X$ ein (reeller) \bet{Hilbertraum} (HR).

\ssect{2.2 Beispiele}{(Hilbertraum)}
\begin{enumerate}[(1)]
	\item $X:= \R^d$ mit $(x,y) := \sum_{i=1}^{d} x_i y_i$ ist ein HR.
	\item $X:= L^2(\Omega)$ mit $(x,y) := \int_{\Omega} f(x)g(x)\dint x $ ist ein HR.
	\item $X:=C^0([0,1])$ mit $(f,g) := \int_{0}^{1} f(x) g(x) \dint x$ ist kein HR.
\end{enumerate}

\ssect{2.3 Lemma}{ }
Seien $X$ und $Y$ reelle Vektorräume.
Ist die Abbildung $T:X\to Y$ linear und $x_0\in X$, so sind äquivalent:
\begin{enumerate}[(1)]
	\item $T$ ist stetig.
	\item $T$ ist stetig in $x_0$.
	\item $\sup\limits_{\norm{x}_X \le 1} \norm{Tx}_Y < \infty$.
	\item $\exists$ Konstante $C$ mit $\norm{Tx}_Y \le C\norm{x}_X ~\forall x\in X$.
\end{enumerate}
\bet{Beweis:} s. \cite{Alt06}, Lemma 3.1, S.~141f.

\ssect{2.4 Definition}{(Lineare Operatoren)}
Seinen $X$ und $Y$ reelle Vektorräume.
Wir definieren
\[
L(X;Y) := \lbrace T:X\to Y~;~ T \text{ ist linear und stetig} \rbrace.
\]
Abbildungen in $L(X;Y)$ nennen wir \bet{lineare Operatoren}.
Nach Lemma 2.3 (3) ist für jeden Operator $T\in L(X;Y)$ die \bet{Operatornorm} von $T$ definiert durch
\[
\norm{T}_{L(X;Y)} := \sup\limits_{\norm{x}_X\le 1} \norm{Tx}_Y < \infty,
\]
oder in kurz $\norm{T}$.
Es ist $L(X) := L(X;X)$.

\ssect{2.5 Definition}{(Spezielle lineare Operatoren)}
\begin{enumerate}[(1)]
	\item $X' := L(X;\R)$ ist der \bet{Dualraum} von $X$.
	Die Elemente von $X'$ nennen wir auch \bet{lineare Funktionale}.
	\item Die Menge der kompakten (linearen) Operatoren von $X$ nach $Y$ ist definiert durch
	\[
	K(X;Y) := \lbrace T\in L(X;Y)~;~ \overline{T(B_1(0))} \text{ kompakt} \rbrace.
	\]
	\item Eine lineare Abbildung $P:X\to X$ heißt (lineare) \bet{Projektion}, falls $P^2=P$.
	\item Für $T\in L(X;Y)$ ist $\ker(T) := \penbrace{x\in X~;~ Tx = 0}$ der \bet{Nullraum} oder \bet{Kern} von $T$.
	Aus der Stetigkeit von $T$ folgt, dass $\ker(T)$ ein abgeschlossener Unterraum ist.
	Der \bet{Bildraum} von $T$ ist $\bild(T) := \penbrace{Tx\in Y~;~x\in X}$.
	\item Ist $T\in L(X;Y)$ bijektiv, so ist $T^{-1}\in L(Y;X)$.
	Dann heißt $T$ (linear, stetiger) \bet{Isomorphismus}.
	\item $T\in L(X;Y)$ heißt \bet{Isometrie}, falls
	\[
	\norm{Tx}_Y = \norm{x}_X ~\forall x\in X
	\]
\end{enumerate}

\ssect{2.6 Beispiel}{ }
Sei $g\in L^2(\Omega)$.
Dann ist nach der Hölderungleichung durch 
\[
T_g f := \int_{\Omega} f(x) g(x)\dint x
\]
ein Funktional $T_g\in  L^2(\Omega)'$ definiert.

\ssect{2.7 Satz}{(Projektionssatz)}
Sei $X$ ein Hilbertraum und $A\subseteq X$ nicht leer, abgeschlossen und konvex.
Dann gibt es genau eine Abbildung $P:X\to A$ mit
\[
\norm{x-Px}_X = \dist(x,A) = \inf\limits_{y\in A} \norm{x-y}_X ~\forall x\in X.
\]
Die Abbildung $P:X\to A$ heißt orthogonale Projektion von $X$ auf $A$.

\bet{Beweis:} s. \cite{Alt06}, Satz 2.2, S.~96.

\ssect{2.8 Folgerung}{ }
Ist $A\subseteq X$ nicht-leer, abgeschlossen und Unterraum, so ist $P$ linear und $Px\in A$ charakterisiert durch $(x-Px,a)_X = 0~\forall a\in A$.
Falls $\dim(A) = n<\infty$ und $\enbrace{\varphi_i}_{i=1}^n$ Orthonormalbasis von $A$, gilt
\[
Px = \sum_{i=1}^{n} (x, \varphi_i)_X\varphi_i.
\]

\ssect{2.9 Satz}{(Riesz'scher Darstellungssatz)}
Ist $X$ Hilbertraum, so ist $J:X\to X'$ definiert durch
\[
J(v)(w) := (v,w)_X ~\forall v,w\in X 
\]
eine stetige, lineare, bijektive Isometrie.
Insbesondere existiert zu $l\in X'$ ein eindeutiger \bet{Riesz Repräsentant} $V_l := J^{-1}(l)\in X$ mit $l(.) = (v_l,.)_X$.\\

\bet{Beweis:}\\
C-S-Ungleichung: $\abs{J(v)(w)}\le \norm{v}_X\norm{w}_X$.
Dann folgt: $ J(v)\in X'$ mit 
\[
\norm{J(v)}_{X'} = \sup\limits_{w\in X\backslash\{0\}} \frac{\abs{J(v)(w)}}{\norm{w}_X} = \sup\limits_{w\in X\backslash\{0\}} \frac{\abs{(v,w)_X}}{\norm{w}_X} \le \norm{v}_X \Rightarrow J \text{ stetig}.
\]
Da $\abs{J(v)(v)} = \norm{v}_X^2$ folgt:
\[
\sup\limits_{w\in X\backslash\{0\}} \frac{\abs{J(v)(w)}}{\norm{w}_X} \ge \frac{\abs{J(v)(v)}}{\norm{v}_X} = \frac{\norm{v}_x^2}{\norm{v}_X} = \norm{v}_X.
\]
Also ist $J$ eine Isometrie und insbesondere ist $J$ injektiv.\\
Zeige $J$ surjektiv: Sei $l\in X',~l\neq 0$, Kern$(l)$ ist abgeschlossener Teilraum, also existiert $P:X\to \ker(l)$ orthogonale Projektion nach Satz 2.7.
Sei $v_0\in X$mit $l(v_0)=1$. 
Setze $v_1:=v_0-Pv_0\Rightarrow l(v_1) =l(v_0) =1$ und $v_1\neq 0$.
Mit Folgerung 2.8:
\[
\Rightarrow (w,v)_X =0~ \forall x\in\ker(l) \Rightarrow v_1 \perp \ker(l). 
\]
Für $v\in X$ gilt
\[
v\underbracket{v-l(v)}_{\in \ker(l)}\cdot v_1 + l(v)\cdot v_1
\]
und $v-l(v)v_1\in \ker(l)$ wegen
\[
l(v-l(v)v_1) = l(v) -l(v)l(v_1) =0.
\]
Also ist
\begin{align*}
(v_1,v)_X &= (\underbracket{v_1,v-l(v)v_1}_{=0,\text{ da }\ker(l) \perp v_1})_X + (v_1,l(v)v_1)_X\\
&= l(v) \norm{v_1}_X^2\\
&\Rightarrow l(v) = \enbrace{\frac{v_1}{\norm{v_1}_X^2},v}_X = J\enbrace{\frac{v_1}{\norm{v_1}_X^2}}(v).\\
&\Rightarrow l\in \bild(J) \Rightarrow J\text{ bijektiv.}\\
\end{align*}
\hfill $\square$

\ssect{2.10 Folgerung / Beispiel:}{ }
Mit Hilfe des Rieszschen Darstellungssatz können wir damit $L^2(\Omega)$' - den Dualraum von $L^2(\Omega)$ - charakterisieren.
Wie in 2.6 definieren wir für $g\in L^2(\Omega)$ das Funktional
\[
T_g f := \int_{\Omega} f(x)g(x)\dint x.
\]

\ssect{Definition 2.11}{(Bilinearformen)}
Seien $X_1,X_2$ Hilberträume, $b:X_1\times X_2 \to \R$ eine Bilinearform.
\begin{enumerate}[(1)]
	\item Falls 
	\[
	\gamma:= \sup\lim\ablim{u\in X_1\backslash\{0\}} \qquad\sup\lim\ablim{v\in X_2\backslash\{0\}}\qquad \frac{b(u,v)}{\norm{u}_{X_1}\norm{v}_{X_2}} < \infty
 	\]
 	so ist $b$ stetig mit Stetigkeitskonstante $\gamma$.
 	\item Falls $X=X_1=X_2$, definieren 
 	\[
 	b_s(u,v) = \frac{1}{2} b(u,v)+b(v,u),~b_a = \frac{1}{2} b(u,v)-b(v,u) ~\forall u,v\in X
 	\]
 	den symmetrischen bzw. antisymmetrischen Anteil von $b = b_s +b_a$.
 	\item Falls $X=X_1=X_2$, $b$ stetig und 
 	\[
 	\alpha := \inf\limits_{u\in X\backslash\{0\}} \frac{b(u,u)}{\norm{u}_X^2} > 0
 	\]
 	heißt $b$ Koerziv mit Stetigkeitskonstante $\alpha$.
\end{enumerate}

\ssect{2.12 Bemerkung}{}
\begin{enumerate}[(1)]
	\item $\alpha \in\R$ ist wohldefiniert, denn mit Stetigkeit folgt
	\[
	\frac{b(u,u)}{\norm{u}_X^2} \ge -\gamma \frac{\norm{u}_X\norm{u}_X}{\norm{u}_X^2} = -\gamma.
	\]
	\item $b$ ist koerziv bzgl. $\alpha \Leftrightarrow b_s$ ist koerziv bzgl. $\alpha$.
\end{enumerate}

\ssect{2.13 Satz}{(Operatoren und Bilinearformen)}
Seien $X_1,X_2$ Hilberträume.
\begin{enumerate}[(1)]
	\item Zu $B\in L(X_1,X_2)$ existiert eine eindeutig definierte stetige Bilinearform $b:X_1\times X_2\to \R$ mit 
	\begin{align}
	b(u,v) = (Bu,v)_{X_2} ~\forall u\in X_1,v\in X_2.
	\end{align}
	\item Zu $b:X_1\times X_2 \to \R$ stetige Bilinearform existiert eindeutiges $B\in L(X_1,X_2)$ welches (2.1) erfüllt.
\end{enumerate}

\bet{Beweis:}\\
\begin{enumerate}[(1)]
	\item $b$ definiert durch (2.1) ist bilinear wegen Bilinearität von $(.,.)$ und Linearität von $B$.
	Stetigkeit:
	\[
	b(u,v) = (Bu,v)_{X_2} \stackrel{\text{C.S.}}{\le} \norm{B}\norm{u}_{X_1}\norm{v}_{X_2}
	\]
	daraus folgt $\gamma \le \norm{B} < \infty$.
	\item Sei $u\in X_1$ fest.
	Dann ist $b(u,.):X_2\to\R$ linear und stetig:
	\[
	\sup\limits_{v\in X_2\backslash\{0\}} \frac{b(u,v)}{\norm{v}_{X_2}} \le \sup\limits_{v\in X_2\backslash\{0\}} \frac{\norm{u}_{X_1\norm{v}_{X_2}}}{\norm{v}_{X_2}}\cdot \gamma = \gamma \norm{u}_{X_1} < \infty.
	\]
	Daraus folgt $b(u,.)\in X_2'$ und es existiert nach Satz 2.9 ein eindeutiger Riesz-Repräsentant $v_u\in X_2$ mit $b(u,.) = (v_u,.)$.
	Definiere $B:X_1\to X_2$ durch $Bu := v_u\in X_2$.
	Hiermit (2.1) und Eindeutigkeit klar.
	Linearität damit klar.\\
	Stetigkeit:
	\begin{align*}
	\norm{bu}^2 &= (Bu,Bu) = (v_u,Bu)_{X_2} = b(u,Bu) \le \gamma \norm{u}_{X_1}\norm{Bu}_{X_2}\\
	&\Rightarrow \norm{Bu}_{X_2} \le \gamma \norm{u}_{X_1} \Rightarrow \sup\limits_{u\in X_1\backslash\{0\}} \frac{\norm{Bu}_{X_2}}{\norm{u}_{X_1}} \le \gamma.
	\end{align*}
\end{enumerate}
\hfill $\square$\\

\ssect{2.14 Satz}{von Lax-Milgram}
Sei $x$ HR, $b:X\times X\to\R$ koerzive, stetige Bilinearform mit Koerzivitätskonstante $\alpha$.
Dann existiert ein eindeutiger Operator $B\in L(X)$ mit
\[
b(u,v) = (Bu,v)~\forall u,v\in X.
\]
Ferner gilt: $B$ ist bijektiv, $B^{-1}\in L(X)$ mit
\[
\norm{B} \le \gamma \text{ und } \norm{B^{-1}}\le \frac{1}{\alpha}.
\]
\bet{Beweis:} s. \cite{Alt06}, Satz 4.2, S.~172.

\subsubsection{Sobolevräume}

\ssect{2.15 Bemerkung}{(Motivation Sobolevräume)}
Wie in 1.1 motiviert, eignet sich die sogenannte Schwache Formulierung (s. 1.2) einer PDgl besonders gut um Existenz und Eindeutigkeit von Lösungen zu untersuchen.
Die dazu geeigneten Räume sind die \bet{Sobolevräume}.

\ssect{2.16 Definition}{($L_{log}^p(\Omega)$)}
Sei $\Omega\subset \R^d$ ein Gebiet.
Dann ist der Raum $L_{log}^p(\Omega)$ definiert durch 
\[
L_{log}^p(\Omega) := \penbrace{u\in L^p(K) ~|~ \forall K\subset \Omega,~ K\text{ kompakt}}.
\]

\ssect{2.17 Definition}{(schwache Ableitung)}
Sei $\alpha = (\alpha_1\dt{,}\alpha_d)\in \N^d$ ein Multiindex.
Eine Funktion $u\in L_{log}^1(\Omega)$ besitzt eine schwache Ableitung $u_\alpha \in L_{log}^1(\Omega)$, wenn für alle Testfunktionen $\varphi\in C_0^\infty(\Omega)$ gilt
\[
\int_{\Omega} u D^\alpha \varphi = (-1)^{\abs{\alpha}} \cdot \int_{\Omega} u^{(\alpha)} \varphi,
\]
mit $D^\alpha = D_1^{\alpha_1}\cdots D_d^{\alpha_d},~\abs{\alpha} = \alpha_1 \dt{+} \alpha_d$.
Wir schreiben dann auch $u^{(\alpha)} = D^\alpha u$ für die schwache Ableitung.

\ssect{2.18 Lemma}{}
Falls $u\in C^{\abs{\alpha}(\bar{\Omega})}$ und $\abs{\alpha}\ge 1$, gilt: $D^\alpha= u^{(\alpha)}$, d.h. klassische und schwache Ableitung stimmen überein.

\ssect{2.19 Beispiel}{}
Sei $\Omega =(-1,1)$ und $u(x) =\abs{x}$.
Dann ist $u'(x) = -1 (x\le 0), 1 (x> 0)$ die schwache Ableitung von $u$.\\

\bet{Beweis:}\\
Es gilt für beliebige $\varphi\in C_0^\infty(\Omega)$:
\begin{align*}
Foto
\end{align*}

\ssect{2.20 Beispiel}{}
Im Gegensatz zu $\abs{x}$ ist $v(x)= -1 (x\le 0) 1 (x>0)$ auf $\Omega = (-1,1)$ nicht schwach differenzierbar.

\ssect{2.21 Definition}{(Sobolevräume)}
Seien $m\in \N_0,~p\in [1,\infty]$ und $u\in L_{log}^p(\Omega)$.
Wir nehmen an, dass alle schwachen partiellen Ableitungen $D^\alpha u$ existieren für $\abs{\alpha} \le m$.
Dann definieren wir die \bet{Sobolevnormen} $\norm{u}_{H^{m,p}(\Omega)}$, durch
\[
\norm{u}_{H^{m,p}(\Omega)} = \enbrace{\sum_{\abs{\alpha}\le m} \norm{D^\alpha u}_{L^p(\Omega)}^p}^{\frac{1}{p}}\text{ falls } 1\le p< \infty
\]
und für $p=\infty$ als 
\[
\norm{u}_{H^{m,p}(\Omega)} := \max\lim\limits_{\abs{\alpha}\le m} \norm{D^\alpha u}_{L^\infty (\Omega)}.
\]
Schließlich definieren wir die \bet{Sobolevräume} $H^{m,p}(\Omega)$ durch
\[
H^{m,p}(\Omega) := \penbrace{u\in L_{log}^p(\Omega) ~|~ \norm{u}_{H^{m,p}(\Omega)}< \infty}.
\]

\ssect{2.22 Bemerkung}{}
Anstelle von $H^{m,p}(\Omega)$ werden die Sobolevräume in der Literatur auch oft mit $W^{m,p}(\Omega)$ bezeichnet.

\ssect{2.23 Beispiel}{}
Seien $\Omega = B_{\frac{1}{2}}(0)\subset \R^2$ und $u(x) = \ln \abs{\ln \abs{x}},~x\in \Omega$.
Dann gilt: $u\in H^{1,2}(\Omega)$, aber $u\notin C^0(\Omega)$.
D.h. Funktionen in $H^{1,p}(\Omega)$ sind in mehreren Raumdimensionen nicht notwendigerweise stetig.

\ssect{2.24 Satz}{(Vollständigkeit von Sobolevräumen)}
Sei $\Omega\subset \R^d$ ein Gebiet.
Dann ist $H^{m,p}(\Omega)$ $1\le p\le \infty,~m\in \N_0$ mit der in 2.21 definierten Norm ein Banachraum,$H^{m,p}(\Omega)$ ist ein Hilbertraum mit dem Skalarprodukt
\[
(u,v)_{H^{m,p}(\Omega)} := \sum_{\abs{\alpha}\le m} (D^\alpha u, D^\alpha v)_{L^2(\Omega)}.
\]
Da wir uns mit Randwertproblemen befassen wollen, ist es notwendig zu klären in welchem Sinne wir bei Sobolevräumen von Randwerten reden können.
Da die Funktionen zunächst nur bis auf Nullmengen definiert sind und der Rand eines Gebietes eine Nullmenge darstellt, auf der man $L^p$-Funktionen beliebig abändern kann.
In der folgenden Definition klären wir zunächst was wir unter Nullrandwerten im schwachen Sinne verstehen wollen.

\ssect{2.25 Definition}{(schwache Nullrandwerte)}
Für $1\le p\le \infty$ und $m\in \N$ definieren wir die Sobolevräume mit Nullrandwerten $H_0^{m,p}(\Omega)$ durch
\[
H_0^{m,p}(\Omega) := \overline{C_0^m(\Omega)}^{\norm{.}_{H^{m,p}(\Omega)}}.
\]

\ssect{2.26 Satz}{}
Für $1\le p<\infty$ ist $H_0^{m,p}(\Omega)$ ein abgeschlossener Teilraum von $H^{m,p}(\Omega)$ und damit ein Banachraum.\\

Dass aus der Definition von $H_0^{m,p}(\Omega)$ tatsächlich folgt, dass solche Funktionen Randwerte besitzen, drückt der folgende Satz aus.

\ssect{2.27 Satz}{(Spursatz)}
Sei $\Omega\subset \R^d$ ein Lipschitz-Gebiet und $1\le p<\infty$.
Dann gibt es einen linearen \bet{Spuroperator} $\tau: H^{1,p}(\Omega) \to L^p(\partial\Omega)$, so dass für $u\in H^{m,p}(\Omega)\cap C^\infty(\bar{\Omega})$ gilt:
\[
\tau u = u|_{\partial\Omega}.
\]
Insbesondere gilt für $u\in H_0^{1,p}(\Omega): \tau u = 0$.\\

\bet{Beweis:} s. \cite{Alt06}, Satz A 6.6, S.~279; sch\"oner in \cite{Evans02}, S.~257 ff.

\ssect{2.28 Satz}{(2. Soblev'scher Einbettungssatz)}
Sei $1\le p<\infty$, dann gilt:
\[
H^{1,p}((a,b)) \hookrightarrow C^0([a,b]),
\]
d.h. dass (möglicherweise nach Änderung von Funktionswerten auf einer Nullmenge) Funktionen in $H^{1,p}((a,b))$ stetig sind.

Sei nun $\Omega\subset \R^d$ ein Gebiet und $1\le p<\infty$. 
Dann gilt 
\[
H_0^{2,p}(\Omega) \hookrightarrow C^0(\Omega), \text{ falls } 2-\frac{d}{p} > 0.
\]
Ist $\Omega$ ein Lipschitz-Gebiet, so gilt diese Aussage auch für Sobolevräume ohne Nullrandwerte.\\

\bet{Beweis:} s. \cite{Alt06}, S\"{a}tze 8.4--8.13, S.~334 ff.

\ssect{2.29 Satz}{(Poincaré-Friedrichs Ungleichung)}
Sei $\Omega\subset \R^d$ ein Gebiet mit Durchmesser $D:=\diam(\Omega)$ und $1\le p <\infty$.
Dann gibt es eine Konstante $c_p\le 2D$, so dass für alle $v\in H_0^{1,p}(\Omega)$ gilt:
\[
\norm{v}_{L^p(\Omega)}  \le c_p \norm{\nabla v}_{L^p(\Omega)}.
\]

\bet{Beweis:} s. \cite{Dziuk10}, Satz 2.20, S.~55f.

\subsubsection{Schwache Formulierung elliptischer Randwertprobleme}
Wir betrachten zunächst die stationäre Wärmeleitgleichung.
Sei $\Omega\subset \R^d$ ein Gebiet mit glattem Rand und seien $q\in C^0(\Omega)$ und $\kappa\in C^1(\Omega)$ mit $\kappa\ge \kappa_1>0$, $\kappa_1\in \R$ Konstante.
Gesucht ist eine Funktion $u\in C^2(\Omega)\cap C^0(\bar{\Omega})$, die sogenannte klassische Lösung, so dass
\begin{align}
\begin{split}
-\nabla (\kappa \nabla u) &= q \text{ in } \Omega,\\
u &= 0 \text{ auf } \partial\Omega. 
\end{split}
\end{align}

Mit Hilfe von schwachen Ableitungen und den Sobolevräumen können wir nun den klassischen Lösungsbegriff verallgemeinern:

\ssect{2.30 Defintion}{(schwache Formulierung der stationären Wärmeleitgleichnung)}
Seien $\Omega\subset \R^d$ ein Lipschitz-Gebiet, $q\in L^2(\Omega)$ und $\kappa \in L^\infty(\Omega)$ mit $0< \kappa_1 \le \kappa$ für eine Konstante $\kappa_1\in \R$ gegeben.
Dann heißt $u\in H_0^{1,p}(\Omega)$ schwache Lösung des Randwertproblems der stationären Wärmeleitgleichung (2.2), falls für alle Testfunktionen $v\in H_0^{1}(\Omega)$ gilt 
\[
\int_{\Omega} \kappa(x) \nabla u(x) \nabla v(x) \dint x = \int_{\Omega} q(x) v(x) \dint x.
\]

\ssect{2.31 Satz}{(Existenz und Eindeutigkeit von Lösungen)}
Unter den Voraussetzungen von Def. 2.30 gibt es genau eine schwache Lösung $u\in H_0^{1}(\Omega)$ des Randwertproblems der stationären Wärmeleitgleichung.\\

\bet{Beweis:}\\
Zunächst wird durch $l(v) := \int_{\Omega} q(x) v(x) \dint x$ ein lineares Funktional in $(H_0^{1}(\Omega))'$ definiert, denn
\[
\norm{l(v)}_{(H_0^1(\Omega))'} = \sup\limits_{v\in (H_0^1(\Omega))\backslash \{0\}} \frac{(q,v)_{L^2(\Omega)}}{\norm{v}_{H^1(\Omega)}} \le \norm{q}_{L^2(\Omega)}  \infty.
\]
Ferner wird wegen der Poincaré-Friedrichs Ungleichung durch
\[
(w,v)_{H_0^1(\Omega)} := \int_{\Omega} \nabla w(x) \nabla v(x)\dint x
\]
ein Skalarprodukt auf dem Hilbertraum $H_0^1(\Omega)$ definiert.
Daher existiert nach dem Riesz'schen Darstellungsatz 2.9 ein eindeutiger Riesz-Repräsentant $w_l$ mit $l(.) = (w_l,.)_{H_0^1(\Omega)}$.

Um den Beweis zu schließen, müssen wir noch nachweisen, dass die Bilinearform $b:H_0^1(\Omega)\times H_0^1(\Omega) \to \R$ definiert durch
\[
b(w,v) := \int_{\Omega} \kappa(x) \nabla w(x) \nabla v(x) \dint x
\]
die Voraussetzungen des Satzes von Lax-Milgram erfüllt.
Wir müssen also zeigen, dass die Bilinearform $b$ stetig und koerziv ist.\\
\bet{Stetigkeit:}
\[
b(w,v) \le \norm{\kappa}_{L^\infty(\Omega)} \norm{w}_{H_0^1(\Omega)} \norm{v}_{H_0^1(\Omega)}.
\]
\bet{Koerzivität:}
\[
b(w,v) = \ge \kappa_1 \norm{w}_{H_0^1(\Omega)}^2.
\]
Damit existiert ein bijektiver Operator $B\in L(H_0^1(\Omega))$ mit $b(u,v)= (Bu,v)_{H_0^1(\Omega)}$ und wir definieren die eindeutige Lösung $u\in H_0^^(\Omega)$ des Randwertproblems als $u := B^{-1}w_l$, wobei $w_l$ der eindeutige Riesz-Repräsentant mit $l(.)= (w_l,.)_{H_0^1(\Omega)}$ war.
\hfill $\square$

\ssect{2.32 Bemerkung}{}
Mit der gleichen Beweistechnik lassen sich auch allgemeinere PDgl'en behandeln, wie zum Beispiel das Randwertproblem in Divergenzform
\begin{align*}
-\nabla (A(x)\nabla u) +b(x)\nabla u + c(x) u &= q \text{ in } \Omega,\\
u &= 0 \text{ auf } \partial\Omega.
\end{align*}

$A(x) \in C^1(\Omega,\R^{d\times d}),~b(x)\in C^0(\Omega,\R^d),~c(x)\in C^1(\Omega)$, wobei die Koeffizienten gewisse Anforderungen erfüllen müssen damit die Koerzivität der entsprechenden Bilinearform nachgewiesen werden können.

\ssect{2.33 Bemerkung}{(Reduktion auf Nullrandwerte)}
Zur Betrachtung von allgemeinen Dirichletrandwerten, kann man wie folgt vorgehen.
Seien $g_D\in H^1(\Omega)$ und $q\in L^2(\Omega),~\kappa\in L^\infty(\Omega),~\kappa\ge \kappa_1 >0$. 
Dann ist $u\in H^1(\Omega)$ schwache Lösung von $-\nabla(\kappa(x)\nabla u(x)) = q$ in $\Omega$ und $u=g$ auf $\partial\Omega$.
Wenn gilt $\tilde{u}:= u-g_D \in H_0^1(\Omega)$ und für alle $v\in H_0^1(\Omega)$ gilt (2.3).
Dabei ist zu bemerken, dass mit der Definition von $\tilde{u}$ (2.3) äquivalent ist zu 
\[
\int_{\Omega} \kappa(x)\nabla \tilde{u} \nabla v(x)\dint x = \int_\Omega q(x) v(x)\dint x - \int_\Omega \nabla g_D(x) \nabla v(x) \dint x.
\]
Die Existenz und Eindeutigkeit einer Lösung folgt dann daraus, dass durch $l(v) := \int_\Omega q(x) v(x)\dint x - \int_\Omega \nabla g_D(x) \nabla v(x) \dint x$ ein lin. Funktional in $(H_0^1(\Omega))'$ definiert wird.

\ssect{2.34 Definition}{(schwache Formulierung)}
Seien $X$ reeller Hilbertraum, $b:X\times X\to \R$ eine stetige und koerzive Bilinearform mit Stetigkeitskonstante $\gamma$ und Koerzivitätskonstante $\alpha$ und $l\in X'$.
Dann bezeichnen wir mit $u\in X$ die eindeutige Lösung des Problems
\begin{align}
b(u,v) = l(v) ~\forall v\in X.
\end{align}

\subsubsection{Regularität}
Zur Motivation betrachte in einer Raumdimension die Dgl. $u''(x) = q(x)$ mit einer stetigen Funktion $q(x)$.
Dann folgt mit dem Hauptsatz der Differential- und Integralrechnung, dass bereits $u\in C^2$ gelten muss.

\ssect{2.35 Satz}{($H^2$-Regularität)}
Sei $\Omega$ ein Gebiet mit glattem Rand (es gelte $\partial\Omega$ ist in $C^2$) oder ein konvexes Lipschitz-Gebiet.
Ferner seien $q\in L^2(\Omega)$ und $\kappa\in C^1(\bar{\Omega})$.
Dann gilt für die eindeutige schwache Lösung $u\in H_0^1(\Omega)$ der stationären Wärmeleitungsgleichung (2.2) dass $u\in H^2(\Omega)$ und dass eine Konstante $c>0$ existiert, so dass die folgende Abschätzung gilt:
\[
\norm{u}_{H^2(\Omega)} \le c\norm{q}_{L^2(\Omega)}
\]

\bet{Beweis:} f\"{u}r glatten Rand und das Poissonproblem s. \cite{Dziuk10}, S\"{a}tze 2.36/2.38, S.~69 ff., f\"{u}r glatten Rand und Randwertprobleme in Divergenzform s. \cite{Evans02}, Kapitel 6.3, S.~308 ff.

\ssect{2.36 Bemerkung}{}
Betrachtet man nicht konvexe Lipschitz-Gebiete, so kann man im Allgemeinen keine Lösung $u\in H^2(\Omega)$ erwarten.

\subsection{Ritz-Galerkin Verfahren und abstrakte Fehlerabschätzungen}
In diesem Abschnitt wollen wir uns mit der Approximation der Lösung der schwachen Formulierung von (2.4) befassen.

\ssect{2.37 Definition}{(Ritz-Galerkin Verfahren)}
Seien $X,b$ wie in Definition 2.34 und $X_m\subset X$ mit $\dim (X_m) = m$ ein Unterraum.
Dann ist die \bet{Ritz-Galerkin Approximation} $u_m\in X_m$ definiert durch
\[
b(u_m,v_m)=l(v_m)~\forall v_m\in X_m.
\]

\ssect{2.38 Bemerkung}{}
Die Existenz und Eindeutigkeit von $u_m$ folgt unmittelbar aus dem Satz von Lax-Milgram 2.14, da der Unterraum $X_m$ wieder ein Hilbertraum mit dem aus $X$ geerbten Skalarprodukt ist.

\ssect{2.39 Satz}{(Abstrakte Fehlerabschätzung/Lemma von Céa)}
Seien $X,X_m,b,u$ und $u_m$ wie in den Definitionen 2.34 und 2.37 definiert. 
Dann gilt die abstrakte Fehlerabschätzung
\[
\norm{u-u_m}_X \le \frac{\gamma}{\alpha} \inf\limits_{v_m\in X_m} \norm{u-v_m}_X.
\]

Außerdem gilt die Galerkin-Orthogonalität
\[
b(u-u_m,v_m)=0~\forall v_m\in X_m.
\]

\bet{Beweis:}\\
Wir zeigen zunächst die Galerkin-Orthogonalität:
Dazu sei $v_m\in X_m$ und es folgt mit $X_m\subset X$:
\[
b(u-u_m,v_m) = b(u,v_m)-b(u_m,v_m) = l(v_m)-l(v_m)= 0.
\]
Mit der Stetigkeit und Koerzivität von $b$ folgt weiter
\begin{align*}
\alpha \norm{u-u_m}_X^2 &\le b(u-u_m,u-u_m) = b(u-u_m,u-v_m)\\
&\le \gamma\norm{u-u_m}_X\norm{u-v_m}_X\\
&\Rightarrow \norm{u-u_m}_X \le \frac{\gamma}{\alpha} \norm{u-v_m}_X
\end{align*}
Gehe auf beiden Seiten der Ungleichung zum Infimum über, dann folgt die Behauptung.
\hfill $\square$

\ssect{2.40 Bemerkung}{}
Die abstrakte Fehlerabschätzung zeigt, dass der Fehler zwischen Ritz-Galerkin Approximationen und exakter Lösung abgeschätzt werden kann durch die Bestapproximation in dem Teilraum $X_m$.
Die weitere numerische Analyse beruht somit allein auf der Approximationstheorie.
Insbesondere bestimmt im wesentlichen der Teilraum $X_m$ die Approximationsgüte.

\ssect{2.41 Beispiel}{(mögliche Wahl von Teilräumen)}
Betrachten wir konkret die Stationäre Wärmeleitungsgleichung (2.2) oder allgemeiner ein elliptisches Problem in Divergenzform mit $X=H^1_0(\Omega)$ auf einem Gebiet $\Omega\subset \R^d$, so sind neben den Finiten Elemente Verfahren, die wir im nächsten Abschnitt betrachten wollen,vor allem folgende Wahlen von Teilräumen gebräuchlich:
\begin{itemize}
	\item Polynomräume $X_M := \Pw^{k(m)}(\Omega)\cap \penbrace{v_m\in C^0(\bar{\Omega})~|+v_m=0\text{ auf }\partial\Omega}$, wobei $\Pw^{k(m)}(\Omega)$ der Raum der Polynome mit Grad $\le k(m)$ über $\Omega$ ist.
	Die zugehörigen Verfahren nennt man \bet{Spektralverfahren}.
	\item $X_m := \spann \penbrace{u_i\in X~|~Lu_i=\lambda u_i,~i=1\dt{,}m}$ wobei $u_i$ die $i$-te Eigenfunktion des zugrundeliegenden Differentialoperators $L$ ist.
	\item $X_m := \spann \penbrace{u_i\in X~|~ \Delta u_i =\lambda u_i,~i=1\dt{,}m}$, wobei $u_i$ die $i$-te Eigenfunktion des Laplaceoperators $\Delta$ ist.
\end{itemize}

\ssect{2.42 Folgerung}{(Matrix-Vektor von Ritz-Galerkin Verfahren)}
Seien $X,X_m,b,u$ un d$u_m$ wie in den Definitionen 2.34 und 2.37 definiert und sei zudem $X_m$ endlichdimensional, mit Dimension $m := \dim X_m$.
Ist dann $\Phi := \penbrace{\varphi_1\dt{,}\varphi_m}$ eine Basis von $X_m$ so folgt mit der Darstellung $u_m = \sum_{i=1}^{m}U_i\varphi_i$ aus der Definition von $u_m$
\[
b\enbrace{\sum_{i=1}^{m}U_i\varphi_i,\varphi_j} = l(\varphi_j),~j=1\dt{,}m.
\]
Durch Ausnutzen der Linearität von $b$ im 1. Argument folgt weiter:
\[
\sum_{i=1}^{m} b(\varphi_i,\varphi_j)U_i = l(\varphi_j),~j=1\dt{,}m.
\]
Definieren wir also die Matrix $\bet{S}\in \R^{m\times m}$ durch $\bet{S}_{ji} := b(\varphi_i,\varphi_j),~i,j=1\dt{,}m$ und die Vektoren $\bet{u},\bet{l}\in \R^m$ durch $\bet{u}_i:= U_i,~\bet{l}_i := l(\varphi_i),~i=1\dt{,}m$, so ist $u_m$ genau dann Lösung des Ritz-Galerkin Verfahren, wenn $u$ das folgende lineare Gleichungssystem löst: $\bet{Su}=\bet{l}$.

\subsection{Finite Elemente Verfahren}
Finite Elemente Verfahren sind Spezialfälle von Ritz-Galerkin Verfahren für eine bestimmte Klasse von Teilräumen $X_h\subset X$, wobei $X_h$ der \bet{Finite Elemente Raum} ist.
Die Konstruktion von $x_h$ im Falle von Finite Elemente (FE) Verfahren beruht auf einer Zerlegung des Gebietes $\Omega$ in nicht überlappende Teilgebiete, die selbst wiederum einfache geometrische Objekte sind.
Die einfachste Klasse von Finiten Elementen sind Lagrange Elemente, auf welche wir uns in dieser Vorlesung beschränken werden.
Ferner betrachten wir nur Teilräume $X_h$ welche auf einer simplizialen Zerlegung des Gebietes $\Omega$ beruhen.
In zwei Raumdimensionen besteht das Rechengitter aus Dreiecken, in drei Raumdimensionen aus Tetraedern.
Eingeschränkt auf einen Simplex wird eine Funktion aus $X_h$, dann ein Polynom mit Grad $\le k,~k\in \N$ sein.
Für andere FE siehe z.B. \cite{BreSco08}.

\ssect{2.43 Definition}{(Simplex)}
Seien $s\in \penbrace{1\dt{,}d}$ und $a_0\dt{,}a_s\in \R^d$ Punkte, so dass $(a_j-a_0)_{j=1\dt{,}s}$ linear unabhängig sind.
Dann heißt 
\[
T:= \penbrace{x\in \R^d~|~ x=\sum_{i=0}^{s}\lambda_ia_i,~0\le \lambda_i,~\sum_{i=0}^{s}\lambda_i=1}
\]
nicht-degeneriertes $s$-dimensionaler Simplex im $\R^d$.
Die Punkte $a_0\dt{,}a_s$ heißen Ecken des Simplex.
Ist $r\in \{0\dt{,}s\}$ und $\tilde{a}_0\dt{,}\tilde{a}_r\in \{a_0\dt{,}a_s\}$, so heißt
\[
\tilde{T}:= \penbrace{x\in \R^d~|~ x=\sum_{i=0}^{s}\lambda_i\tilde{a}_i,~0\le \lambda_i,~\sum_{i=0}^{s}\lambda_i=1}
\]
$r$-dimensionales Seitensimplex von $T$.
Die nulldimensionalen Seitensimplexe heißen Ecken, die eindimensionalen Kanten.
Wir bezeichnen mit $T_0$ den Simplex zu den Punkten $a_0=e_0=(0\dt{,}0),a_i = e_i,~i=1\dt{,}d$, $T_0$ heißt $d$-dimensionaler Einheitssimplex.
Der \bet{Durchmesser} von $T$ ist gegeben durch $h(T):= \diam (T) = \max_{i,j=1\dt{,}s}\abs{a_i-a_j}$. 
Mit 
\[
\rho(T):= 2\sup\{R~|~B_R(x_0)\subset T\}
\]
bezeichnen wir den \bet{Inkugeldurchmesser} von $T$ und mit 
\[
\delta(T):= \frac{h(T)}{\rho(T)}
\]
den Quotienten aus $h$ und $\rho$.

\ssect{2.44 Definition}{(Baryzentrische Koordinaten)}
Die baryzentrischen Koordinaten $\lambda_0\dt{,}\lambda_s\in [0,1]$ eines Punktes $x\in T$ des $s$-dim. Simplex $T$ sind die Lösung des linearen Gleichungssystems
\[
x= \sum_{i=0}^{s}\lambda_i a_i,~ \sum_{i=0}^{s} \lambda_i =1.
\]
Der Schwerpunkt $x_s$ von $T$ ist definiert durch $x_s := \frac{1}{s+1}\sum_{i=0}^{s}a_i$ und hat die baryzentrischen Koordinaten $\lambda_i :=\frac{1}{s+1}$.
Für die Eckpunkte $a_k$ von $T$ sind die baryzentrischen Koordinaten gegeben durch $\lambda_k=1,~\lambda_i =0,~i\neq k$.
Die baryzentrischen Koordinaten sind eindeutig bestimmt.

\ssect{2.45 Lemma}{(Referenzabbildung)} 
Jedes $s$-dimensionale Simplex $T$ im $\R^s$ ist affin äquivalent zum Einheitssimplex $T_0$ der gleichen Dimension.
Die eindeutige affine Abbildung $F:T_0\to T, ~ F(x) =Ax+b,~A\in \R^{s\times s},~b\in \R^s, ~\det A \neq 0$ mit $F(e_j)= a_j,~j=0\dt{,}
s$ heißt \bet{Referenzabbildung}.
$F$ ist invertierbar und es gelten die Abschätzungen
\[
\norm{\nabla F} = \norm{A}\le \frac{h(T)}{\rho(T_0)},~\norm{\nabla(F^{-1})}= \norm{A^{-1}}\le \frac{h(T_0)}{\rho(T)}
\]
sowie
\[
c\cdot \rho(T)^s \le \abs{\det (\nabla F)}= \abs{\det A} =\frac{\abs{T}}{\abs{T_0}}\le C\cdot h(T)^s,~c,C>0.
\]

\bet{Beweis:} s. \cite{Dziuk10}, Hilfssatz 3.13, S.~91 ff.

\ssect{2.46 Definition}{(Zulässige Triangulierung)}
Sei $\Omega\subset\R^d$ ein Gebiet und 
\[
\mathbb{T}_h := \penbrace{T_j~|~j=1\dt{,}m,~T_j\text{ ist $d$-dim. Simplex im }\R^d}.
\]
$\mathbb{T}_h$ heißt zulässige Triangulierung der Feinheit $h$ und Gute $\rho$ von $\Omega$, falls gilt:
\[
\bar{\Omega} = \bigcup\limits_{j=1}^{m} T_j,~\partial\Omega = \bigcup\limits_{j=1}^{m} \tilde{T}_j,
\]
wobei $\tilde{T}_j$ Flächen der Simplexe $T_j$ sind.
Für je zwei $T_1,T_2\in \mathbb{T}_h$ mit $S:=T_1\cap T_2$ gilt $S=\emptyset$ oder $S$ ist $(d-k)$-dim. Seitensimplex von $T_1$ und $T_2$ für ein $k\in \{1\dt{,}d\}$.
Mit $h:=\max_{j=1\dt{,}m}h(T_j)$ und $\rho := \min_{j=1\dt{,}m}\rho(T_j)$.\\

Zur Definition von Finite Elemente Räumen basierend auf einer Triangulierung $\mathbb{T}_h$ müssen wir nun lediglich lokale Funktionenräume auf dem Simplexen $T\in\mathbb{T}_h$ angeben und festlegen wie solche lokalen Funktionen global zusammengesetzt werden.
Ein Tripel bestehend aus einem geometrischen Objekt $T$, einer lokalen Basis $\Phi$ und lokalen Freiheitsgraden $\delta$, wollen wir um folgenden \bet{Element} nennen.

\ssect{2.47 Definition}{(lineares simpliziales Lagrange Element)}
Sei $T\subset\R^d$ ein $d$-dim. Simplex.
Sei $\delta := \penbrace{a_k~|~k=0\dt{,}d}$ die Menge der Ecken von $T$.
Dann ist durch Angabe von Werten in den Punkten $a_k\in \delta$ eindeutig eine lineare Funktion $p\in \Pw^1(T)$ definiert.
Durch $\Phi := \penbrace{\varphi_i~|~\varphi_i(a_k)=\delta_{ik}~i,k=1\dt{,}d}$ ist eine modale Basis von $\Pw^1(T)$ gegeben.
Wir nennen das Tripel $(T,\Phi,\delta)$ \bet{lineres simpliziales Lagrange Element}.
Die Basisfunktionen $\varphi_i\in\Phi$ werden \bet{Formfunktionen} oder im Englischen \bet{Shapefunctions} genannt und $\delta$ ist die Menge der \bet{modalen Variablen}.
Zur Wohldefiniertheit s. \cite{Dziuk10}, Satz 3.16, S.~95f.

\ssect{2.48 Beispiel}{(lineares Lagrange Element für $d=2$)}
Wir betrachten das Einheitsdreieck $T_0$ mit Eckpunkten $a_0^0=(0,0),a_1^0=(1,0),a_2^0=(0,1)$.
Die Formfunktionen sind dann gegeben durch
\[
\varphi_0^0(x,y)= 1-x-y,~\varphi_1^0(x,y) = x,~\varphi_2^0(x,y) = y.
\]
Sind $p(a_0^0),p(a_1^0),p(a_2^0)$ Funktionswerte einer linearen Funktion $p\in\Pw^1(T_0)$, so ist $p$ gegeben durch
\[
p(x,y) = \sum_{i=0}^{2} p(a_i^0)\varphi_i^0(x,y).
\]
Für ein beliebiges Dreieck $T\subset\R^2$ erhält man das Lagrange Element mit Hilfe der Referenzabbildung $F:T_0\to T$ aus Lemma 2.45.

\ssect{2.49 Bemerkung}{}
Das Beispiel 2.48 zeigt, dass es ausreicht ein Finites Element auf einer Referenzgeometrie zu definieren.
Durch die Referenzabbildung erhält man dann die entsprechende Klasse von Elementen auf beliebigen Geometrien im Raum.\\

Ein Finites Element legt lediglich ein lokalen Funktionenraum auf einen Simplex $T$ fest, um zu einem Unterraum von $H^1_0(\Omega)$ zu gelangen, müssen wir zusätzlich festlegen auf welche Weise die lokalen Funktionen global zusammengesetzt werden.

\ssect{2.50 Definition}{(linearer Finite Elemente Raum $S_h^1$)}
\begin{enumerate}[(1)]
\item Sei $\Omega\subset\R^d$ und $\mathbb{T}_h$ eine zulässige Triangulierung von $\Omega$.
Wir definieren den Raum der \bet{linearen Finite Elemente} auf simplizialen Gittern $S_h^1$ durch
\[
S_h^1 := \penbrace{v_h\in C^0(\Omega)~|~v_h|_T\in \Pw^1(\Omega),~T\in\mathbb{T}_h}.
\]
\item Sind $\bar{a}_j,~j=1\dt{,}N_h$ die Ecken der Triangulierung $\mathbb{T}_h$, so ist eine Funktion $v_h\in S_h^1$ durch die Vorgabe von Funktionswerten in den Ecken $v_h(\bar{a}_j)$ eindeutig definiert.
Insbesondere gilt $\dim(S_h^1)= \mathcal{N}_h$.
Eine Basis von $S_h^1$ ist durch die Funktionen
\[
\bar{\varphi}_i\in S_h^1,~\bar{\varphi}_i(\bar{a}_j)=\delta_{ij},~i,j=1\dt{,}\mathcal{N}
\]
gegeben.
Diese Basis heißt \bet{Knotenbasis} oder \bet{modale Basis}.
\item Ist $(T_0,\Phi,\delta)$ das lineare Lagrange Element auf dem Einheitssimplex $T_0$ und $v_h\in S_h^1$ gegeben durch
\[
v_h(x):=\sum_{i=1}^{\mathcal{N}_h}v_n(\bar{a}_i)\bar{\varphi}_i(x),
\]
so gilt für beliebige Simplexe $T\in \mathbb{T}_h$ mit Ecken $a_0\dt{,}a_d$
\[
v_h|_T(x) = \sum_{i=1}^{d}v_h(a_i)\varphi_i^0(T^{-1}(x)),
\]
wobei $F:T_0\to T$ die Referenzabbildung und $\varphi_i^0\in \Phi$ die Formfunktionen von $T_0$ sind.
\end{enumerate}

\ssect{2.51 Definition}{(lineares Finite Elemente Verfahren)}
Sei $\Omega\subset\R^d$ und $\mathbb{T}_h$ zulässige Triangulierung von $\Omega$.
Seien $X:=H_0^1(\Omega)$ und $X_h:= S_{h,0}^1:=S_h^1\cap \penbrace{v\in C^1(\Omega)~|~v=0 \text{ auf }\partial\Omega}$.
Weiter seien eine stetige und koerzive Bilinearform $b:X\times X\to \R$ und ein $l\in X'$ gegeben.
Dann ist $X_h\subset X$ ein Teilraum und $u_h\in X_h$ heißt Lösung des \bet{linearen Finite Elemente Verfahrens} für das Problem aus 2.34, falls gilt:
\[
b(u_h,v_h)= l(v_h) ~\forall v_h\in X_h.
\]

\ssect{2.52 Satz}{(A priori Fehlerabschätzung)}
Sei $\Omega\subset\R^d,~d\le 3$ ein Lipschitz-Gebiet und $\mathbb{T}_h$ eine zulässige Triangulierung von $\Omega$ mit $\sigma(T)\le \sigma<\infty,~\sigma\in \R,~\forall T\in \mathbb{T}_h$.
Seien $X,X_h,b,u_h$ wie in Definition 2.51 und $u\in X$ wie in Definition 2.34.
Liegt nun $u\in H_2(\Omega)$ so gibt es eine Konstante $c>0,c\in \R$, die nur von $d,\sigma,\Omega$ abhängt, so dass gilt:
\begin{align}
\norm{u-u_h}_{H^1(\Omega)}\le ch\abs{u}_{H^2(\Omega)},
\end{align}
wobei $\abs{u}_{H^2(\Omega)}:= \norm{D^2u}_{L^2(\Omega)}$.\\

\bet{Beweis:}\\
Für das Poissonproblem s. \cite{Dziuk10}, Satz 3.34, S.~116f.

\ssect{2.53 Bemerkung}{(a priori $\leftrightarrow$ a posteriori Fehlerabschätzung)}
Satz 2.52 macht eine Aussage über die Konvergenz des linearen FE Verfahrens.
Da auf der rechten Seite der Ungleichung (2.4) aber der Term $\abs{u}_{H^2(\Omega)}$ auftaucht, ist (2.4) nicht geeignet um den tatsächlichen Wert des Approximationsfehlers abzuschätzen.
Zu diesem Zweck leitet man A posteriori Fehlerabschätzungen her, bei denen der Fehler ausschließlich durch berechenbare Größen abgeschätzt wird.
Für eine Übersicht über A posteriori Fehlerabschätzungen für FE Verfahren verweisen wir auf \cite{Ver13}.

\ssect{2.54 Bemerkung}{}
Betrachten wir Definition 2.51 des linearen FE Verfahrens, so stellen wir zunächst fest, dass wir in der schwachen Formulierung exakte Integrale bestimmen müssen, was im Allgemeinen nicht realisierbar ist.
in der Praxis verwendet man Quadraturformeln.
Ferner schränkt uns Definition 2.51 auf polygonal berandete Gebiete ein.
Um auch Gebiete mit glattem Rand behandeln zu können , kann man z.B. eine Gebietsapproximation durchführen bei dem alle Ecken auf dem Rand des polygonal berandeten approximierenden Gebiets $\partial\Omega_h$ auch auf $\partial\Omega$ liegen.
Hier ist dann $X_h$ nicht Teilraum von $X$ und das Lemma von Céa nicht anwendbar.
Allerdings kann man in beiden Fällen unter gewissen Voraussetzungen zeigen, dass die zusätzlichen Approximationsfehler die Konvergenzordnung des FE Verfahrens nicht beeinflussen.
Für weitere Details siehe z.B. \cite{Dziuk10,BreSco08}.
\newpage
\section{Reduzierte Basis Methoden für lineare, koerzive Probleme}

\subsection{Parameterabhängigkeit}

\ssect{3.1 Definition}{(parametrische Formen)}
Sei $\mathcal{P}\subset \R^d$ eine beschränkte Parametermenge.
Dann nennen wir 
\begin{enumerate}[(1)]
	\item $f:X\times \mathcal{P}\to \R$ eine parametrische stetige Linearform oder ein parametrisches stetiges lineares Funktional, falls $\forall \mu\in\mathcal{P}: f(.,\mu)\in X$.
	\item Wir nennen $b:X_1\times X_2\times \mathcal{P}\to \R$ eine parametrische stetige koerzive Bilinearform, falls $\forall \mu\in \mathcal{P}: b(.,.,\mu):X_1\times X_2\to \R$ bilinear stetig und koerziv ist. Wir bezeichnen die Stetigkeitskonstante mit $\gamma(\mu)$ und die Koerzivitätskonstante mit $\alpha(\mu)$.
\end{enumerate}

\ssect{3.2 Bemerkung}{}
Eine parametrische stetige Bi-/Linearform ist nicht unbedingt stetig bzgl. $\mu$.
Betrachte dazu das Beispiel $X=\R,\mathcal{P}=[0,1],f:X\times \mathcal{P}\to\R$ mit
\[
f(x,\mu):=\left\{\begin{array}{l}x,\text{ falls }\mu<\frac{1}{2}\\ \frac{1}{2}x,\text{ sonst.}  \end{array}\right.
\]

\ssect{3.3 Defintion}{(Parametrische Beschränktheit, Stetigkeit)}
\begin{enumerate}[(1)]
	\item Wir nennen eine parametrische stetige Linearform $f$ beschränkt bzw. Bilinearform $b$ \bet{gleichmäßig beschränkt} bzgl. $\mu$, falls $\gamma_0,\gamma_1\in\R^+$ existieren so dass
	\[
	\sup\limits_{\mu\in\mathcal{P}}\norm{f(.,\mu)}_X\le \gamma_0 \text{ bzw. } \sup\limits_{\mu\in\mathcal{P}}\gamma(\mu)\le \gamma_1.
	\]
	\item Wir nennen $b$ \bet{glm. koerziv} bzgl. $\mu$, falls ein $\alpha_0>0$ existiert so dass
	\[
	\inf\limits_{\mu \in \mathcal{P}} \alpha(\mu)\ge \alpha_0>0.
	\]
	\item Wir nennen $f$ bzw. $b$ Lipschitz-stetig bzgl. $\mu$, falls ein $L_f\in \R^+$ bzw. ein $L_b\in \R^+$ existiert, so dass für alle $\mu_1,\mu_2\in\mathcal{P}$ gilt
	\[
	\abs{f(u,\mu_1)-f(u,\mu_2)}\le L_f\norm{u}_X\norm{\mu_1-\mu_2} ~\forall  u\in X,
	\]
	bzw.
	\[
	\abs{b(u,v,\mu_1)-b(u,v,\mu_2)}\le L_b\norm{u}_{X_1}\norm{v}_{X_2}\norm{\mu_1-\mu_2}~\forall u\in X_1,v\in X_2.
	\]
\end{enumerate}

\ssect{3.4 Lemma}{(Energienorm)}
Sei $X$ HR, $b:X\times X\times \mathcal{P}\to \R$ parametrische , koerzive, stetige Bilinearform.
Dann ist für $\mu\in\mathcal{P}$ durch
\[
(((u,v)))_\mu := b_s(u,v;\mu)
\]
ein Skalarprodukt auf $X$ und durch
\[
|||u|||_\mu := \sqrt{(((u,u)))_\mu}
\]
die \bet{Energienorm} definiert.
Diese ist äquivalent zur $X$-Norm und es gilt
\[
\sqrt{\alpha(\mu)}\norm{u}_X \le |||u|||_\mu \le \sqrt{\gamma(\mu)}\norm{u}_X ~\forall u\in X.
\]

\bet{Beweis:}\\
Skalarprodukt klar wegen Bilinearität, Stetigkeit und Koerzivität.
Normäquivalenz folgt aus Stetigkeit und Koerzivtät von $b_s$:
\[
\alpha(\mu)\norm{v}_X^2\le b_s(v,v;\mu) \le \gamma(\mu) \norm{v}_X^2.
\]

\ssect{3.5 Definition}{(Parametrische schwache Formulierung; Parametrisches Variationsproblem ($P(\mu)$))}
Sei $X$ HR, $\mathcal{P}\subset \R^p$ beschränkt, $b: x\times X\times \mathcal{P}\to \R$ parametrische, stetige, koerzive Bilinearform, $f,l:X\times \mathcal{P}\to \R$ parametrische stetige Linearform.
Zu $\mu \in\mathcal{P}$ bezeichnet $u(\mu)\in X$ die eindeutige Lösung des parametrischen Variationsproblems
\begin{align}
b(u(\mu),v;\mu) = f(v,\mu) ~\forall v\in X,
\end{align}
mit Ausgabe $s(\mu)=l(u(\mu),\mu)$.

\ssect{3.6 Bemerkung}{}
Existenz und Eindeutigkeit der Lösung $u(\mu)$ folgen mit dem Satz von Lax Milgram.

\ssect{3.7 Definition}{(schwache Formulierung der parametrischen, stationären Wärmeleitungsgleichung)}
Seien $\Omega\subset\R^d$ Lipschitz-Gebiet, $\mathcal{P}\subset \R^p$ beschränkt, $q(\mu)\in L^2(\Omega)$ und $\kappa(\mu)\in L^\infty(\Omega)$ mit $0<\kappa_1\le \kappa(\mu)$ für alle $\mu\in\mathcal{P}$ und Konstante $\kappa_1\in \R^+$.
Dann heißt $u(\mu)\in H_0^{1}(\Omega)$ schwache Formulierung des RWP der parametrischen, stationären WLG aus 1.1, falls gilt
\[
\int_{\Omega} \kappa(x;\mu)\nabla u(x;\mu) \nabla v(x)\dint x = \int_{\Omega} q(x,\mu)v(x)\dint x~\forall v\in H_0^1(\Omega).
\]

\ssect{3.8 Folgerung}{(Existenz und Eindeutigkeit von Lösungen)}
Unter den Voraussetzungen von Definition 3.7 gibt es für jedes $\mu\in\mathcal{P}$ genau eine schwache Lösung $u(\mu)\in H_0^1(\Omega)$ des RWP der parametrischen, stationären WLG aus 1.1.\\

\bet{Beweis:}\\
Analog zum Beweis von Satz 2.31.

\ssect{3.9 Definition}{((lineares) FE Verfahren für parametrsiche Variationsprobleme ($P_h(\mu)$))}
Sei $X$ HR, $\mathcal{P}\in \R^p$ beschränkt, $b:X\times X\times \mathcal{P}\to \R$ parametrische, stetige, koerzive Bilinearform, $f,l:X\times \mathcal{P}\to \R$ parametrische, stetige Linearform.
Sei ferner $\mathbb{T}_h$ eine zulässige Triangulierung des Rechengebietes $\Omega\subset \R^d$ und $X_h\subset X$ eine zugehöriger (linearer) Finite Elemente Raum, wobei $X_h$ Unterraum von $X$.
Zu $\mu\in\mathcal{P}$ heißt $u_h(\mu)\in X_h$ Lösung des (linearen) FE Verfahrens für das parametrische Variationsproblem, falls gilt
\[
b(u_h(\mu),v_h;\mu) = f(v_h;\mu) ~\forall v_h\in X_h,~ s_h(\mu)= l(u_h(\mu);\mu).
\]

\ssect{3.10 Bemerkung}{}
Das Verfahren aus Definition 3.9 ist nach Bemerkung 1.6 ein 'hochdimensionales, diskretes' Modell.

\subsection{Reduzierte Basisverfahren}

\ssect{3.11 Definition}{(Reduzierte Basis, Reduzierte Basis Räume)}
Sei $S_N := \penbrace{\mu^1\dt{,}\mu^N}\subset \mathcal{P}$ eine Menge von Parametern mit (oBdA) linear unabhängigen Lösungen $\penbrace{u(\mu^i)}_{i=1}^N$ von $(P_h(\mu))$.
Dann ist $X_N := \spann\penbrace{u(\mu^i)}_{i=1}^N$ ein $N$-dimensionaler \bet{Lagrange Reduzierte Basis-Raum}.
Eine Basis $\Phi_N :=\penbrace{\phi_1\dt{,}\phi_N}\subset X_h$ eines Reduzierte Basis-Raumes ist eine Reduzierte Basis (RB).

\ssect{3.12 Bemerkung}{}
Es existieren weitere Arten von RB-Räumen.
Im weiteren Verlauf der Vorlesung werden wir z.B. noch \bet{POD-Räume} kennenlernen.
Auch die POD-Räume werden aus sogenannten Snapshots, d.h. Lösungen $u_h(\mu^i),~1\le i\le k$ mit $k\gg N$, erzeugt.

\ssect{3.13 Definition}{(RB-Modell $(P_N(\mu))$, symmetrischer Fall)}
Sei ein Problem $P(\mu)$ und ein diskretes Modell $(P_h(\mu))$ gegeben und zusätzlich gelte $b$ symmetrisch und $f=l$ ("compliant").
Sei $X_h\subset X$ ein RB-Raum.
Zu $\mu \in \mathcal{P}$ ist die RB-Lösung $u_N(\mu)\in X_N$ und die RB-Ausgabe $s_N(\mu)\in \R$ gesucht, so dass
\[
b(u_N(\mu),v;\mu) = f(v;\mu) ~\forall v\in X_N
\]
und 
\[
s_N(\mu) = l(u_N(\mu);\mu).
\]

\ssect{3.14 Bemerkung}{}
Falls $b$ nicht symmetrisch oder $f\neq l$ ist obiges immer noch sinnvoll, aber es bestehen bessere Möglichkeiten $s_N(\mu)$ mittels eines dualen Problems zu bestimmen.

\ssect{3.15 Bemerkung}{}
Da $X_N\subset X_h\subset X$, $X_N$ Teilraum von $X_h$, ist das RB-Modell ein Ritz-Galerkin Verfahren.

\ssect{3.16 Folgerung}{(Existenz, Eindeutigkeit, Stabilität, Wohlgestelltheit)}
Zu $\mu\in\mathcal{P}$ existiert eine eindeutige RB-Lösung $u_N(\mu)\in X_N$ und RB-Ausgabe $S_N(\mu)$ von $(P_N(\mu))$.
Diese sind beschränkt durch $\norm{u_N(\mu)}_X \le \frac{1}{\alpha(\mu)} \norm{f(.;\mu)}_{X}$ und $\abs{s_N(\mu)}\le \frac{1}{\alpha(\mu)}\norm{f(.;\mu)}_{X'}\norm{l(.;\mu)}_{X'}.$\\

\bet{Beweis:}\\
Existenz und Eindeutigkeit von $u_N(\mu)$ folgt mit dem Satz von Lax-Milgram, wobei
\[
\alpha_N(\mu) := \inf\limits_{u\in X_N} \frac{b(u,u:\mu)}{\norm{u}_X^2} \ge \inf\limits_{u\in X} \frac{b(u,u:\mu)}{\norm{u}_X^2} = \alpha(\mu) > 0.
\]
Dann ist auch $s_N(\mu) = l(u_N(\mu);\mu)$ eindeutig und die Stabilität folgt mit
\[
\norm{u_N(\mu)}_X = \norm{B^{-1}(\mu) v_f(\mu)}_X \le \norm{B^{-1}(\mu)}_X \norm{v_f(\mu)}_X \le \frac{1}{\alpha(\mu)}\norm{f(.;\mu)}_X.
\]
Hierbei ist $B(\mu)$ der eindeutige invertierbare Operator aus dem Satz von Lax-Milgram und $v_f(\mu)$ der Riesz-Repräsentant von $f(.;\mu)\in X_N'$.
\[
\abs{s_N(\mu)} = \abs{l(u_N(\mu);\mu)} \le \norm{l(.;\mu)}_{X'}\norm{u_N(\mu)}_X \le \frac{1}{\alpha(\mu)} \norm{f(.;\mu)}_{X'}\norm{l(.;\mu)}_{X'}
\]
\hfill $\square$

\ssect{3.17 Folgerung}{(Galerkin-Projektion, Galerkin-Orthogonalität)}
Zu $\mu\in\mathcal{P},X_h,X_N$ HR mit Energieskalarprodukt $(((.,.)))_\mu,P_\mu:X_h\to X_N$ die orthogonale Projektion aus Satz 2.7, $u_h(\mu),u_N(\mu)$ Lösungen von $(P_h(\mu))$ bzw.$(P_N(\mu))$ und der Fehler $e_N(\mu) = u_h(\mu)-u_N(\mu)$.
Dann gilt
\begin{enumerate}[(1)]
	\item $u_N(\mu) = P_\mu(u_h(\mu))$ "Galerkin-Projektion"
	\item $(((e_N(\mu),v_N)))_\mu = 0 ~\forall v_N\in X_N$ "Galerkin-Orthogonalität"
\end{enumerate}

\bet{Beweis:}\\
Lemma 3.4 impliziert $(X_h, (((.,.)))_\mu)$ HR und $X_N = \spann\{\Phi_i \}_{i=1}^N$ endlichdimensional, also abgeschlossen ist. 
Daher ist $P_\mu$ nach Satz 2.7 wohldefiniert.\\
Mit Folgerung 2.8 folgt
\begin{align*}
&(((P_\mu(u_h(\mu)-u_h(\mu),\Phi_i)))_\mu = 0,~i=1\dt{,}N\\
\Leftrightarrow &b(P_\mu(u_h(\mu))-u_h(\mu),\Phi_i; \mu) = 0,~i=1\dt{,}N\\
\Leftrightarrow &b(P_\mu(u_h(\mu)),\Phi_i; \mu) = b(u_h(\mu),\Phi_i;\mu),~i=1\dt{,}N\\
\Leftrightarrow &b(P_\mu(u_h(\mu)),\Phi_i; \mu) = f(\Phi_i;\mu),~i=1\dt{,}N\\
\end{align*}
Da $u_N(\mu)$ eindeutig folgt $P_\mu(u_n(\mu)) = u_N(\mu)$ daraus folgt (1). (2) folgt entweder aus 2.8 oder Satz 2.39.

\ssect{3.18 Folgerung}{}
Sei $\mu\in\mathcal{P},u_h(\mu),u_N(\mu)$ Lösungen von $(P_h(\mu))$ bzw. $(P_N(\mu))$.
Falls $u_h(\mu)\in X_N \Rightarrow u_N(\mu) = u_h(\mu)$.\\

\bet{Beweis:}\\
Da $u_h(\mu),u_N(\mu)\in X_N \Rightarrow e_N(\mu) := u_h(\mu) - u_N(\mu)\in X_h$ und $(((e_N(\mu),v_N)))_\mu = 0 ~\forall v_N\in X_N$ nach Folgerung 3.17 (2).
Damit gilt $(((e_N(\mu),l_N(\mu))))_\mu = 0 \Rightarrow e_N(\mu) = 0 \Rightarrow u_N(\mu) = u_h(\mu)$.
\hfill $\square$

\ssect{3.19 Satz}{(abstrakte Fehlerabschätzung; Relation zur Bestapproximation)}
Sei $\mu \in \mathcal{P}$ und $ u_h(\mu),s_h(\mu)$ bzw. $u_N(\mu),s_N(\mu)$ Lösungen von $(P_h(\mu))$ bzw. $(P_N(\mu))$.
Dann gilt:
\begin{enumerate}[(1)]
	\item Der Fehler der ($\mu$-abhängigen Energienorm) erfüllt
	\[
	|||u_h(\mu)-u_N(\mu)|||_\mu = \inf\limits_{v\in X_N} |||u_h(\mu)-v|||_\mu.
	\]
	\item Der Fehler in der ($\mu$-unabhängigen) $X$-Norm erfüllt
	\[
	\norm{u_h(\mu)-u_N(\mu)}_X \le \sqrt{\frac{\gamma(\mu)}{\alpha(\mu)}} \inf\limits_{v\in X_N} \norm{u(\mu)-v}_X.
	\]
	mit $\gamma(\mu),\alpha(\mu)$ Stetigkeits- bzw. Koerzivitätskonstante.
	\item Für den Ausgabefehler gilt (wegen $f=l$)
	\[
	0\le s_h(\mu)-s_N(\mu) = |||u_h(\mu)-u_N(\mu)|||_\mu^2 = \inf\limits_{v\in X_N} |||u_h(\mu)-v|||\mu^2 \le \gamma(\mu) \inf\limits_{v\in X_N} \norm{u_h(\mu)-v}_X^2.
	\]
\end{enumerate}

\bet{Beweis:}\\
\begin{enumerate}[(1)]
	\item Nach Folgerung 3.17 ist $u_N(\mu)$ orthogonale Projektion, also Bestapproximation
	\[
	|||u_h(\mu)-u_N(\mu)|||_\mu \bgl{3.17(1)} |||u_h(\mu)-P_\mu(u_N(\mu))|||_\mu \bgl{2.7} = \inf\limits_{v\in X_h} |||u_h(\mu)-v|||_\mu.
	\]
	\item Mit der Normäquivalenz 3.4 folgt 
	\[
	\sqrt{\alpha_h(\mu)}\norm{u_h(\mu)-u_N(\mu)}_X \stackrel{3.4}{\le} |||u_h(\mu)-u_N(\mu)|||_\mu \bgl{(1)} \inf\limits_{v\in X_N} |||u_h(\mu)-v|||_\mu \stackrel{3.4}{\le} \sqrt{\gamma_h(\mu)} \inf\limits_{v\in X_h} \norm{u_h(\mu)-v}_X,
	\]
	wobei $\alpha_h(\mu) := \inf_{v\in X_h} \frac{b(v,v;\mu)}{\norm{v}_X^2}$ und $\gamma_h(\mu):= \sup_{u,v\in X_h} \frac{b(u,v;\mu)}{\norm{u}_X\norm{v}_X}$.
	Wie in Beweis von Folgerung 3.16 folgt $\alpha_h(\mu)\ge \alpha(\mu)$ und $\gamma_h(\mu)\le \gamma(\mu)~\forall \mu\in \mathcal{P}$ und damit die Behauptung.
	\item
	\begin{align*}
	s_h(\mu)-s_N(\mu) &\bgl{Def} l(u_h(\mu);\mu)-l(u_N(\mu);\mu) \bgl{l=f} f(u_h(\mu))-f(u_N(\mu))\\
	&\bgl{(P_h(\mu)} b(u_h(\mu),u_h(\mu)-u_N(\mu);\mu)\\
	&= b(u_h(\mu),u_h(\mu)-u_N(\mu);\mu)- b(u_h(\mu)-u_N(\mu),u_N(\mu);\mu)\\
	&= b(u_h(\mu)-u_N(\mu),u_h(\mu)-u_N(\mu);\mu)
	\end{align*}
	Damit folgt
	\[
	s_h(\mu)-s_N(\mu) = |||u_h(\mu)-u_N(\mu)|||_\mu^2 \bgl{(1)} \inf\limits_{v\in X_h} |||u_h(\mu)-v|||_\mu^2 \stackrel{3.4}{\le} \gamma(\mu)\inf\limits_{v\in X_N} \norm{u_h(\mu)-v}_X^2
	\]
	Insbesondere gilt auch $s_h(\mu)-s_N(\mu) = |||u_h(\mu)-u_N(\mu)|||_\mu^2 \ge 0$.
\end{enumerate}
\hfill $\square$

\ssect{3.20 Bemerkung}{}
\begin{enumerate}[(1)]
	\item $s_N(\mu)$ ist also untere Schranke für $s_h(\mu)$.
	\item Der Ausgabefehler ist im Allgemeinen sehr klein, da das Quadrat des RB-Fehlers eingeht.
	\item Mit dem Lemma von Céa (Satz 2.39) erhalten wir für nicht notwendigerweise symmetrische Bilinearformen $\norm{u_h(\mu)-u_N(\mu)}_X \le \frac{\gamma(\mu)}{\alpha(\mu)} \inf_{v\in X_N} \norm{u_h(\mu)-v}_X$.
	Damit ist Satz 3.19 eine Verschärfung für symmetrische Bilinearformen.
\end{enumerate}

\ssect{3.21 Korollar}{(Monotoner Fehlerabfall in der Energienorm)}
Sei $(X_N)_{N=1}^{N_{\max}}$ Folge von RB-Räumen mit $X_N\subseteq X_{N'}$ für $N\le N'\le N_{\max}$ ("Hierarchische Räume") und $e_N(\mu)= u_h(\mu)-u_N(\mu)$ für $\mu\in \mathcal{P}$.
Dann ist die Folge $(|||e_N(\mu)|||)_{N=1}^{N_{\max}}$ monoton fallend.\\

\bet{Beweis:}\\
\[
|||e_N(\mu)|||\mu = \inf\limits_{v\in X_N} |||u_h(\mu)-v|||_\mu \ge  \inf\limits_{v\in X_{N'}} |||u_h(\mu)-v|||_\mu = |||e_{N'}(\mu)|||_\mu.
\]
\hfill $\square$

\ssect{3.22 Bemerkung}{}
\begin{enumerate}[(1)]
	\item 'Worst Case'~ist eine Stagnation des Fehlers (unrealistisch, da jeder neue Basisvektor orthogonal zu $e_N(\mu)$ sein müsste).
	In der Praxis ist bei geschickter Basiswahl exponentielle Konvergenz zu beobachten.
	\item Monotonie gilt nicht notwendigerweise für andere Normen trotz Normenäquivalenz:
	\[
	c|||e_N(\mu)|||_\mu \le \norm{e_N(\mu)} \le C |||e_N(\mu)|||_\mu
	\]
	mit $c,C$ Konstanten unabhängig von $N$.
	Fehlernorm $\norm{e_N(\mu)}$ kann gelegentlich anwachsen, bleibt aber in einem 'Korridor'~um $|||e_N(\mu)|||_\mu$.\\
	"Beweis":
	\[
	\norm{e_{N'}(\mu)} \le C|||e_{N'}(\mu)|||_\mu \le C|||e_N(\mu)|||_\mu \le \frac{C}{c} \norm{e_N(\mu)}
	\]
\end{enumerate}

\ssect{3.23 Folgerung}{(Fehlerabschätzung für den Fehler zwischen exakter Lösung und RB-Lösung)}
Sei $\mu\in\mathcal{P}$ und $u(\mu),s(\mu),u_h(\mu),s_h(\mu)$ bzw. $u_N(\mu),s_N(\mu)$ Lösung von $(P(\mu)),(P_h(\mu))$ bzw. $(P_N(\mu))$, wobei $X_h$ linearer Finite Elemente Raum.
Zusätzlich gelte $b$ symmetrisch und $f=l$ ("compliant").
Liegt nun $u\in H^2(\Omega)$, so gibt es eine Konstante $c>0$ die nur von $d,\sigma$ und $\Omega$ abhängt, so dass gilt:
\begin{enumerate}
	\item Der Fehler in der ($\mu$-unabhängigen) $X$-Norm erfüllt
	\[
	\norm{u(\mu)-u_h(\mu)}_X \le \sqrt{\frac{\gamma(\mu)}{\alpha(\mu)}} \enbrace{ch\abs{u(\mu)}_{H^2(\Omega)} + \inf\limits_{v\in X_N} \norm{u_h(\mu)-v}_X}
	\]
	\item Für den Ausgabefehler gilt:
	\[
	0 \le s(\mu)-s_N(\mu) \le \gamma(\mu) \enbrace{c^2h^2\abs{u(\mu)}_{H^2(\Omega)} + \inf\limits_{v\in X_N} \norm{u_h(\mu)-v}_X^2}
	\]
	Beweis: Analog zum Beweis von Satz 3.19 unter Verwendung von Satz 2.52.
\end{enumerate}

\ssect{3.24 Bemerkung}{}
Den Fehleranteil $\norm{u(\mu)-u_h(\mu)}_X$ nennt man \bet{Diskretisierungsfehler} und den Anteil $\norm{u_h(\mu)-u_N(\mu)}$ nennt man \bet{Modellfehler}.
Um eine gute Approximation der exakten Lösung $u(\mu)$ und der Ausgabe $s(\mu)$ zu erhalten, müssen beide Fehleranteile klein sein.

\ssect{3.25 Satz}{(Lipschitzstetigkeit)}
Falls $b$ und $f$ gleichmäßig beschränkt und Lipschitz-stetig bzgl. $\mu$ und $b$ gleichmäßig koerziv bzgl. $\mu$, so sind auch die Lösungen $u_N(\mu)$ und $s_N(\mu)$ von $(P_N(\mu))$ Lipschitz-stetig bzgl. $\mu$.

\ssect{3.26 Satz}{(Gleichungssytem und numersische Stabilität)}
Sei $\Phi_N = \penbrace{\phi_1\dt{,}\phi_N}$ eine reduzierte Basis von $X_N$.
Für $\mu\in\mathcal{P}$ definieren wir $\mathbb{B}_N(\mu) \in\R^{N\times N}$ und $\mathbb{F}_N(\mu)\in \R^N$ durch 
\[
(\mathbb{B}_N(\mu))_{nm} := b(\phi_m,\phi_n;\mu),~ (\mathbb{F}_N(\mu)_n := f(\phi_n;\mu)
\]
und
\[
\mathbb{U}_N(\mu) = (U_1^N(\mu)\dt{,}U_N^N(\mu))\in \R^N
\]
als Lösung von 
\begin{align}
\mathbb{B}_N(\mu)\mathbb{U}_N(\mu) = \mathbb{F}_N(\mu).
\end{align}
\begin{enumerate}[(1)]
	\item Dann ist $u_N(\mu)= \sum_{n=1}^{N} U_n^N(\mu)\phi_n$ und $s_N(\mu) = \mathbb{F}_N^T(\mu)\mathbb{U}_N(\mu)$ Lösung von $(P_N(\mu))$.
	\item Falls $\Phi_N$ orthogonal, so ist die Kondition von (3.2) unabhängig von beschränkt durch
	\[
	\cond_2(\mathbb{B}_N(\mu)) = \norm{\mathbb{B}_N(\mu)}_2 \norm{\mathbb{B}_N^{-1}(\mu)}_2\le \frac{\gamma(\mu)}{\alpha(\mu)}.
	\]
\end{enumerate}

\bet{Beweis:}\\
(1) klar.\\
(2): Wegen Symmetrie con $\mathbb{B}_N(\mu)$ ist 
\begin{align}
\cond_2(\mathbb{B}_N(\mu)) = \frac{\abs{\lambda_{\max}}}{\abs{\lambda_{\min}}}
\end{align}
mit betragsmäßig größten/kleinstem Eigenwert $\lambda_{\max},\lambda_{\min}$ von $\mathbb{B}_N(\mu)$.
Sei $\mathbb{U}_{\max} = (U_n)_{n=1}^N\in \R^N$ Eigenvektor zu $\lambda_{\max}$ und $u_{\max} := \sum_{n=1}^{N} U_n\phi_n$.
Dann gilt
\begin{align*}
\lambda_{\max} \norm{\mathbb{U}_{\max}}_2^2 &= \lambda_{\max} \mathbb{U}_{\max}^T\cdot\mathbb{U}_{\max} = \mathbb{U}_{\max}^T \mathbb{B}_N(\mu) \mathbb{U}_{\max}\\
&= \sum_{n,m=1}^N U_nU_m b(\phi_n,\phi_m;\mu) = b(\sum_{n=1}^{N} U_n\phi_n,\sum_{m=1}^{N} U_m\phi_m;\mu) \\
&= b(u_{\max},u_{\max};\mu)
\end{align*}
Aus der Orthogonalität folgt
\[
\norm{u_{\max}}_X^2 = 
\]

\ssect{3.27 Bemerkung}{}
Im Gegensatz zu großen aber dünn besetzten Matrizen bei FEM ist (3.2) klein aber voll besetzt, weil $\phi_i$ im Allgemeinen keinen disjunkten Träger haben.

\ssect{3.28 Folgerung}{}
Sei $\bar{\varphi}_i,~i=1\dt{,}\mathcal{N}_h$ die Knotenbasis von $X_h$ wie in Def. 2.50 definiert.
Für $\mu\in \mathcal{P}$ definieren wir $\mathbb{B}_h(\mu)\in \R^{\mathcal{N}_h\times \mathcal{N}_h}$ und $\mathbb{F}_h(\mu)\in \R^{\mathcal{N}_h}$ durch
\begin{align}
(\mathbb{B}_h(\mu))_{ij} := b(\bar{\varphi}_j,\bar{\varphi}_j;\mu),~ (\mathbb{F}_h(\mu))_i := f(\bar{\varphi}_i;\mu),~1\le i,j\le \mathcal{N}_h.
\end{align}
Indem wir dnun die RB-Basisfunktionen in der Knotenbasis darstellen
\begin{align}
\Phi_n = \sum_{i=1}^{\mathcal{N}_h} \phi_h^i\bar{\varphi}_i,~n=1\dt{,}N
\end{align}
können wir eine Transformationsmatrix $\mathbb{V}\in \R^{\mathcal{N}_h\times N}$ definieren deren Spalten die Koeffizienten der RB-Basisfunktionen in (3.5) enthalten:
\begin{align}
\mathbb{V}_{in} := \phi_n^i,~1\le i\le \mathcal{N}_h,~1\le n\le N.
\end{align}
Dann gilt für $\mathbb{B}_N(\mu)$ und $\mathbb{F}_N(\mu)$ aus Satz 3.25:
\[
\mathbb{B}_N(\mu)= \mathbb{V}^T\mathbb{B}_h(\mu)\mathbb{V} \text{ und } \mathbb{F}_N(\mu) = \mathbb{V}^T\mathbb{F}_h(\mu).
\]

\bet{Beweis:}\\
\begin{align*}
(\mathbb{V}^T\mathbb{B}_h(\mu)\mathbb{V})_{mn} &= \sum_{r,s=1}^{\mathcal{N}_h} (\mathbb{V}^T)_{mr} (\mathbb{B}_h(\mu))_rs (\mathbb{V}_{sn})\\
&= \sum_{r,s=1}^{\mathcal{N}_h} \phi_m^r b(\bar{\varphi}_s,\bar{\varphi}_r;\mu) \phi_n^s\\
&= b\enbrace{\sum_{s=1}^{\mathcal{N}_h}\phi_n^s \bar{\varphi}_s,\sum_{r=1}^{\mathcal{N}_h}\phi_m^r \bar{\varphi}_r;\mu}\\
&= b(\phi_n,\phi_m;\mu) = (\mathbb{B}_N(\mu))_{mn}.
\end{align*}
\hfill $\square$

\subsection{Offline/Online Zerlegung des RB-Modells}

\ssect{3.29 Bemerkung}{(Komplexitätsbetrachtungen)}
Da $\mathbb{B}_h(\mu)$ aus (3.4) dünn besetzt ist, erfordert die Berechnung von $u_h(\mu)$ $ \mathcal{O}(\mathcal{N}_h^2)$ Rechenschritte.
Da $\mathbb{B}_N(\mu)$ vollbesetzt ist das lineare Gleichungssystem (LGS) (3.2) in $\mathcal{O}(N^3)$ Rechenschritten lösbar.
Daher ist nur für $N\ll \mathcal{N}_h$ da RB-Modell ein Gewinn.
Genauere Betrachtung der Berechnung einer reduzierten Lösung $u_N(\mu)$:
\begin{enumerate}[(1)]
	\item $N$ Snapshots, also $N$ Lösungen $u_h(\mu)$ von $(P_h(\mu))$ berechnen: $\mathcal{O}(N\mathcal{N}_h^2)$. ('Offline')
	\item $N^2$ Auswertungen von $b(\phi_m,\phi_n;\mu)$: $\mathcal{O}(N^2\mathcal{N}_h)$.
	\item $N$ Auswertungen von $f(\phi_n;\mu)$: $\mathcal{O}(N\mathcal{N}_h)$.
	\item Die Lösung des LGS (3.2): $\mathcal{O}(N^3)$. ('Online')
\end{enumerate}
Damit lohnt sich das RB-Modell für ein einzelnes $\mu\in\mathcal{P}$ oder wenige $\mu\in\mathcal{P}$ \uline{nicht}.
Wenn wir $(P_N(\mu))$ aber für viele verschiedene Parameter $\mu\in\mathcal{P}$ lösen müssen, wie zum Beispiel in einem \textit{many-query} Kontext lohnt sich das RB-Modell, wenn man eine sogenannte \bet{Offline/Online Zerlegung} durchführt.

In der einmalig durchgeführten \bet{Offline-Phase} werden \uline{$\mu$-unabhängige, hochdimensionale} Größen in $\mathcal{O}(\mathcal{N}_h^m),m\in \N$ Rechenschritten, typischerweise teuer vorberechnet.
In der \uline{vielfach} durchgeführten \bet{Online-Phase} werden die Offline-Daten kombiniert um \uline{$\mu$-abhängige} Größen wie das reduzierte LGS (3.2) zu assemblieren.
Die RB-Lösung $u_N(\mu)$ und $s_N(\mu)$ können dann schnell berechnet werden, wobei die Anzahl der Rechenschritte idealerweise $\mathcal{O}(N^k),k\in \N$ ist, d.h. unabhängig von $\mathcal{N}_h$.

Vor diesem Hintergrund können wir Schritt 1 klar der Offline-Phase und Schritt 4 der Online-Phase zu ordnen.
Schritt 2 und 3 lassen sich direkt keiner der beiden Phasen klar zu ordnen, da sie sowohl teure als auch Parameter-abhängige Operationen benötigen.
Um eine klare Trennung auch von Schritt 2 und 3 zu erreichen benötigen wir eine spezielle Struktur der Bilinearform $b(.,.;\mu)$ und Linearform $f(.;\mu)$.
 
\ssect{3.30 Definition}{}
Seinen $X,X_1,X_2$ HR, $\mathcal{P}$ beschränkte Parametermenge.
\begin{enumerate}[(1)]
	\item Eine Funktion $v:\mathcal{P}\to X$ nennen wir \bet{affin parametrisch}, falls Funktionen $v^q\in X$ und Koeffizientenfunktionen $\theta_v^q: \mathcal{P}\to \R$ für $q=1\dt{,}Q_v$ existieren, so dass
	\[
	v(x;\mu) := \sum_{q=1}^{Q_v} \theta_v^q(\mu)v^q(x).
	\]
	\item Eine parametrische stetige Linearform $f:X\times \mathcal{P}\to \R$ bzw. stetige Bilinearform $b:X_1\times X_2\times \mathcal{P}\to \R$ ist \bet{affin parametrisch}, falls $f^q\in X'$ und $\theta_f^q:\mathcal{P}\to \R$ für $q=1\dt{,} Q_f$ bzw. $b^q:X_1\times X_2 \to \R$ und $\theta_B^q: \mathcal{P}\to \R$ für $q=1\dt{,}Q_b$ existieren, so dass
	\begin{align*}
	f(v;\mu) &= \sum_{q=1}^{Q_f} \theta_f^q(\mu) f^q(v)~\forall v\in X \text{ bzw.}\\
	b(u,v;\mu) &= \sum_{q=1}^{Q_b} \theta_b^q(\mu) b^q(u,v)~\forall u\in X_1,v\in X_2.
	\end{align*}
\end{enumerate}

\ssect{3.31 Folgerung}{(Offline/Online-Zerlegung von $(P_N(\mu))$)}
Sei $(P_N(\mu))$ gegeben und $b,f$ affin parametrisch. Dann erlaubt $(P_N(\mu))$ die folgende Offline/Online-Zerlegung:
\begin{itemize}
	\item[Offline-Phase:]
	Nach Berechnung einer reduzierten Basis $\Phi_N := \penbrace{\phi_1\dt{,}\phi_N}$ assemblieren wir die parameterunabhägige Matrizen und Vektoren $\mathbb{B}_N^q\in \R^{N\times N}$ und $\mathbb{F}_N^q\in \R^N$, definiert durch
	\[
	(\mathbb{B}_N^q)_{nm} := b^q(\phi_m,\phi_n),~1\le m,n \le N,~1\le q\le Q_b,
	\]
	\[
	(\mathbb{F}_N^q)_n := f^q(\phi_n),~1\le n\le N,~1\le q\le Q_f.
	\]
	\item[Online-Phase]
	Für einen gegebenen Parametervektor $\mu\in\mathcal{P}$ werten wir die parameterabhängigen Koeffizientenfunktionen $\theta_b^q(\mu),\theta_f^q(\mu)$ für $1\le q\le Q_b,Q_f$ aus und assemblieren die Matrix und den Vektor
	\[
	\mathbb{B}_N(\mu) := \sum_{q=1}^{Q_b} \theta_b^q(\mu) \mathbb{B}_N^q \text{ bzw. } \mathbb{F}_N(\mu) := \sum_{q=1}^{Q_f} \theta_f^q(\mu) \mathbb{F}_N^q,
	\]
	welche mit der Matrix und dem Vektor aus dem LGS (3.2) aus Satz 3.26 übereinstimmen.
	Dieses LGS kann dann nach $u_N(\mu)$ und $s_N(\mu)$ gelöst werden.
\end{itemize}

\ssect{3.32 Bemerkung}{}
Die Matrizen $\mathbb{B}_N^q\in \R^{N\times N}$ und die Vektoren $\mathbb{F}_N^q\in \R^N$ können mit dem in Folgerung 3.28 beschriebenen Verfahren einfach aus den entsprechenden FE-Matrizen und den FE-Vektoren mit der in (3.6) definierten Transformationsmatrix assembliert werden.

\ssect{3.33 Bemerkung}{(Rechenaufwand/Laufzeit)}
\begin{enumerate}[(1)]
	\item Für die Berechnung der Snapshots und der anschließenden Assemblierung von $\mathbb{B}_N^q,\mathbb{F}_N^q$ benötigen wir $\mathcal{O}(N\mathcal{N}_h^2 + N^2\mathcal{N}_hQ_b + N\mathcal{N}_hQ_f)$ Rechenschritte.
	In der Online-Phase kann dann die Assemblierung und das Lösen von LGS (3.2) in $\mathcal{O}(N^2Q_b + NQ_f + N^3)$ Rechenschritten erfolgen.
	Insbesondere hängt die Komplexität der Online-Phase nicht von $\mathcal{N}_h$ ab.
	\item Die Offline/Online Zerlegung lässt sich auch in einem Laufzeitdiagramm veranschaulichen. $t_{hoch},t_{offline},t_{online}$ bezeichnen die Laufzeit dür eine Lösung des hochdimensionalen, diskreten Problems $(P_h(\mu))$, die Offline- und die Online-Phase von $(P_N(\mu))$.
	Wir nehmen an, dass diese Zeiten für unterschiedliche Parameter jeweils dieselben sind und erhalten dadurch einen linearen Zusammenhang zwischen der gesamten benötigten Laufzeit und der Anzahl $k$ von Berechnungen der Lösungen $u_h(\mu),u_N(\mu)$.
	Die Gesamtlaufzeit für $k$ hochdimensionale Lösungen ist $t_h(k) = k\cdot t_{hoch}$, während das reduzierte Modell eine Laufzeit von $t_N(k) = t_{offline} + k\cdot t_{online}$ benötigt.
	Wie bereits in 3.29 erwähnt lohnt sich ein RB-Modell bei mehr als $k* := \frac{t_{offline}}{t_{hoch}-t_{online}}$ benötigten Approximationen von $u(\mu)$.
\end{enumerate}

\subsection{A posteriori Fehlerschätzer}

\subsubsection{A posteriori Fehlerschranken und Effektivität}

\ssect{3.34 Lemma}{(Fehler-Residuum Beziehung)}
Für $\mu \in \mathcal{P}$ definieren wir mittels der RB-Lösung $u_N(\mu)$ das Residuum $r(.;\mu)\in X_h'$ durch
\begin{align}
r(v;\mu) := f(v;\mu)- b(u_N(\mu),v;\mu) \forall v\in X_h
\end{align}
und den zugehörigen Riesz-Repräsentanten $R(\mu)\in X_h$ als Lösung von
\begin{align}
(R(\mu),v)_X = r(v;\mu) ~\forall v\in X_h.
\end{align}
Dann erfüllt der Fehler $e_N(\mu) := u_h(\mu)-u_N(\mu)$
\begin{align}
b(e_N(\mu),v;\mu) = r(v;\mu) ~\forall v\in X_h.
\end{align}

\bet{Beweis:}\\
\begin{align*}
b(e_N(\mu),v;\mu) &= b(u_h(\mu)-u_N(\mu),v;\mu)\\
&= b(u_h(\mu),v;\mu) - b(u_N(\mu),v;\mu)\\
&= f(v;\mu) - b(u_N(\mu),v;\mu) = r(v;\mu)
\end{align*}
\hfill $\square$

\ssect{3.35 Satz}{(A posteriori Fehlerschätzer)}
Für $\mu\in\mathcal{P}$ seinen $u_h(\mu),s_h(\mu)$ Lösungen von $(P_h(\mu))$ und $u_N(\mu),s_N(\mu)$ Lösungen von $(P_N(\mu))$.
Ferner sei $\alpha_{LB}(\mu) > 0$ eine berechenbare untere Schranke für die Koerzivitätskonstante $\alpha_h(\mu)$ von $b(.,.;\mu)$ und $R(\mu)$ der Riesz-Repräsentant des Residuums aus Lemma 3.34.
Dann erfüllen die A posteriori Fehlerschätzer , definiert durch 
\begin{align}
\Delta_N^{en}(\mu) := \frac{\norm{R(\mu)}_X}{\sqrt{\alpha_{LB}(\mu)}} \text{ und } \Delta_N^s(\mu) := \frac{\norm{R(\mu)}_X^2}{\alpha_{LB}(\mu)},
\end{align}
die folgenden Ungleichungen
\begin{align}
|||u_h(\mu)-u_N(\mu)|||_\mu = |||e_N(\mu)|||_\mu \le \Delta_N^{en}(\mu)\\
s_h(\mu)-s_N(\mu) \le \Delta_N^s(\mu).
\end{align}

\bet{Beweis:}\\
Testen von Gleichung (3.9) mit $e_N(\mu)$ ergibt:
\begin{align*}
|||e_N(\mu)|||_\mu &\bgl{Def} b(e_N(\mu),e_N(\mu);\mu) \bgl{(3.9)} r(e_N(\mu);\mu)\\
&\bgl{(3.8)} (R(\mu),e_N(\mu))_X \stackrel{C.S}{\le} \norm{R(\mu)}_X\norm{e_N(\mu)}_X\\
&\bgl{3.4} \frac{1}{\sqrt{\alpha_h(\mu)}} \norm{R(\mu)}_X |||e_N(\mu)|||_\mu \le \frac{1}{\sqrt{\alpha_{LB}(\mu)}} \norm{R(\mu)}_X |||e_N(\mu)|||_\mu\\
&\Rightarrow |||e_N(\mu)|||_\mu \le \Delta_N^{en}(\mu) \Rightarrow (3.11)
\end{align*}
Aus Satz 3.19 folgt
\[
s_h(\mu) -s_N(\mu) = |||e_n(\mu)|||_\mu^2 \le (\Delta_N^{en}(\mu))^2 = \Delta_N^s(\mu).
\]
\hfill $\square$

\ssect{3.36 Folgerung}{}
Durch $\hat{s}_N(\mu) := s_N(\mu) + \Delta_N^s(\mu)$ ist eine obere Schranke für $s_h(\mu)$ gegeben, das heißt es gilt
\[
s_h(mu) \le \hat{s}_N(\mu).
\]

\bet{Beweis:}\\
Folgt direkt aus (3.12).

\ssect{3.37 Bemerkung}{}
Das Beschränken des Fehlers durch das Residuum ist eine Standardtechnik zum Herleiten von A posteriori Fehlerschätzern für FEM.
Da in diesem Fall dein Schätzer für den Fehler $|||u(\mu)-u_h(\mu)|||_\mu$ gesucht wird, ist $X$ unendlich-dimensional und die Norm $\norm{r(.;\mu)}_{X'}$ kann nicht berechnet werden.
Im Fall von RB Methoden ist $\norm{r(.;\mu)}_{X_h'}$ mit Hilfe des Riesz-Repräsentanten berechenbar.

\ssect{3.38 Bemerkung}{}
Da $\Delta_N^{en}(\mu)$ und $\Delta_N^s(\mu)$ unter den Voraussetzungen von Satz 3.35 obere Schranken für die Fehler sind, werden sie auch als \uline{rigorose} Fehlerschranken bezeichnet.
Bei A posteriori Fehlerschätzern für FEM treten häufig Konstanten in den Abschätzungen auf, welche nicht entsprechend nach oben/unten durch berechenbare Konstanten beschränkt werden können, so dass in diesen Fällen die A posteriori Fehlerschätzer den Fehler auch unterschätzen können;
sie also keine rigorose Fehlerschranken zu sein brauchen.
Mit Hilfe des Fehlerschätzers können wir die Dimension des RB-Raumes so bestimmen, dass der Approximationsfehler kleiner als eine vorgegebene Toleranz ist.
Um ein möglichest effizientes Verfahren zu erhalten ist es daher wünschenswert, dass der Quotient $\frac{\Delta_N^{en}(\mu}{|||e_N(\mu)|||_\mu}$ möglichst nahe an 1 ist.
Er ist $\ge 1$ wegen Satz 3.35.
Diesen Quotienten werden wir im Folgenden weiter untersuchen.

\ssect{3.39 Satz}{(Effektivitäten der Fehlerschätzer)}
Wir definieren die Effektivitäten $\eta_N^{en}(\mu)$ und $\eta_N^s(\mu)$ der Fehlerschätzer $\Delta_N^{en}(\mu)$ und $\Delta_N^s(\mu)$, definiert in (3.10), durch
\begin{align}
\eta_N^{en}(\mu) := \frac{\Delta_N^{en}(\mu)}{|||u_h(\mu)-u_N(\mu)|||_\mu} \text{ und } \eta_N^s(\mu) :=\frac{\Delta_N^s(\mu)}{s_h(\mu)-s_N(\mu)}.
\end{align}
Unter den Voraussetzungen von Satz 3.35 gilt dann
\begin{align}
&\eta_N^{en}(\mu) \le \sqrt{\frac{\gamma(\mu)}{\alpha_{LB}(\mu)}} \text{ und }\\
&\eta_N^s(\mu)\le \frac{\gamma(\mu)}{\alpha_{LB}(\mu)}.
\end{align}

\bet{Beweis:}\\
Zunächst folgt aus der Definition de Riesz-Repräsentanten und des Residuums in Lemma 3.34:
\begin{align*}
\norm{R(\mu)}_X^2 = (R(\mu),R(\mu))_X \bgl{(3.8)} r(R(\mu);\mu)\\
&\bgl{(3.9)} b(e_N(\mu),R(\mu);\mu \stackrel{C.S}{\le} |||e_N(\mu)|||_\mu |||R(\mu)|||_\mu\\
&\stackrel{3.4}{\le} |||e_N(\mu)|||_\mu \sqrt{\gamma(\mu)}\norm{R(\mu)}_X.
\end{align*}
\begin{align}
\Rightarrow \norm{R(\mu)}_X \le |||e_N(\mu)|||_\mu \sqrt{\gamma(\mu)}
\end{align}
\begin{align*}
\eta_N^{en}(\mu) &= \frac{\Delta_N^{en}(\mu)}{|||e_N(\mu)|||_\mu} \bgl{Def} \frac{\norm{R(\mu)}_X}{\sqrt{\alpha_{LB}(\mu)}|||e_N(\mu)|||_\mu}\\
&\stackrel{(3.16)}{\le} \sqrt{\frac{\gamma(\mu)}{\alpha_{LB}(\mu)}} \frac{|||e_N(\mu)|||_\mu}{|||e_N(\mu)|||_\mu}\\
&\Rightarrow (3.14)
\end{align*}
Aus Satz 3.19 folgt dann:
\begin{align*}
\eta_N^s(\mu)\bgl{Def} \frac{\Delta_N^s(\mu)}{s_h(\mu)s_N(\mu)} \bgl{Def/3.19} \frac{(\Delta_N^{en}(\mu))^2}{|||e_N(\mu)|||_\mu^2} \stackrel{(3.14)}{\le} \frac{\gamma(\mu)}{\alpha_{LB}(\mu)} \Rightarrow (3.15).
\end{align*}

\ssect{3.40 Folgerung}{}
Falls $u_h(\mu) = u_N(\mu)$ dann gilt automatisch $\Delta_N^{en}(\mu) = \Delta_N^s(\mu) = 0$.\\

\bet{Beweis:}\\
Folgt direkt aus Satz 3.39, kann aber auch unabhängig davon wie folgt eingesehen werden:
Da $0=b(0,v;\mu) = b(e_N(\mu),v;\mu) \bgl{(3.9)} r(v;\mu) \bgl{(3.8)} (R(\mu),v)_X$ für alle $v\in X$ gilt, folgt $\norm{R(\mu)}_X=0$ und damit $\Delta_N^{en}(\mu) = \Delta_N^s(\mu) = 0$.

\ssect{3.41 Bemerkung}{}
Folgerung 3.40 ist insbesondere dann relevant, wenn für einen Fehlerschätzer Schranken für die Effektivität (noch) nicht verfügbar sind.

\ssect{3.42 Folgerung}{(Fehlerschätzer für die $X$-Norm)}
Unter den Voraussetzungen von Satz 3.35 gilt für den Fehlerschätzer $\Delta_N(\mu) := \frac{\norm{R(\mu)}_X}{\alpha_{LB}(\mu)}$, dass 
\[
\norm{u_h(\mu)-u_N(\mu)}_X \le \Delta_N(\mu).
\] 
Ferner gilt für die Effektivität des Fehlerschätzers $\eta_N(\mu) := \frac{\Delta_N(\mu)}{\norm{u_h(\mu)-u_N(\mu)}_X}$ die folgende Schranke
\[
\eta_N(\mu) \le \frac{\gamma(\mu)}{\alpha_{LB}(\mu)}.
\]

\bet{Beweis:}\\
Analog zu den Beweisen von Satz 3.35/3.39 unter Verwendung von Lemma 3.4.
\hfill $\square$\\

Zusätzlich zu absoluten Fehlerschätzern wollen wir schließlich noch relative Fehlerschätzer herleiten und die zugehörigen Effektivitäten untersuchen.

\ssect{3.34 Satz}{(Relative Fehlerschätzer)}
Wir definieren die relativen Fehlerschätzer 
\[
\Delta_N^{en,rel}(\mu) := 2\frac{\norm{R(\mu)}_X}{\sqrt{\alpha_{LB}(\mu)}}\cdot \frac{1}{|||u_N(\mu)|||_\mu}
\]
für den relativen Fehler in der Energienorm,
\[
\Delta_N^{rel}(\mu) := 2\frac{\norm{R(\mu)}_X}{\alpha_{LB}(\mu)}\cdot \frac{1}{||u_N(\mu)||_X}
\]
für den relativen Fehler in der $X$-Norm und 
\[
\Delta_N^{s,rel}(\mu) := \frac{\norm{R(\mu)}_X^2}{\alpha_{LB}(\mu)s_N(\mu)}
\]
für den relativen Ausgabefehler.
Dann gilt unter den Voraussetzungen von Satz 3.35 und falls $\Delta_N^{en,rel}(\mu)\le 1, \Delta_N^{rel}(\mu)\le 1$
\begin{align}
\frac{|||u_h(\mu)-u_N(\mu)|||_\mu}{|||u_h(\mu)|||_\mu} &\le \Delta_N^{en,rel}(\mu),\\
\frac{||u_h(\mu)-u_N(\mu)||_X}{||u_h(\mu)||_X} &\le \Delta_N^{rel}(\mu),\\
\frac{s_h(\mu)-s_N(\mu)}{s_h(\mu)} &\le \Delta_N^{s,rel}(\mu).
\end{align}

\bet{Beweis:}\\
Falls $\Delta_N^{en,rel}(\mu)\le 1$, so gilt
\begin{align}
\abs{\frac{|||u_h(\mu)|||_\mu-|||u_N(\mu)|||_\mu}{|||u_N(\mu)|||_\mu}} \le \frac{|||u_h(\mu)-u_N(\mu)|||_\mu}{|||u_N(\mu)|||_\mu} \stackrel{(3.11)}{\le} \frac{\norm{R(\mu)}_X}{\alpha_{LB}(\mu)|||u_N(\mu)|||_\mu} = \frac{\Delta_N^{en,rel}(\mu)}{2} \le \frac{1}{2}.
\end{align}
Aus (3.20) folgt:
\[
|||u_N(\mu)|||_\mu - |||u_h(\mu)|||_\mu \le \frac{1}{2} |||u_N(\mu)|||_\mu
\]
\begin{align}
\Rightarrow \frac{1}{2} |||u_N(\mu)|||_\mu \le |||u_h(\mu)|||\mu.
\end{align}
Damit gilt:
\begin{align*}
\frac{|||e_N(\mu)|||_\mu}{|||u_h(\mu)|||_\mu} \stackrel{(3.11)}{\le} \frac{\norm{R(\mu)}_X}{\sqrt{\alpha_{LB}(\mu)}|||u_h(\mu)|||_\mu} \stackrel{(3.21)}{\le} \frac{\norm{R(\mu)}_X}{\sqrt{\alpha_{LB}(\mu)}|||u_N(\mu)|||_\mu}\cdot 2 = \Delta_N^{en,rel}(\mu).
\end{align*}
Daraus folgt (3.17).
Und (3.18) folgt analog zu (3.17).
Schließlich gilt 
\begin{align*}
\frac{s_h(\mu)-s_N(\mu)}{s_h(\mu)} \stackrel{(3.12)}{\le} \frac{\Delta_N^{s}(\mu)}{s_h(\mu)} \stackrel{3.19}{\le} \frac{\Delta_N^s(\mu)}{s_N(\mu)} = \Delta_N^{s,rel}(\mu).
\end{align*}
\hfill $\square$

\ssect{3.44 Satz}{(Effektivitäten der relativen Fehlerschätzer)}
Wir definieren die Effektivitäten der relativen Fehlerschätzer $\eta_N^{en,rel}(\mu),\eta_N^{rel}(\mu),\eta_N^{s,rel}(\mu)$ wie folgt:
\begin{align*}
\eta_N^{en,rel}(\mu) := \frac{\Delta_N^{en,rel}(\mu)}{|||e_N(\mu)|||_\mu / |||u_h(\mu)|||_\mu},~ \eta_N^{rel}(\mu) := \frac{\Delta_N^{rel}(\mu)}{||e_N(\mu)||_X / ||u_h(\mu)||_X},~ \eta_N^{s,rel}(\mu) := \frac{\Delta_N^{s,rel}(\mu)}{(s_h(\mu)-s_N(\mu)/ s_h(\mu)}.
\end{align*}
Dann gilt unter den Voraussetzungen von Satz 3.35, falls $\Delta_N^{en,rel}(\mu)\le 1,\Delta_N^{rel}(\mu)\le 1,\Delta_N^{s,rel}(\mu)\le 1$:
\begin{align*}
\Delta_N^{en,rel}(\mu) \le 3\sqrt{\frac{\gamma(\mu)}{\alpha_{LB}(\mu)}} \tag{3.21b},
\end{align*}
\begin{align}
\Delta_N^{rel}(\mu) &\le 3\frac{\gamma(\mu)}{\alpha_{LB}(\mu)}\\
\eta_N^{s,rel}(\mu) &\le 2\frac{\gamma(\mu)}{\alpha_{LB}(\mu)}.
\end{align}

\bet{Beweis:}\\
Wie in Beweis von Satz 3.43 impliziert $\Delta_N^{en,rel}(\mu)\le 1$ dass
\[
\abs{\frac{|||u_h(\mu)|||_\mu-|||u_N(\mu)|||_\mu}{|||u_N(\mu)|||_\mu}}  \le \frac{1}{2}.
\]
Damit gilt $|||u_h(\mu)|||_\mu-|||u_N(\mu)|||_\mu \le \frac{1}{2} |||u_N(\mu)|||_\mu$ und damit
\begin{align}
|||u_h(\mu)|||_\mu \le \frac{3}{2} |||u_N(\mu)|||_\mu.
\end{align}
Damit folgt:
\begin{align*}
\eta_N^{en,rel}(\mu) &\bgl{Def} \frac{2\norm{R(\mu)}_X}{\sqrt{\alpha_{LB}(\mu)}|||u_h(\mu)|||_\mu} \frac{|||u_h(\mu)|||_\mu}{|||e_N(\mu)|||_\mu}\\
&\stackrel{(3.16)}{\le} 2\frac{\sqrt{\gamma(\mu)}\norm{e_N(\mu)}_X}{\sqrt{\alpha_{LB}(\mu)}|||u_h(\mu)|||_\mu} \frac{|||u_h(\mu)|||_\mu}{|||e_N(\mu)|||_\mu} \stackrel{(3.24)}{\le} 3 \sqrt{\frac{\gamma(\mu)}{\alpha_{LB}(\mu)}} \Rightarrow (3.21b).
\end{align*}
(3.22) folgt analog zu (3.21b).
Schließlich gilt
\begin{align*}
\eta_N^{s,rel}(\mu) = \frac{\Delta_N^s(\mu) / s_N(\mu)}{(s_h(\mu)-s_N(\mu)) / s_h(\mu)} = \eta_N^s(\mu) \frac{s_h(\mu)}{s_N(\mu)} \stackrel{(3.15)}{\le} \frac{\gamma(\mu)}{\alpha_{LB}(\mu)} \frac{s_h(\mu)}{s_N(\mu)}.
\end{align*}
Der letzte Faktor ist beschränkt, da
\[
\frac{s_h(\mu)}{s_N(\mu)} = 1 + \frac{s_h(\mu)-s_N(\mu)}{s_N(\mu)} \stackrel{(3.19)}{\le} 1 + \Delta_N^{s,rel}(\mu) \le 2 \Rightarrow (3.23).
\]
\hfill $\square$

\ssect{3.45 Bemerkung}{}
Durch "Tauschen" des Skalarprodukts/der Norm auf $X_h$ können die Fehlerschätzer und Effektivitäten verbessert werden, ohne die Ausgabe des reduzierten Modells oder die reduzierte Lösung zu verändern.
Wähle $\bar{\mu}\in\mathcal{P}$ fest und betrachte auf $X_h$ das Skalarprodukt $(((.,.)))_{\bar{\mu}}$ mit induzierter Norm $|||.|||_{\bar{\mu}}$.
Wegen Lemma 3.4 Normäquivalenz folgt aus der Stetigkeit und Koerzivität der Bilinearform $b$ auf $X_h$ bzgl. der $X$-Norm die Stetigkeit und Koerzivität auf $X_h$ bzgl. der $|||.|||_{\bar{\mu}}$-Norm und umgekehrt.
Gleiches gilt für die Stetigkeit von $l$ und $f$.
Dann gilt $\alpha_h(\bar{\mu}) := \inf_{v\in X_h} \frac{b(v,v;\bar{\mu})}{|||v|||_{\bar{\mu}}^2} = 1$ und $\gamma_h(\bar{\mu}) := \sup_{u,v\in X_h} \frac{b(u,v;\bar{\mu})}{|||u|||_{\bar{\mu}}|||v|||_{\bar{\mu}}}  = \sup_{u,v\in X_h} \frac{(((u,v)))_{\bar{\mu}}}{|||u|||_{\bar{\mu}}|||v|||_{\bar{\mu}}} \le 1$.
Für einen Fehlerschätzer welcher auf der $|||.|||_{\bar{\mu}}$-Norm des Riesz-Repräsentanten $R(\mu)$ basiert gilt also $\eta(\bar{\mu})=1$.
Er ist in diesem Sinne optimal.
Nimmt man an, dass $\alpha_h(\mu)$ und $\gamma_h(\mu)$ stetig von Parameter $\mu$ abhängen, kann man erwarten auch in einer Umgebung von $\bar{\mu}$ sehr effektive Fehlerschätzer zu erhalten.

\subsubsection{Offline/Online-Zerlegung des Fehlerschätzers}
Damit wir in der Online-Phase verifizeren können, dass der Approximationsfehler unter einer vorgegeben Toleranz liegt, ist es wichtig, dass wir auch den Fehlerschätzer Offline/Online zerlegen können.
Die Erkenntnis, dass sich die affine Parameterabhängigkeit der Bilinearform $b$ und der Linearform $f$ auf das Residuum und die Norm des Riesz-Repräsentanten überträgt ist hierbei von zentraler Bedeutung.
Mit Lemma 3.34 und der afiinen Parameterabhängigkeit von $b$ und $f$ folgt:
\begin{align*}
(R(\mu),v) &= r(v;\mu) = f(v;\mu) - b(u_N(\mu),v;\mu)\\
&= \sum_{q=1}^{Q_f} \theta_f^q(\mu)f^q(v)- \sum_{q=1}^{Q_b}\sum_{n=1}^{N} \theta_b^q(\mu)U_n^N(\mu)b^q(\phi_n,v).
\end{align*}
Nach dem Riesz'schen Darstellungssatz 2.9 existieren $R_f^q\in X-h$ mit 
\begin{align}
(R_f^q,v)_X = f^q(v) ~\forall v\in X_h,~1\le q\le Q_f
\end{align}
und $R_b^{q,n}\in X_h$ mit
\begin{align}
(R_b^{q,n},v)_X = b^q(\phi_n,v) ~\forall v\in X_h,~1\le q\le Q_b,~1\le n\le N.
\end{align}
Daher können wir weiter umformen:
\begin{align*}
(R(\mu),v)_X &= \sum_{q=1}^{Q_f} \theta_f^q(\mu)(R_f^q,v)_X - \sum_{q=1}^{Q_b}\sum_{n=1}^{N} \theta_b^q(\mu)U_n^N(\mu)(R_b^{q,n},v)_X\\
&= \enbrace{\sum_{q=1}^{Q_f} \theta_f^q(\mu)R_f^q - \sum_{q=1}^{Q_b}\sum_{n=1}^{N} \theta_b^q(\mu)U_n^N(\mu)R_b^{q,n},v}_X ~\forall v\in X_h\\
&\Rightarrow R(\mu) =  \sum_{q=1}^{Q_f} \theta_f^q(\mu)R_f^q - \sum_{q=1}^{Q_b}\sum_{n=1}^{N} \theta_b^q(\mu)U_n^N(\mu) R_b^{q,n}.
\end{align*}
Wir fassen dieses Resultat im folgenden Lemma zusammen.

\ssect{3.46 Lemma}{(Affine Parameterabhängigkeit von $R(\mu)$}
Seien $b,f$ affin parametrisch und $R_f^q,R_b^{q,n}\in X_h$ definiert wie in (3.25)/(3.26) .
Sei $Q_R:= Q_f+N\cdot Q_b$ und $R_R^q$ für $1\le q\le Q_R$ eine Aufzählung von $R_f^q,R_b^{q,n}$:
\begin{align*}
(R_R^1\dt{,}R_R^{Q_R}) := (R_f^1\dt{,}R_f^{Q_f},R_b^{1,1}\dt{,}R_b^{Q_b,1},R_b^{1,2}\dt{,}R_b^{Q_b,2}\dt{,}R_b^{1,N}\dt{,}R_b^{N,Q_b}).
\end{align*}
Für $\mu\in\mathcal{P}$ sei $u_N(\mu) = \sum_{n=1}^{N} U_n^N(\mu) \phi_n$ die Lösung von $(P_N(\mu))$.
Dann definieren wir $\theta_R^q: \mathcal{P}\to \R,~1\le q\le Q_R$ durch
\begin{align*}
(\theta_R^1(\mu)\dt{,}\theta_R^{Q_R}(\mu)) := (\theta_f^1(\mu)\dt{,}\theta_f^{Q_f}(\mu), -\theta_b^1(\mu)U_1^N(\mu)\dt{,} -Q_b^{Q_b}(\mu)\dt{,} -\theta_b^{Q_b}(\mu)U_N^N(\mu)).
\end{align*}
Dann ist der Riesz-Repräsentant $R(\mu)\in X_h$ des Residuums affin parametrisch:
\begin{align*}
R(\mu) = \sum_{q=1}^{Q_R} \theta_R^q(\mu)R_R^q.
\end{align*}

\ssect{3.47 Lemma}{}
Sei $g\in X_h'$ und $\bar{\varphi}_i,~1=1\dt{,}N_h$ die Knotenbasis von $X_h$ wie in Def 2.50 definiert.
Dann definieren wir die (Skalarprodukt-)Matrix $\mathbb{X}_h\in\R^{N_h\times N_h}$ durch
\[
(\mathbb{X})_{ij} := (\bar{\varphi}_j,\bar{\varphi}_i)_X,~1\le i,j\le N_h.
\]
Indem wir den Riesz-Repräsentanten $v_g\in X_h$ zu $g\in X_h'$ in der Knotenbasis darstellen:
\[
v_g = \sum_{i=1}^{N_h} V_g^i \bar{\varphi}_i
\]
erhalten wir den Koeffizientenvektor $\mathbb{V}_g= (V_g^1\dt{,}V_g^{N_h})\in \R^{N_h}$ und damit $v_g$ durch Lösen des linearen Gleichungssystems
\[
\mathbb{X}_h\mathbb{V}_g = \mathbb{G},\text{ wobei } \mathbb{G} := (g(\bar{\varphi}_1)\dt{,}g(\bar{\varphi}_{N_h})\in \R^{N_h}.
\]

\bet{Beweis:}\\
Betrachte dazu eine beliebige Testfunktion $w = \sum_{i=1}^{N_h} W_i\bar{\varphi}_i\in X_h$ mit Koeffizientenvektor $\mathbb{W} = (W_1\dt{,}W_{N_h})$.
Dann gilt:
\begin{align*}
g(w) &= \sum_{i=1}^{N_h} W_i g(\bar{\varphi}_i) = \mathbb{W}^T \mathbb{G} = \mathbb{W}^T\mathbb{X}_h\mathbb{V}_g\\
&= \enbrace{\sum_{i=1}^{N_h} V_g^i \bar{\varphi}_i,\sum_{j=1}^{N_h} W_j \bar{\varphi}_j}_X = (v_g,w)_X.
\end{align*}
\hfill $\square$\\

Da $R(\mu)$ affin parametrisch ist, können wir die Berechnung der entsprechenden Norm in eine Offline- und eine Online-Phase zerlegen.

\ssect{3.48 Satz}{(Offline/Online-Zerlegung von $\norm{R(\mu)}$)}
\bet{Offline-Phase:}
Nach der Offline-Phase des RB-Modells wie in Folgerung 3.31 beschrieben, assemblieren wir die Matrix $\mathbb{K}\in \R^{Q_R\times Q_R}$ definiert durch
\[
\mathbb{K}_{ij} := (R_R^j,R_R^i)_X,~1\le i,j\le Q_R
\]
unter der Verwendung der Matrix $\mathbb{X}_h$.\\
\bet{Online-Phase:}
Für gegebenes $\mu$ und berechnete RB-Lösung $u_N(\mu)$ bestimmen wir den Koeffizientenvektor $\hat{\theta}_R(\mu) := (\theta_R^1(\mu)\dt{,}\theta_R^{Q_R}(\mu))^T\in \R^{Q_R}$ und erhalten
\[
\norm{R(\mu)}_X = \sqrt{\hat{\theta}_R(\mu)^T\mathbb{K}\hat{\theta}_R(\mu)}.
\]

\bet{Beweis:}\\
\begin{align*}
\norm{R(\mu)}_X^2 &= \enbrace{\sum_{q=1}^{Q_R}\theta_R^q(\mu)R_R^q,\sum_{q'=1}^{Q_R}\theta_R^{q'}(\mu)R_R^{q'} }_X = \hat{\theta}_R^T(\mu)\mathbb{K}\hat{\theta}_R(\mu).
\end{align*}
Ferner benötigen wir für die relativen Fehlerschätzer die Norm von $u_N(\mu)$.

\ssect{3.49 Satz}{(Offline/Online Zerlegung der Norm von $U_N(\mu)$)}
\bet{Offline-Phase:}
Im Fall von $\Delta_N^{rel}(\mu)$ assemblieren wir nach der in Folgerung 3.31 beschriebenen Offline-Phase des RB-Modells die reduzierte (Skalarprodukt-)Matrix $\mathbb{X}_N\in \R^{N\times N}$ definiert durch
\[
(\mathbb{X}_N)_{nm} := (\phi_m,\phi_n)_X.
\]
\bet{Online-Phase:}
Für gegebenes $\mu$ und wie in Folgerung 3.31 beschrieben berechnetes $u_N(\mu) = \sum_{n=1}^{N} U_n^N(\mu)\phi_n$ mit Koeffizientenvektor $\mathbb{U}_N(\mu) = (U_1^N(\mu)\dt{,}U_N^N(\mu))$ berechnen wir $\norm{u_N(\mu)}_X = \sqrt{\mathbb{U}_N^T(\mu)\mathbb{X}_N\mathbb{U}_N(\mu)}$ für $\Delta_N^{rel}(\mu)$ oder $|||u_N(\mu)|||_\mu = \sqrt{\mathbb{U}_N^T(\mu)\mathbb{B}_N(\mu)\mathbb{U}_N(\mu)}$ für $\Delta_N^{en,rel}(\mu)$, wobei $\mathbb{B}_N(\mu)$ definiert wie in Satz 3.26.\\

\bet{Beweis:}\\
\begin{align*}
|||u_N(\mu)|||_\mu^2 = b(u_N(\mu),u_N(\mu);\mu) = \sum_{n,m=1}^{N} U_n^N(\mu)U_m^N(\mu) b(\phi_n(\mu),\phi_m(\mu);\mu) = \mathbb{U}_N^T(\mu) \mathbb{B}_N(\mu) \mathbb{U}_N(\mu).
\end{align*}
Wie in Folgerung 3.28 können wir mit Hilfe der Transformationsmatrix $\mathbb{V}\in \R^{N_h\times N}$, definiert durch
\[
\mathbb{V}_{in} := \phi_n^i,~1\le i\le N_h,~1\le n\le N,
\]
$\mathbb{X}_N = \mathbb{V}^T\mathbb{X}_N\mathbb{V}$ folgern.
Der Beweis für $\norm{u_N(\mu)}_X$ geht dann analog.

Schließlich benötigen wir noch eine untere Schranke für die Koerzivitätskonstante $\alpha_{LB}(\mu)$.
Falls $b$ gleichmäßig koerziv bzgl. $\mu$ mit $\inf_{\mu\in\mathcal{P}}\alpha_h(\mu)\ge \alpha_0$ und $\alpha_0$ analytisch bestimmbar oder berechenbar, kann $\alpha_0$ gewählt werden.
Für manche Randwertprobleme können wir sogar $\alpha(\mu)$ analytisch bestimmen und dann $\alpha_{LB}(\mu) = \alpha(\mu)$ wählen.

Im allgemeinen Fall lässt sich unter bestimmten Voraussetzungen die sogenannte \bet{Min-Theta-Methode} anwenden.

\ssect{3.50 Satz}{(Min-Theta-Methode zur Berechnung von $\alpha_{LB}(\mu)$)}
Sei $b$ koerziv und affin parametrisch mit $b^q(v,v)\ge 0 ~\forall v\in X_h$ und $\theta_b^q(\mu)>0~\forall\mu\in\mathcal{P},~q=1\dt{,}Q_b$.
Sei $\mu\in\mathcal{P}$ fest und $\alpha_h(\mu)$ bekannt.
Dann gilt
\[
0<\alpha_{LB}(\mu) \le \alpha_h(\mu)~\forall\mu\in\mathcal{P}
\]
mit der unteren Schranke 
\[
\alpha_{LB}(\mu) := \alpha_h(\bar{\mu}) \cdot \min\limits_{q=1\dt{,}Q_b} \frac{\theta_b^q(\mu}{\theta_b^q(\bar{\mu})}.
\]

\bet{Beweis:}\\
Da $\alpha_h(\mu)> 0$ und $0< c(\mu) := \min\limits_{q=1\dt{,}Q_b} \frac{\theta_b^q(\mu}{\theta_b^q(\bar{\mu})}$, gilt
\[
0 < \alpha_h(\mu) c(\mu) = \alpha_{LB}(\mu).
\]
Für alle $v\in X_h$ gilt dann:
\begin{align*}
b(v,v;\mu)&= \sum_{q=1}^{Q_b} \theta_b^q(\mu)b^q(v,v) = \sum_{q=1}^{Q_b} \frac{\theta_b^q(\mu)}{\theta_b^q(\bar{\mu})} \theta_b^q(\bar{\mu})b^q(v,v)\\
&\ge \sum_{q=1}^{Q_b} \enbrace{\min\limits_{q=1\dt{,}Q_b} \frac{\theta_b^q(\mu}{\theta_b^q(\bar{\mu})}} \theta_b^q(\bar{\mu})b^q(v,v)\\
&= c(\mu) b(v,v;\bar{\mu}) \ge c(\mu) \alpha_h(\bar{\mu})\norm{v}_X^2 = \alpha_{LB}(\mu) \norm{v}_X^2.
\end{align*}
Also insbesondere:
\[
\alpha_h(\mu) := \min\limits_{v\in X_h} \frac{b(v,v;\mu)}{\norm{v}_X^2}\ge \alpha_{LB}(\mu).
\]
\hfill $\square$\\

Fpr die Min-Theta-Methode benötigen wir eine Auswertung von $\alpha_h(\mu)$ für $\mu=\bar{\mu}$ in der Offline-Phase.
Dazu ist es im Allgemeinen notwendig ein hochdimensionales Eigenwertproblem zu lösen.

\ssect{3.51 Satz}{(Berechnung von $\alpha_h(\mu)$ für das hochdimensionale, diskrete Modell)}
Sei $\mathbb{B}_h(\mu)$ definiert wie in (3.4) und $\mathbb{X}_h$ definiert wie in Lemma 3.47.
Dann ist
\begin{align}
\alpha_h(\mu) = \lambda_{\min} (\mathbb{X}_h^{-1}\mathbb{B}_h(\mu)),
\end{align}
wobei $\lambda_{\min}$ der kleinste Eigenwert der Matrix $\mathbb{X}_N^{-1}\mathbb{B}(\mu)$ bezeichnet.

\bet{Beweis:}\\
Da $\mathbb{X}_h$ symmetrisch und positiv definit, lässt sich $\mathbb{X}_h$ mittels Cholesky-Zerlegung als $\mathbb{X}_h = \mathbb{LL}^T$ darstellen.
Mittels Substitution erhalten wir ($y\neq 0$):
\begin{align*}
\alpha_h(\mu) &= \inf\limits_{y\in X_h} \frac{b(y,y;\mu)}{\norm{y}_X^2} =\inf\limits_{\mathbb{Y}\in \R^{N_h}}\frac{\mathbb{Y}^T\mathbb{B}_h(\mu)\mathbb{Y}}{\mathbb{Y}^T\mathbb{X}_h\mathbb{Y}}\\
\text{mit }\mathbb{W}:=\mathbb{L}^T\mathbb{Y} &= \inf\limits_{\mathbb{W}}\in \R^{N_h} \frac{(\mathbb{L}^{-T}\mathbb{W})^T\mathbb{B}_h(\mu)(\mathbb{L}^{-T}\mathbb{W})}{(\mathbb{L}^{-T}\mathbb{W})^T\mathbb{L}\mathbb{L}^T(\mathbb{L}^{-T}\mathbb{W})} = \inf\limits_{\mathbb{W}}\in \R^{N_h} \frac{\mathbb{W}^T\mathbb{L}^{-1}\mathbb{B}_h(\mu)\mathbb{L}^{-T}\mathbb{W}}{\mathbb{W}^T\mathbb{W}}.
\end{align*}
Damit minimiert $\alpha_h(\mu)$ den Rayleigh-Quotienten zur symmetrischen Matrix $\tilde{\mathbb{B}}_h(\mu) := \mathbb{L}^{-1}\mathbb{B}_h(\mu)\mathbb{L}^{-T}.$

\bet{Satz von Courand-Fischer (Minimum-Maximums Prinzip)}\\
Ist $A\in \R^{n\times n}$ symmetrisch mit aufsteigend sortierten Eigenwerten $\lambda_1\dt{\le}\lambda_n$ und bezeichnet $X_i$ die Menge der $i$-dimensionalen Untervektorräume von $\R^n,~i=1\dt{,}n$, dann hat der $i$-te Eigenwert von $A$ die Darstellung:
\[
\lambda_i = \min\limits_{X\in X_i}\max\limits_{x\in X,x\neq 0} \frac{(x,Ax)_2}{(x,x)} = \max\limits_{X\in X_{n-i+1}}\min_{x\in X,x\neq 0} \frac{(x,Ax)_2}{(x,x)_2}. 
\]

Nach diesem Prinzip ist $\alpha_h(\mu)$ damit der kleinste Eigenwert von $\tilde{\mathbb{B}}_h(\mu)$.
Da die Matrizen $\tilde{\mathbb{B}}_h(\mu)$ und $\mathbb{X}_h^{-1}\mathbb{B}_h(\mu)$ ähnlich sind, wegen
\[
\mathbb{L}^T(\mathbb{X}_h^{-1}\mathbb{B}_h(\mu))\mathbb{L}^{-T} = \mathbb{L}^T\mathbb{L}^{-T}\mathbb{L}^{-1}\mathbb{B}_h(\mu)\mathbb{L}^{-T} = \tilde{\mathbb{B}_h(\mu)}
\]
und daher identische Eigenwerte haben, folgt damit die Behauptung.

\ssect{3.52 Bemerkung}{}
Da insbesondere die großen Matrizen $\mathbb{X}_h$ nicht invertiert werden sollte, verwendet man in der Praxis entweder einen Eigenwertlöser, welcher nur mittels Matrix-Vektor Produkten operiert:
Sobald das Produkt $\mathbb{Y} = \mathbb{X}_h^{-1}\mathbb{B}_h(\mu)\mathbb{W}$ berechnet werden muss, löst man stattdessen  $\mathbb{X}_h\mathbb{Y} = \mathbb{B}_h(\mu)\mathbb{W}$. 
Alternativ können auch Löser für generalisierte Eigenwertprobleme der Form $\mathbb{B}_h(\mu)\mathbb{Y} = \lambda\mathbb{X}\mathbb{Y}$ verwendet werden.

\ssect{5.53 Bemerkung}{}
Auch im kontinuierlichen Fall kann man ein Eigenwertproblem betrachten und lösen um $\alpha(\mu)$ zu bestimmen. Siehe dazu \cite{Haa11}.

\ssect{3.54 Bemerkung}{}
Für Probleme in denen man die Min-Theta-Methode nicht anwenden kann, kann man stattdessen die \bet{Successive Constraint Methode} verwenden (s. \cite{HRSP07}).

\ssect{3.55 Bemerkung}{}
Damit können wir die Offline/Online Zerlegung des Fehlers wie folgt zusammenfassen:
\bet{Offline:}\\
Berechnung des Riesz-Repräsentanten und Assemblierung der Matrix $\mathbb{K}$ aus Satz 3.48 in 
\[
\mathcal{O}((Q_f+NQ_b)N_h^2+(Q_f+NQ_b)^2N_h) \text{ Rechenschritten.}
\]
Für relativen Fehlerschätzer möglicherweise Assemblierung von $X_N$ aus Satz 3.49.
Falls die Min-Theta Methode verwendet wird, muss noch $\alpha_h(\bar{\mu})$ in $\mathcal{O}(N_h\sigma)$ mit $\sigma\ge 2$ einmalig berechnet werden, wie in Satz 3.51 beschrieben.\\

\bet{Online:}\\
Wie in Satz 3.48 beschrieben bestimmen wir $\norm{R(\mu)}_X$ in $\mathcal{O}((Q_f+NQ_b)^2)$ Rechenschritten.
Für den rel. Fehlerschätzer berechnen wir zusätzlich entweder $|||u_N(\mu)|||_\mu$ in $\mathcal{O}(N^2)$ Rechenschritten oder $\norm{u_N(\mu)}_X$ in $\mathcal{O}(N^2)$ Rechenschritten.
Schließlich können wir für die Min-Theta Methode $\alpha_{LB}(\mu)$ in $\mathcal{O}(Q_b)$ Rechenschritten bestimmen.

Die Online Phase zur Bestimmung des Fehlerschätzers ist damit unabhängig von $N_h$.
Da die reduzierte Lösung in $\mathcal{O}(N^3)$ Rechenschritten berechnet werden kann, ist für große $Q_b$ die Berechnung des Fehlerschätzers aber möglicherweise der Teil, der Online Phase, welcher die meiste zeit benötigt.

\ssect{3.56 Bemerkung}{(Relevanz des Fehlerschätzers)}
Fehlerschätzer werden in den RB Methoden sowohl in der Online Phase zur 'Certification' der Approximation, als auch in der Offline Phase zur Basisgenerierung eingesetzt, wie wir im nächsten Kapitel sehen werden.
\newpage

\section{Basiskonstruktion}

\bet{Ziel:}\\
\begin{enumerate}[(1)]
	\item Bestimmung eines 'möglichst guten' RB-Raumes $X_N = \spann\{u_h(\mu)\}\subset X_h$ mit Basis $\phi_N$, welche $M:=\penbrace{u_h(\mu)~|~\mu\in\mathcal{P}}$ global approximiert.
	\item Optimales $X_N$: formalisierbar durch Minimierung eines Funktionals, z.B. minimiere den maximalen Energiefehler:
	\begin{align}
	\min\limits_{Y\subset X_h,\dim Y = N} \enbrace{\max\limits_{\mu\in\mathcal{P}} |||u_h(\mu)-u_N(\mu)|||_\mu}
	\end{align}
	oder Minimierung der mittleren Projektionsfehlers
	\begin{align}
	\min\limits_{Y\subset X_h,\dim Y = N} \int_P \norm{u_h(\mu)-P_Yu_h(\mu)}_X^2 \dint \mu,
	\end{align}
	wobei $P_Y$ die orthogonale Projektion auf $Y$ bezeichnet.
	\item Gute Basis $\phi_N$: orthogonal für numerische Stabilität; Hierachie, so dass Basisvektoren nach Relevanz geordnet sind, d.h.
	\[
	X_N:= \spann\{\varphi_1\dt{,}\varphi_{N'}\} \text{ Sequenz von optimalen Räumen,}
	\]
	für $1\le N' \le N$, damit $N'$ Variation einer Fehlerkontrolle erlaubt.
\end{enumerate}
Was wir formal unter einem optimalen Unterraum verstehen wollen, können wir mit Hilfe der sogenannten Kolmogorov-$n$-Weite beschreiben.

\ssect{4.1 Definition}{(Kolmogorov-$n$-Weite, optimaler Unterraum)}
Sei $A$ eine kompakte Teilmenge in einem HR $X$.
Zu einem abgeschlossenen Unterraum $X_n\subset X$ mit $\dim X_n = n$ nennen wir
\begin{align*}
E(X_n,A) &= \sup\limits_v\in X \inf\limits_{w\in X_n} \norm{v-w}_X\\
&= \sup\limits_{v\in A} \norm{v-P_{X_n}v}_X
\end{align*}
Abstand von $X_n$ zu $A$.
Für $n\in \N$ nennen wir 
\begin{align}
d_n(A,X) := \inf\limits_{X_n\subset X, \dim X_n = n} E(X_n,A)
\end{align}
die \bet{Kolmogorov-$n$-Weite} der Menge $A$.
Ein Unterraum $X_n\subset X$ mit $\dim X_n = n$, welcher die Kolmogorov-$n$-Weite minimiert heißt optimaler Unterraum für $d_n(A,X)$.

\ssect{4.2 Bemerkung}{}
\begin{enumerate}[(1)]
	\item Die Kolmogorov-n-Weite ist also ein Maß für die Bestapproximation durch lineare Unterräume.
	\item Es gilt trivialerweise $d_0(A,X)= \sup\limits\norm{v}_X$ und falls $n_0:=\dim span(A) <\infty$, so gilt $d_n(A,X)=0 \forall n\ge n_0$.
	\item Für $A=M\subset X_h$ mit $u_h(\mu)$ Lösung von $(P_h(\mu))$ und $u_N(\mu)$ Lösung von $(P_N(\mu))$ gilt also
	\begin{align*}
	\norm{u_N(\mu)-u_h(\mu)}_X &\le \sqrt{\frac{\gamma(\mu)}{\alpha(\mu)}} \inf\limits_{v\in X_N} \norm{u_N(\mu)-v}\\
	&\le E(X_n,M) \sup\limits_{\mu\in P} \sqrt{\frac{\gamma(\mu)}{\alpha(\mu)}}.
	\end{align*}
	Falls also $E(X_n,M)$ und $\frac{\gamma(\mu)}{\alpha(\mu)}$ beschränkt, so ist der RB-Fehler klein.
	\item Präzise Werte für $d_n$ sind selten bekannt.
	Für endliche Mengen der Einheitskugeln können aber exakte Werte der Schranken für $d_n$ angegeben werden [Pinkus, 85].
	\item \bet{Beispiel:} $A:=\penbrace{v\in X~|~\norm{v}\le 1}$ erfüllt $d_n(A,x)=1$ für alle $n\le \dim X$ und $d_n(A,X)=0$ für $n\ge \dim X$.
	\item \bet{Beispiel:} $A:=[-1,1]^m\subset X:=\R^m$ erfüllt $d_n(A,X)=\sqrt{m-n}$ für alle $n\le m$ und $d_n(A,X) = 0$ für $n\ge m$.
	Da die am Anfang des Kapitels genannten Optimierungsprobleme sehr komplex sind, treffen wir Vereinfachungen:
	\begin{enumerate}[(i)]
		\item \bet{Diskretisierung des Parameterraums:}\\
		Wähle endliche TM $S_{\train}:=\{\mu^i\}_{i=1}^{n_{\train}} \subset P$ von Trainingsparametern, welche Trainingssnapshots $M_{\train} := \penbrace{u_h(\mu)~|~\mu\in S_{\train}}\subset M$ definieren.
		\item Schränke Optimierungsproblem auf $S_{\train}$ ein.
		Problem (4.2) wird dann in 
		\begin{align}
		\min\limits_{Y\subset X_h, \dim Y = N} \frac{1}{n_{\train}} \sum_{i=1}^{n_{\train}} \norm{u_h(\mu_i)-P_Y u_h(\mu_i)}^2_X
		\end{align}
		überführt.
	\end{enumerate}
\end{enumerate}
Wir werden in Abschnitt 4.2 sehen, dass die sogenannten POD-Räume Optimierungsproblem (4.4) lösen.
Zunächst beschäftigen wir uns jedoch mit dem Greedy-Algorithmus, welcher auf einer weiteren Approximation des Optimierungsproblems
\begin{align}
\min\limits_{Y\subset X_h, \dim Y = N} \max\limits_{\mu\in S_{\train}} |||u_h(\mu)-u_N(\mu)|||_\mu
\end{align}
beruht.

\subsection{Greedy-Algorithmus}
Da die Optimierung in (4.5) über die Unterräume immer noch ein sehr komplexes Optimierungsproblem ist, geht man stattdessen iterativ vor:

Wir konstruieren einen Approximationsraum, indem wir iterativ neue Basisfunktionen hinzufügen.
Hierbei wird die neue Basisfunktion gerade so gewählt, dass sie den Ausdruck
\[
\max\limits_{\mu\in S_{\train}} |||u_h(\mu)-u_N(\mu)|||_\mu
\]
minimiert.
Dies ist das Grundprinzip des \bet{Greedy-Algorithmus}, welcher Schrittweise sowohl die Sample-Menge $S_N$ als auch die Basis $\Phi_N$ erweitert.
Ein wichtiger Bestandteil ist ein Fehlerindikator $\Delta(Y,\mu)\in \R^+$, welcher den zu erwartenden Approximationsfehler für den Parameter $\mu$ schätzt, wenn man $Y=X_N$ als den Approximationsraum wählt.
Schließlich st $N_{\max}$ eine vorgegebene maximale Dimension des Raumes $X_N$.

\ssect{4.3 Definition}{(Greedy-Algorithmus)}
Sei $S_{\train}\subset P$ eine gegebene Trainingsmenge von Parametern und $\varepsilon_{tol}>0$ eine gegebene Fehlertoleranz.
Setze $X_0:={0}, S_0 := \emptyset, \Phi_0:= \emptyset$ und definiere iterativ:
\begin{lstlisting}
for n in range(1,N_max):
	Finde 
	%* $\mu^{(n)} := \argmax\limits_{\mu\in S_{\train}} \Delta(X_{n-1},\mu) $\\
	$S_n := S_{n-1} \cup \{\mu^{(n)}\}$
	$\phi_n = u_h(\mu^{(n)})$
	$\Phi_n := \Phi_{n-1} \cup\{\phi_n\}$
	$X_n:= \spann \{\Phi_n\}$ &*
	if %* $\max\limits_{\mu\in S_{\train}} \Delta(X_n,\mu) \le \varepsilon_{tol}$ &*:
		break
N:= n
return %* $S_N,\Phi_N$ &*
\end{lstlisting}

\ssect{4.4 Bemerkung}{}
Angenommen $N_{\max} = |S_{\train}|$. Wenn für alle $\mu \in P$ und Unterräume $Y\subset X_n$ gilt, dass
\begin{align}
u_h(\mu)\in Y \Rightarrow \Delta(Y,\mu) = 0,
\end{align}
dann endet der Greedy-Algorithmus nach höchstens $N\le |S_{\train}|$.
Dies liegt daran, dass (4.6) sicher stellt, dass kein Parameterwert in $S_{\train}$ zweimal gewählt wird.

\ssect{4.5 Bemerkung}{}
\begin{enumerate}[(1)]
	\item Der Greedy-Alg. erzeugt eine hierachische Basis.
	\item $\Phi_N$ erzeugt vom Greedy-Alg. ist eine Lagrange-RB Basis zur Samplemenge $S_N$, im Allgemeinen also nicht orthogonal.
	Um ein numerisch möglichst stabiles Verfahren zu erhalten, orthonomalisiert man die Basis mit Gram-Schmidt.
\end{enumerate}

\ssect{4.6 Bemerkung}{(Wahl des Fehlerindikators)}
Es gibt verschiedene Möglichkeiten für die Wahl des Fehlerindikators $\Delta(Y,\mu)$ im Greedy-Alg.
\begin{enumerate}[(1)]
	\item \bet{Projektionsfehler:} In manchen Fällen ist es sinnvoll im Greedy-Alg. den Bestapproximationsfehler zu verwenden:
	\[
	\Delta(Y,\mu) := \inf\limits_{v\in Y} |||u_h(\mu)-v|||_\mu = |||u_h(\mu)-P_Y u_h(\mu)|||_\mu.
	\]
	Dieser Fehlerindikator ist teuer, da seine Auswertung Operationen erfordert, welche mit $N_h$ skalieren.
	Daher kann $S_{\train}$ im Allgemeinen nur relativ klein gewählt werden.
	Ferner müssen alle Snapshots $u_h(\mu)$ verfügbar sein, was aus Speichergründen zusätzlich die Größe von $S_{\train}$ begrenzt.
	Der Vorteil dieser Fehlerindikatoren ist allerdings, dass weder ein RB Modell, noch ein a posteriori Fehlerschätzer für die Basisergänzung notwendig sind, was insbesondere für komplexe Probleme von Relevanz ist.
	Ferner schleißen wir aus dem Céa-Lemma, dass der so konstruierte RB-Raum gute Approximationseigenschaften hat.
	Wir bezeichnen diese Version des Greedy-Alg. als 'strong greedy'.
	\item \bet{RB-Fehler zwischen $u_h(\mu)$ und $u_N(\mu)$:} Falls wir ein RB-Modell, aber kein Fehlerschätzer zur Verfügung haben, können wir als Fehlerindikator
	\[
	\Delta(Y,\mu) = \ener{u_h(\mu)-u_N(\mu)}
	\]
	verwenden.
	Auch dieser Fehlerindikator ist teuer zu berechnen und benötigt alle Snapshots $u_h(\mu)$, was beides die Größe der Trainingsmenge $S_{\train}$ limitiert.
	Der Vorteil dieses Indikators ist, dass wir das Fehlermaß aus dem Optimierungsproblem in (4.5) betrachten.
	\item \bet{A posteriori Fehlerschätzer:} Falls ein Fehlerschätzer zur Verfügung steht verwendet man im Allgemeinen
	\[
	\Delta(Y,\mu) = \Delta_N^{en}(\mu)
	\]
	oder ein (relativen) Fehlerschätzer (für eine andere Norm). 
	Da Aufgrund der Offline/Online Zerlegung die Auswertung von $\Delta(Y,\mu) = \Delta_N^{en}(\mu)$ sehr billig ist und wir während dieses Greedy-Alg. nur $N$-mal das hochdimensionale diskrete Modell $(P_h(\mu))$ lösen müssen, kann $S_{\train}$ sehr groß gewählt werden.
	Daher erwarten wir unter gewissen Voraussetzungen an den Fehlerschätzer $\Delta_N^{en}(\mu)$, dass wir mit diesem Fehlerindikator die Lösung des Optimierungsproblems, über den konstanten Parameterraum $P$, (4.1) besser approximieren können.
	Diese Version des Greedy-Alg. bezeichnen wir mit 'weak greedy'.
\end{enumerate}
Alle drei Fehlerindikatoren erfüllen (4.6).
Dies gilt trivialerweise für den Projektionsfehler, folgt aus der Reproduzierbarkeit von Lösungen in Folgerung 3.18 für den RB-Fehler und schließlich aus Folgerung 3.40 für den Fehlerschätzer.

\ssect{4.7 Bemerkung}{}
Alternativ kann man auch Fehlerindikatoren für die Ausgabe wie $\abs{s_h(\mu)-s_N(\mu)}$ oder $\Delta_N^s(\mu)$ im Greedy-Alg. verwenden.
Diese werden häufig auch als 'goal-oriented error indicators' bezeichnet.
Letztere führen im Allgemeinen zu einem RB-Raum von niedriger Dimension, als die Indikatoren in Bemerkung 4.6, welcher $s_h(\mu)$, aber nicht notwendigerweise $u_h(\mu)$, gut approximiert.
Im Gegensatz dazu resultieren die Indikatoren aus Bemerkung 4.6 in einer größeren Basis, welche sowohl $u_h(\mu)$, als auch $s_h(\mu)$ gut approximiert.

\subsubsection{Konvergenzraten des Greedy-Algorithmus}
Bis vor einigen Jahren wurde der Greedy-Alg. als heuristischer Algorithmus angesehen, welcher in der Praxis in vielen Fällen zwar ausgezeichnet funktioniert, dem aber die theoretische Grundlage fehlt.
Dann wurden aber sehr nützliche approximationstheoretische Resultate gezeigt, zuerst f\"{u}r exponentielle \cite{Bufetal2012} und dann auch für algebraische Konvergenz \cite{Binetal2011} in HR und schließlich in \cite{DePeWo13} für Banachräume.

\ssect{4.8 Satz}{(Konvergenzraten für Greedy-Alg. \cite{Binetal2011})}
Sei $M_e := \penbrace{u(\mu)~|~\mu\in P}$, $P$ kompakt und der Fehlerindikator $\Delta(Y,\mu)$ so gewählt, dass für ein geeignetes $\tau\in (0,1]$ gilt:
\begin{align}
\ener{u(\mu^{(n+1)})-P_{X_n} u(\mu^{(n+1)})} \ge \tau \cdot \max\limits_{u\in M_e} \ener{u-P_{X_n}u}.
\end{align} 
Sei ferner $\sigma_n := \max\limits_{u\in M_e} \ener{u-P_{X_n}u}$.
Dann gilt:
\begin{enumerate}[(i)]
	\item \bet{Algebraische Konvergenz:}\\
	Falls $d_n(M_e,X)\le M n^{-\beta}$ für Konstanten $M,\beta\in \R^+$ unabhängig von $\tau$ und für alle $n\in \N$ und $d_0(M_e,X)\le M$, dann gilt:
	\[
	\sigma_n \le C\cdot Mn^{-\beta}
	\]
	mit geeigneter (berechenbarer) Konstante $C>0$.
	\item \bet{Exponentielle Konvergenz:}\\
	Falls $d_n(M_e,X) \le M e^{-an^\beta}$ für $n\ge 0$ und für Konstanten $M,\beta, a\in \R^+$, dann gilt:
	\[
	\sigma_n\le C\cdot Me^{-cn^\rho}, +n\ge 0
	\]
	mit $\rho := \frac{\beta}{\beta+1}$ mit geeignetem (berechenbaren) Konstanten $c,C>0$.
\end{enumerate}

\bet{Beweis:} siehe \cite{Binetal2011}.

\ssect{4.9 Bemerkung}{}
\begin{enumerate}[(1)]
	\item Lässt sich $M_e$ gut durch einen linearen Unterraum approximieren, so liefert der Greedy-Alg. einen Approximationsraum, welcher nur leicht schlechter ist, als der optimale Unterraum.
	\item In der Praxis verwenden wir im Greedy-Alg. die Menge $M:=\penbrace{u_h(\mu)~|~ \mu \in P}$.
	In \cite{Binetal2011} wurde gezeigt, dass sich in diesem Fall die Konvergenzraten für kleine Diskretisierungsfehler nicht verschlechtern.
\end{enumerate}

\ssect{4.10}{(strong/weak greedy)}
Für $\tau = 1$ heißt der Algorithmus strong greedy und für $\tau <1$ weak greedy.
Für $\Delta(Y,\mu)= \Delta_n^{en}(\mu)$ gilt (4.7) im Diskreten; wegen
\begin{align*}
\ener{u_h(\mu^{(n+1)}).P_{X_h} u_h(\mu^{(n+1)})} &= \inf\limits_{v\in X_h} \ener{u_h(\mu^{(n+1)})-v}\\
&\bgl{3.17/3.19} \ener{u_h(\mu^{(n+1)})-u_n(\mu^{(n+1)})} \bgl{3.39} \frac{\Delta_n^{en}(\mu^{(n+1)})}{\eta_n^{en}(\mu^{(n+1)})}\\
&\bgl{Def Greedy} \frac{1}{\eta_n^{en}(\mu^{(n+1)})}\cdot \max\limits_{\mu\in P} \Delta_n^{en}(\mu)\\
&\stackrel{3.35}{\ge} \frac{1}{\eta_n^{en}(\mu^{(n+1)})}\cdot \max\limits_{\mu\in P} \ener{u_h(\mu)-u_n(\mu)}\\
&\ge \frac{1}{\eta_n^{en}(\mu^{(n+1)})}\cdot \max\limits_{\mu\in P} \ener{u_h(\mu)-P_{X_h} u_h(\mu)}\\
&\stackrel{3.39}{\ge} \sqrt{\frac{\alpha_{LB}(\mu^{(n+1)})}{\gamma(\mu^{(n+1)})}} \cdot \max\limits_{\mu\in P} \ener{u_h(\mu)-P_{X_h} u_h(\mu)}.
\end{align*}
Für glm. beschränkte und koerzive Bilinearformen $b(.,.;\mu)$ erhalten wir dann einen weak greedy, mit $\tau = \sqrt{\frac{\alpha_0}{\gamma_0}}\in (0,1)$, wobei $\alpha_0,\gamma_0$ definiert in Definition 3.3.

\subsubsection{Praktische Realisierung}

\ssect{4.11 Bemerkung}{(Wahl von $S_{\train}$)}
Für niedrig dimensionale Parameterräume, d.h. $\abs{P}\le 5$, wird $S_{\train}$ häufig zufällig gewählt.

\ssect{4.12 Bemerkung}{(Overfitting)}
Die Folge von Fehlern $e_n := \max_{\mu\in S_{\train}}\Delta(X_n,\mu)$, welche durch den Greedy-Alg. produziert wird, kann sehr stark von $S_{\train}$ abhängen.
Wir können aufgrund von möglichem 'Overfitting' nicht notwendigerweise vom Approximationsverhalten des RB-Modells auf $S_{\train}$ auf das Approximationsverhalten auf $P$ schließen; es kann sogar 
\[
\sup\limits_{\mu\in P} \Delta(X_n,\mu) \gg e_n
\]
gelten.
Daher solte die Approximationsgüte des RB-Modells immer auf einer unabhängigen Testmenge $S_{test}\subset P$ validiert werden.

\ssect{4.13 Bemerkung}{(adaptive Verfeinerung der Trainingsmenge)}
Um sowohl eine große Trainingsmenge(hoher Rechenaufwand), als auch eine zu klein/schlecht gewählte Trainingsmenge (Overfitting) zu vermeiden, bietet es sich an die Trainingsmenge adaptiv zu verfeinern (siehe 
\cite{HaaDihOhl2011,HaaOhl2008b}). Hierbei wird die Trainingsmenge einem Gitter zu geordnet, wobei die Punkte der Trainingsmenge entweder als die Knoten \cite{HaaOhl2008b} oder zufällig innerhalb der Gitterelemente gewählt werden \cite{EftPatRon2010}.
Wie bei adaptiven FEM wird dann ein (lokaler) Fehlerschätzer auf den Gitterelementen ausgewertet und die Elemente mit dem größten Fehler zur Verfeinerung markiert.
Durch Verfeinerung der markierten Elemente werden neue Punkte zur Trainingsmenge hinzugenommen.
Auf diese Weise kann die Trainingsmenge an das betrachtete Problem angepasst und 'schwierige' Bereiche der Parametermenge, z.B. kleine Werte der Diffusionskonstanten, können identifiziert werden.
So kann die Trainingsmenge entsprechend verfeinert werden.
Außerdem besteht die Möglichkeit Overfitting vorzubeugen.

\ssect{4.14 Bemerkung}{(Zerlegung des Parameterraums)}
Im Greedy-Alg. aus Definition 4.3 wählt man im Allgemeinen $N_{\max}$ sehr groß, so dass die gewünschte Toleranz erreicht werden kann.
Es wäre wünschenswert, wenn man sowohl $N_{\max}$ und damit die Online-Laufzeit, als auch die Toleranz kontrollieren könnte.
Dies kann mit einer Zerlegung des Parameterraums, dem sogenannten hp-RB-Ansatz \cite{EftPatRon2010} erreicht werden.
Zunächst wird der Parameterraum adaptiv in Teilgebiete zerlegt (h-Adaptivität).
Anschließend werden für jedes Teilgebiet lokale reduzierte Basen generiert.
Falls auf einem Teilgebiet nicht die Toleranz und $N\le N_{\max}$ erreicht werden kann, wird dieses Teilgebiet erneut verfeinert/zerlegt und es werden neue lokale Basen generiert (p-Adaptivität).
In der Online Phase muss dann lediglich für einen Parameterwert $\mu$ das zugehörige Teilgebiet und (lokale) RB-Modell identifiziert werden.
Die da durch erzielte Kontrolle über Genauigkeit und Online-Laufzeit führt allerdings zu einer teureren und speicherintensiven Offline Phase.

\subsection{Proper Orthogonal Decomposition (POD)}
\subsubsection{Exkurs Spektraltheorie/Motivation}

\ssect{4.15 Lemma}{(Adjungierter Operator im HR-Sinne)}
Seien $X,Y$ HR.
Zu $A\in L(X,Y)$ existiert ein eindeutiger Operator $A\adj\in L(Y,X)$ mit
\[
(Ax,y)_Y = (x,A*y)_X ~\forall x\in X,y\in Y,
\]
der sogenannte \bet{adjungierte Operator}.\\

\bet{Beweis:}\\
Für jedes feste $y\in Y$ ist $x\mapsto (Ax,y)_Y$ stetig und linear.
Nach dem Riesz'schen Darstellungssatz existiert ein eindeutiges $z\in X$ mit $(Ax,y)_Y= (x,z)_X~\forall x\in X$.
Setze nun $A\adj y=z$.
Die Linearität ist klar und die Stetigkeit von $A*$ folgt aus der Stetigkeit von $A$, wegen
\begin{align*}
\norm{A\adj y}_X &= \norm{z}_X = \sup\limits_{x\in X\backslash\{0\}} \frac{(Ax,y)_Y}{\norm{x}_X}\\
&\stackrel{C.S.}{\le} \norm{y}_Y \sup\limits_{x\in X\backslash\{0\}} \frac{\norm{Ax}_X}{\norm{x}_X} = \norm{y}_Y\norm{A}.
\end{align*}
Eindeutigkeit folgt aus der Eindeutigkeit des Riesz-Repräsentanten.

\ssect{4.16 Definition}{(selbstadjungiert)}
Sei $A\in L(X)$, $A$ heißt \bet{selbstadjungiert}, falls $A=A\adj$.

\ssect{4.17 Beispiele}{}
\begin{enumerate}[(a)]
	\item Sei $X=\R^n$.
	Wird $A\in L(X)$ durch die Matrix $(a_{ij})_{ij}$ dargestellt, so wird $A\adj$ durch $(a_{ji})_{ij}$ dargestellt.
	Definition 4.16 ist also eine Verallgemeinerung des Begriffes der selbstadjungierten (symmetrischen) Matrix aus der Linearen Algebra.
	\item Sei $X=L^2((0,1)), u\in L^2((0,1)^2)$ und $A_u\in L(X)$ der Integraloperator
	\[
	(A_u,v)(x) = \int_0^1 u(x,\mu)v(\mu)\dint \mu.
	\]
	Dann ist $A_u\adj = A_{u\adj}$ mit $u\adj(x,\mu)=u(\mu,x)$.
	Dies kann als kontinuierliches Analogon von (a) aufgefasst werden.
	\item $A\adj A$ und $AA\adj$ sind stets selbstadjungiert.
\end{enumerate}

\bet{Beweis:}\\
\begin{enumerate}[(b)]
	\item 
	\begin{align*}
	(A_uv,w)_X &= \int_{0}^{1} (A_uv)(x)v(x)\dint x = \int_0^1\int_0^1 u(x,\mu)v(\mu)w(x)\dint\mu\dint x\\
	&= \int_0^1 v(\mu) \int_0^1 u(x,\mu)w(x)\dint x\dint \mu = \int_0^1 v(\mu)(A_u\adj w)(\mu)\dint \mu.
	\end{align*}
	\item
	\[
	(A\adj A v,w)_X = (Av,Aw)_Y = (v,A\adj A w)_X, ~AA\adj \text{ analog.}
	\]
	\hfill $\square$
\end{enumerate}

\bet{Erinnerung:}\\
\uline{Ziel:} Approximiere $u(x,\mu)$ durch $\sum_{n=1}^{N}U_n^N(\mu)\phi_n(x),~X_N=\spann\{\phi_1\dt{,}\phi_N\}$.
Als Minimierer von 
\[
\min\limits_{Y\subset X,\dim Y =N} \int_0^1\int_0^1\abs{u(x,\mu)-\sum_{n=1}^{N}U_n^N(\mu)\phi_n(x)}^2\dint x\dint \mu.
\]
Ein solches Problem wurde erstmals von Schmidt 1907 betrachtet.
Zur Lösung nutzte er aus, dass der Integraloperator $A_u$ aus 4.17 (b) kompakt ist (s. \cite{Alt12}, Satz 8.15, S.353f) und damit auch die Operatoren $A_u\adj A_u$ und $A_uA_u\adj$ kompakt sind, denn:

\ssect{4.18 Satz}{}
Seien $X,Y,Z$ Banachräume.
Sind $T\in L(X,Y)$ und $S\in L(Y,Z)$ und ist $T$ oder $S$ kompakt, so ist $ST$ kompakt.\\

\bet{Beweis:}\\
Ein Operator $T\in L(X,Y)$ ist genau dann kompakt, wenn für jede beschränkte Folge $(x_n)_\N\subset X$, die Folge $(T(x_n))_\N\subset Y$ eine konvergente Teilfolge enthält.
Sei also $(x_n)_\N$ eine beschränkte Folge, sei ferner zunächst $S$ kompakt.
Da $T\in L(X,Y)$ ist auch $(Tx_n)_\N$ beschränkt und $(STx_n)_\N$ besitzt eine konvergente Teilfolge.
Ist $S$ stetig, $T$ kompakt und damit $(Tx_n)_\N$ konvergent,so ist auch $(STx_n)_\N$ konvergent.
\hfill $\square$

\ssect{4.19 Satz}{(Hilbert-Schmidt Theorem)}
Sei $X$ reeller, seperabler HR und $A\in K(X)$ selbstadjungiert.
Dann existiert eine vollständige Orthonormalbasis $\{\psi_k\}_{k=1}^{\infty}$ von $X$, so dass
\[
A\psi_k = \lambda_k\psi_k
\]
und $\lambda_k\to 0,k\to \infty$.\\

\bet{Beweis:} s. \cite{Wer07}, Theorem VI.3.2, S. 269f.
\hfill $\square$\\

Da $A_u\adj A$ (und analog $A_uA_u\adj$) nicht negativ, weil $(A_u\adj A_u v)_X\ge 0 ~\forall v\in L^2((0,1))$, erhalten wir das folgende Resultat:

\ssect{4.20 Satz}{}
Sei $A_u$ wie in 4.17 (b) definiert.
Der Operator $R_u := A_uA_u\adj$ mit
\[
R_u v = \int_0^1 (v,u(\mu))_{L^2((0,1))} u(\mu)\dint \mu
\]
ist nicht negativ, selbstadjungiert und kompakt.
Ferner existiert eine vollständige Orthonormalbasis $\{\phi_k\}_{k=1}^\infty$ für $L^2((0,1))$ und eine Folge $\{\lambda_k\}_{k=1}^\infty$ nicht-negativer reeller Zahlen, so dass
\[
R_u\phi_k= \lambda_k\phi_k,~\lambda_1\ge \lambda_2\dt{\ge}0
\]
und $\lambda_k\to 0$ für $k\to \infty$.\\

\bet{Beweis:}\\
Folgt aus Satz 4.19.
\hfill $\square$\\

Definiere nun
\[
\psi_k(\mu):= (A_u\adj\phi_k)(\mu) = \int_0^1 u(x,\mu)\phi_k(x)\dint x,
\]
mit $\phi_k$ aus Satz 4.20.
Es lässt sich zeigen, dass $\{\phi_k\}_{k=1}^\infty$ Eigenfunktionen von $A_u\adj A_u$.
Die sogenannte 'Hilbert-Schmidt Zerlegung' von $u(x,\mu)$ ist dann gegeben durch:
\[
u(x,\mu) = \sum_{k=1}^{\infty} \psi_k(\mu)\phi_k(x) \text{ fast überall.}
\]
Schmidt zeigte weiterhin, dass
\begin{align*}
&\min \penbrace{\int_0^1\int_0^1 \abs{u(x,\mu)-\sum_{n=1}^{N}U_n^N(\mu)\phi_n(x)}^2\dint x\dint \mu~|~ U_n^N(\mu),\phi_n(x)\in L^2((0,1))}\\
&= \int_0^1\int_0^1 \abs{u(x,\mu)-\sum_{n=1}^{N}\psi_n(\mu)\phi_n(x)}^2\dint x\dint \mu = \sum_{k=N+1}^{\infty} \lambda_k.
\end{align*}

\subsubsection{Kontinuierliche POD}
\ssect{4.21 Satz}{(POD Basis)}
Sei $X$ ein HR mit $H_0^1(\Omega)\subset X\subset H^1(\Omega), P=[a_i,b_i]^p,i=1\dt{,}p$, wir definieren den Operator
\[
Rv := \int_P (v,u(\mu))_X u(\mu)\dint \mu \text{ für } v\in X,u(\mu)\in X.
\]
Ferner nehmen wir an, dass die betrachtete parameterabhängige Differentialgleichung erlaubt zu zeigen, dass $R$ kompakt (dazu reicht z.B. dass $u(\mu)$ stetig von $\mu$ abhängt).
Dann existiert eine vollständige Orthonormalbasis $\{\varphi_i\}_{i=1}^\infty$ von $X$ und eine Folge $\{\lambda_i\}_{i=1}^\infty$ nicht-negativer reeller Zahlen, so dass
\begin{align}
R\varphi_i = \lambda_i\varphi_i, \lambda_1\ge \lambda_2\dt{\ge} 0 \text{ und } \lambda_i \xrightarrow{i\to \infty} 0.
\end{align}
Sei $N'$, so dass $\lambda_{N'}>0$.
Für $1\le N\le N'$ definieren wir $\Phi_N:=\penbrace{\varphi_1\dt{,}\varphi_N}$ als POD-Basis und $X_N:= \spann\penbrace{\varphi_1\dt{,}\varphi_N}$ als POD-Raum.\\

\bet{Beweis:}\\
Wie oben zeigt man, dass $R$ nicht-negativ und selbstadjungiert. 
Aufgrund der angenommenen Kompaktheit folgt die Aussage, dann aus dem Hilbert-Schmidt Theorem.
\hfill $\square$\\

Es lässt sich nun zeigen, dass der POD-Raum den mittleren Projektionsfehler minimiert, also Minimierer des Minimierungsproblems (4.2) ist.

\ssect{4.22 Satz}{(Optimalität der POD-Basis)}
Für $l\in\N$ definieren wir die Abbildung $J: \underbracket{X\dt{\times} X}_{l-\text{mal}}\to \R$ durch
\begin{align}
J(\phi_1\dt{,}\phi_l) = \int_P \norm{u(\mu)-\sum_{i=1}^{l} (u(\mu),\phi_i)_X \phi_i}_X^2\dint \mu.
\end{align}
Seien $\{\lambda_i\}_{i=1}^\infty$ und $\{\phi_i\}_{i=1}^\infty$ die Eigenwerte und Eigenfunktionen von $R$.
Dann lösen für alle $l\in \N$ die ersten $L$ Eigenfunktionen $\phi_1\dt{,}\phi_L\in X$ das Minimierungsproblem
\begin{align}
\min J(\psi_1\dt{,}\psi_l) \text{ so dass } (\psi_j,\psi_i) = \delta_{ij},~1\le i,j\le l.
\end{align}
Ferner gilt:
\begin{align}
J(\phi_1\dt{,}\phi_l) = \sum_{i=l+1}^{\infty} \lambda_i \text{ für jedes } l\in \N.
\end{align}

\bet{Beweis:}\\
Der Beweis zu Teil 1 der Aussage beruht darauf, dass das Eigenwert-Problem (4.8) die notwendige Optimalitätsbedingung erster Ordnung (nOb1) für (4.10) ist.
Den Beweis findet man zum Beispiel in \cite{HoLuBeRo12}.
Für (4.11):
Da $\{\phi_i\}_{i=1}^\infty$ Orthonormalbasis (ONB) von $X$ gilt
\[
u(\mu) = \sum_{i=1}^{\infty} (u(\mu),\phi_i)_X\phi_i. \qquad(\text{s. Alt, Satz 7.7})
\]
Damit folgt:
\begin{align*}
\int_P \norm{u(\mu)-\sum_{i=1}^{\infty} (u(\mu),\phi_i)_X\phi_i }_X^2\dint \mu &= \int_P \norm{\sum_{i=1}^{\infty}(u(\mu),\phi_i)\phi_i - \sum_{i=1}^{\infty} (u(\mu),\phi_i)_X\phi_i }_X^2\dint \mu\\
&= \int_P \norm{\sum_{i=l+1}^{\infty} (u(\mu),\phi_i)_X\phi_i }_X^2\dint \mu\\
&= \int_P \sum_{i,j=l+1}^{\infty} (u(\mu),\phi_i)_X (u(\mu),\phi_j)_X \underbracket{(\phi_i,\phi_j)_X}_{\text{ONB, }=\delta_{ij}}\dint \mu\\
&= \int_P \sum_{i=l+1}^{\infty} (u(\mu),\phi_i)_X^2\dint \mu \\
&= \int_P \sum_{i=l+1}^{\infty} ((u(\mu),\phi_i)_X u(\mu),\phi_i)_X \dint \mu = \sum_{i=l+1}^{\infty} (R\phi_i,\phi_i)_X \\
&= \sum_{i=l+1}^{\infty} (\lambda_i\phi_i,\phi_i)_X = \sum_{i=l+1}^{\infty} \lambda_i.
\end{align*}
\hfill $\square$

\ssect{4.23 Folgerung}{}
Seien $\{\lambda_i\}_{i=1}^\infty$ und $\{\phi_i\}_{i=1}^\infty$ die Eigenwerte und Eigenfunktionen von $R$.
Dann gilt:
\begin{enumerate}[(1)]
	\item $(R\phi_i,\phi_i)_X = \int_P (u(\mu),\phi_i)_X (u(\mu),\phi_j)_X \dint\mu = \delta_{ij}\lambda_i$ für alle $i,j\in \N$, dass heißt die POD-Koeffizienten sind unkorreliert.
	\item Sei $\{X_i\}_{i=1}^\infty$ beliebige Orthonormalbasis von $X$.
	Für jedes $l\in \N$ gilt
	\[
	\sum_{i=1}^{l} \int_P \abs{(u(\mu),\phi_i)_X}^2\dint\mu \ge \sum_{i=1}^{l} \int_P \abs{(u(\mu),X_i)_X}^2\dint\mu.
	\]
	Daher erfassen die ersten $l$ POD-Basisfunktionen mehr Energie als die ersten $l$ Funktionen jeder anderen Basis von $X$.
\end{enumerate}

\bet{Beweis:}\\
\begin{enumerate}[(1)]
	\item Zunächst gilt: 
	\[
	(R\phi_i,\phi_j)_X = \enbrace{\int_P (u(\mu),\phi_i)_X u(\mu) \dint\mu,\phi_j}_X = \int_P (u(\mu),\phi_i)_X (u(\mu),\phi_j)_X \dint\mu.
	\]
	Außerdem gilt:
	\[
	(R\phi_i,\phi_j)_X = (\lambda_i\phi_i,\phi_j)_X = \lambda_i \delta_{ij}.
	\]
	\item siehe \cite{HoLuBeRo12}.
\end{enumerate}

\subsubsection{POD im Diskreten Setting}

\ssect{4.24 Satz}{(POD Basis)}
Sei $X_N$ HR von Dimension $N$ und $\{u_i^N\}_{i=1}^n\subset X_N$.
Dann definieren wir den empirischen Korrelationsoperator
\[
R_N: X_N\to X_N \text{ durch } R_N v := \frac{1}{n} \sum_{i=1}^{n} (u_i,v)_{X_N} u_i ,~ v\in X_N.
\]
Es ist $R_N\in K(X_N)$ und es existieren orthonormale Funktionen $\{\phi_i\}_{i=1}^{n'}, n'\le n$ und reelle Zahlen $\{\lambda_i\}_{i=1}^{n'}$ mit $\lambda_1\ge \lambda_2\dt{\ge} \lambda_{n'}> 0$, so dass
\[
R_N\phi_i = \lambda_i\phi_i,~i=1\dt{,} n'.
\]
Für $1\le M\le n'$ definieren wir die POD Basis $\Phi_M := \{\phi_1\dt{,}\phi_M\}$ und den POD-Raum $X_M:= \spann\{\phi_1\dt{,}\phi_M\}$.\\

\bet{Beweis:}\\
Analog zu oben zeigt man, dass $R_N$ linear, beschränkt, selbstadjungiert und nicht-negativ.
Da $R_N$ endlich dimensionales Bild hat, ist $R_N$ ferner kompakt.
Damit folgt aus dem Hilbert-Schmidt Theorem die Behauptung, wobei das Orthonormalsystem aus den $\phi_i$ nicht unendlich sein kann, da $R_N$ endliches Bild hat.

\ssect{4.25 Beispiel}{(POD-Basis zur Modellreduktion parameter abhängiger PDgl)}
Betrachte dazu den HR $X_h$, die Snapshotmenge $M_{\train} = \penbrace{ u_h(\mu_i)~|~ \mu_i\in S_{\train}}$ und\\ $R_hv = \frac{1}{n_{\train}} \sum_{i=1}^{n_{\train}} (u_h(\mu_i),v)_{X} u_h(\mu_i)$. 
Die POD-Basis wird dann über das entsprechende Eigenwertproblem wie in Satz 4.24 bestimmt.

\ssect{4.26 Beispiel}{(POD in der statistischen Datenanalyse)}
Die Projektion auf den POD-Raum wird in der statistischen Datenanalyse auch Hotelling-Transformation, Principal Component Analysis (PCA) oder Karhunen-Lo\`{e}ve Transformation genannt. Hierbei betrachtet man einen Datensatz,welcher als Menge von $n$ Punkten in $N$-dimensionalen Raum veranschaulicht werden kann, dass heißt $X_N=\R^N$ und
\[
R_Nv = \frac{1}{n} \sum_{j=1}^n (u_j^N,v)_{\R^N} u_j^N \text{ für } v\in \R^N.
\]

\ssect{4.27 Bemerkung}{(Eigenschaften der POD-Basis)}
\begin{enumerate}[(1)]
	\item $\{\phi_i\}_{i=1}^N$ ist orthonormale Basis, aber nicht eindeutig.
	\item Die POD-Basen sind  hierachisch, dass heißt $X_{N'}\subset X_N$ für $N'\le N$.
	\item Die POD hängt nicht von der Reihenfolge der Daten ab (im Gegensatz zu einer Basis welche aus einer Gram-Schmidt Orthogonalisierung hervorgeht).
	\item Maximierung der Varianz: $\phi_1$ ist die Richtung höchster Varianz von $\{u_i^N\}_{i=1}^n$, $\phi_2$ ist die Richtung höchster Varianz von $\{ P_{X_1^c} u_i^N\}_{i=1}^n$, usw.
	\item Die Koordinaten der Daten bzgl. der POD-Basis sind unkorreliert.
	Wir wollen nun exemplarisch für $X_N=\R^N$ die Optimalität der POD-Basis nachweisen, dass heißt wir wollen zeigen, dass die POD-Basis das folgende nichtlineare Optimierungsproblem löst:
	\begin{align}
	\min\limits_{\psi_1\dt{,}\psi_l\in\R^N} \frac{1}{n} \sum_{j=1}^n \norm{u_j^N - \sum_{i=1}^l (u_j^N,\psi_i)_{\R^N}\psi_i}_{\R^N}^2,
	\end{align}
	so dass $(\psi_i,\psi_j)_{\R^N} = \delta_{ij}$ für $1\le i,j\le l$.
	Dazu benötigen wir einige Definitionen und Resultate aus der nichtlinearen Optimierung.
	Problem (4.12) ist ein Optimierungsproblem der folgenden allgemeinen Form:
	\[
	\min J(x) \text{, so dass } g(x) = 0, \tag*{(Op)}
	\]
	wobei $J:\R^N\to \R$ das Zielfunktional und $g:\R^k\to \R^m,~m\le k$ der Nebenbedingung.
\end{enumerate}

\ssect{4.28 Definition}{(zulässige Lösung)}
Ein Punkt $x\in\R^n$ heißt zulässig, falls $g(x)=0$ gilt.
Die Menge zulässiger Lösungen ist definiert als
\[
E(Op) = \penbrace{x\in \R^n~|~g(x)=0}.
\]

\ssect{4.29 Definition}{(lokale Lösung)}
Der Punkt $\bar{x}$ heißt \bet{lokale Lösung} von (Op), falls $\bar{x}\in E(Op)$ gilt und $J(\bar{x})\le J(x) ~\forall U(\bar{x})\cap E(Op)$, wobei $U(\bar{x})\subset \R^N$ offene nichtleere Umgebung von $\bar{x}$.

\ssect{4.30 Definition}{(regulärer Punkt)}
Ein Punkt $\bar{x}\in E(Op)$ heißt \bet{regulärer Punkt} bzgl. der Nebenbedingung $g(x)=0$, falls die Gradienten $\{\nabla g(\bar{x})\}_{i=1}^n\subset \R^N$ linear unabhängig sind.

\ssect{4.31 Satz}{(Notwendige Optimalitätsbedingung 1. Ordnung)}
Seien $J$ und $g$ stetig differenzierbar.
Sei ferner $\bar{x}$ eine lokale Lösung von (Op) und ein regulärer Punkt für $g(x)=0$.
Dann existiert ein eindeutiger Lagrange-Multiplikator $\bar{\lambda}=(\lambda_1\dt{,}\lambda_m)\in \R^m$, welcher
\begin{align}
\nabla J(\bar{x}) + \sum_{i=1}^m \lambda_i \nabla g(\bar{x}) = \nabla J(\bar{x}) + \nabla g(\bar{x})^T \bar{\lambda} =0 
\end{align}
löst.\\

\bet{Beweis:} z.B. \cite{NocWri06}.
\hfill $\square$\\

Nun haben wir alle Hilfsmittel zusammen um den folgenden Satz zu zeigen:

\ssect{4.32 Satz}{}
Sei $U = [u_1^N\dt{,}u_n^N]\in \R^N$ und $\Phi_N=\{\phi_1\dt{,}\phi_N\}$ POD-Basis definiert in 4.24.
Dann löst die POD-Basis das folgende Optimierungsproblem:
\begin{align*}
\max\limits_{\psi_1\dt{,}\psi_l\in \R^N} \frac{1}{n} \sum_{i=1}^l \sum_{j=1}^n \abs{(u_j^N,\psi_i)_{\R^N}}^2 \text{ s.d. } (\psi_i,\psi_j)_{\R^N} = \delta_{ij} ~\forall 1\le i,j\le l. \tag*{$(Op^l)$}
\end{align*}
Ferner gilt 
\[
\argmax (Op^l) = \sum_{i=1}^l \lambda_i,
\]
wobei $\lambda_i$ die Eigenwerte aus Satz 4.24 sind.

\bet{Beweis:}\\
Zunächst betrachten wir das Optimierungsproblem für $l=1$
\begin{align*}
\max\limits_{\psi\in \R^N} \frac{1}{n} \sum_{j=1}^n \abs{(u_j^N,\psi)_{\R^N}}^2 \text{ s.d. } \norm{\psi}^2_{\R^N} = 1. \tag*{$(Op^1)$}
\end{align*}
Um die Lösung von $(Op^1)$ zu bestimmen, betrachten wir die notwendige Optimalitätsbedingung 1. Ordnung (nOB1).
Dazu führen wir zunächst die Funktion $g:\R^N \to \R$ ein, welche durch $g(\psi) = 1- \norm{\psi}^2_{\R^N}$ für $\psi\in \R^N$ definiert ist.
Die Nebenbedingung in $(Op^1)$ kann dann als $g(\Psi)=0$ formuliert werden:
\[
\nabla g(\psi) = 2 \psi^T \text{ ist linear unabhängig, falls } \psi \neq 0.
\]
Aufgrund der Nebenbedingung $\norm{\psi}_{\R^N}^2=1$ gilt für eine Lösung von $(Op^1)$, dass $\psi\neq 0$.
Damit ist jede Lösung von $(Op^1)$ ein regulärer Punkt.
Sei $\mathcal{L}:\R^N\times \R \to \R$ das Lagrange-Funktional zu $(Op^1)$, das heißt
\[
\mathcal{L}(\psi,\lambda) := \frac{1}{n} \sum_{j=1}^n \abs{(u_j^N,\psi)_{\R^N}}^2 + \lambda (1-\norm{\psi}_{\R^N}^2),
\]
für $(\psi,\lambda)\in\R^N\times\R$.
Angenommen $\psi\in\R^N$ ist eine Lösung von $(Op^1)$.
Da $\psi$ regulärer Punkt existiert nach Satz 4.31 ein eindeutiger Lagrange-Multiplikator $\lambda\in\R$, welcher die nOB1 erfüllt:
\[
\nabla \mathcal{L}(\psi,\lambda) \bgl{!} 0 \text{ in } \R^N\times \R.
\]
Wir berechnen den Gradienten von $\mathcal{L}$ bezüglich $\psi$:
\begin{align*}
\diff{\mathcal{L}}{\psi_i} (\psi,\lambda) &= \diff{}{\psi_i} \enbrace{ \frac{1}{n} \sum_{j=1}^n \abs{\sum_{k=1}^N (u_{kj}\psi_k)}^2 + \lambda \enbrace{1- \sum_{k=1}^N \psi_k^2 } }\\
&= 2 \cdot \frac{1}{n} \sum_{j=1}^n \enbrace{\sum_{k=1}^N u_{kj}\psi_k}u_{ij} - 2\lambda\psi_i\\
&= \frac{2}{n} \sum_{n=1}^N \enbrace{ \underbracket{\sum_{j=1}^n u_{ij}u_{jk}^T\psi_k}_{=(uu^T)_{ik}}} - 2 \lambda\psi_i.
\end{align*}
Daher folgt:
\begin{align}
\nabla_\psi \mathcal{L}(\psi,\lambda) = 2 \enbrace{\frac{1}{n} uu^T\psi - \lambda \psi} = 2 (R_N\psi-\lambda\psi) \bgl{!} 0 \text{ in } \R^N\times \R.
\end{align}
Gleichung (4.14) ergibt das folgende Eigenwertproblem
\begin{align}
\frac{1}{n} uu^T\psi = \lambda \psi \text{ in } \R^N, 
\end{align}
welches gerade mit dem Eigenwertproblem aus Satz 4.24 übereinstimmt.
Aus $\diff{\mathcal{L}}{\lambda} (\mathcal{L},\lambda) \bgl{!} 0$ in $\R$ schließen wir die Nebenbedingung
\begin{align*}
\norm{\psi}=1. \tag*{(4.15b)}
\end{align*}
Der erste POD-Vektor $\phi$, definiert in Satz 4.24 löst damit (4.15).
Ferner gilt:
\begin{align*}
\frac{1}{n} \sum_{j=1}^n \abs{ (u_j^N,\phi_1)_{\R^N}}^2 &= \enbrace{\frac{1}{n} \sum_{j=1}^n (u_j^N,\phi_1)_{\R^N} u_j^N,\phi_1}_{\R^N}\\
&= (R_N\phi_1,\phi_1) = \lambda_1 \norm{\phi_1}^2_{\R^N} = \lambda_1.
\end{align*}
Schließlich zeigen wir, dass $\phi_1$ $(Op^1)$ löst.
Sei dazu $\xi\in R^N$ ein beliebiger Vektor mit $\norm{\xi}_{\R^N}=1$.
Da die Eigenvektoren $\{\phi_i\}_{i=1}^N$ von (4.15) eine ONB für $\R^N$ sind, gilt
\[
\xi = \sum_{i=1}^{N} (\xi,\phi_i)_{\R^N}\phi_i.
\]
Es folgt:
\begin{align*}
\frac{1}{n} \sum_{j=1}^n \abs{(u_j^N,\xi)_{\R^N}}^2 &= \frac{1}{n} \sum_{j=1}^n \abs{\enbrace{u_j^N, \sum_{i=1}^N(\xi,\phi_i)_{\R^N}\phi_i}_{\R^N}}^2\\
&= \frac{1}{n} \sum_{j=1}^n \sum_{i,k=1}^N (u_j^N,(\xi,\phi_i)\phi_i)(u_j^N,(\xi,\phi_k)\phi_k)\\
&= \frac{1}{n} \sum_{j=1}^n \sum_{i,k=1}^N \bigg( (u_j^N,\phi_i)(u_j^N,\phi_k)(\xi,\phi_i)(\xi,\phi_k)\bigg)\\
&= \sum_{i,k=1}^N \enbrace{ \enbrace{ \underbracket{\frac{1}{n} \sum_{j=1}^n (u_j^N,\phi_i)u_j^N}_{=R_N\phi_i},\phi_k}(\xi,\phi_i)(\xi,\phi_k)}\\
&= \sum_{i,k=1}^N  (\underbracket{\lambda_i\phi_i,\phi_k}_{=\lambda_i\delta_{ik}})(\xi,\phi_k)(\xi,\phi_k)\\
&= \sum_{i=1}^N \lambda_i \abs{(\xi,\phi_i)}^2\\
&\le \lambda_1 \sum_{i=1}^N \abs{(\xi,\phi_i)}^2 = \lambda_1 \norm{\xi}^2 =\lambda_1\\
&= \frac{1}{n} \sum_{j=1}^n \abs{(u_j^N,\phi_1)_{\R^N}}^2.
\end{align*}
Damit löst $\phi_1~(Op^1)$ und $\argmax (Op^1) = \lambda$.

Suchen wir nach einem zweiten Vektor, welcher orthogonal auf $\phi_1$ steht, normiert ist und die Daten $\{u_i^N\}_{i=1}^n$ so gut wie möglich beschreibt, so müssen wir das folgende Optimierungsproblem lösen:
\[
\max\limits_{\psi\in\R^N} \frac{1}{n} \sum_{j=1}^n \abs{(u_j^N,\psi)}^2, \text{ so dass } \norm{\psi}=1 \text{ und } (\psi,\phi_1)= 0,
\]
wobei $\norm{.} = \norm{.}_{\R^N}$ und $(.,.)=(.,.)_{\R^N}$.
Die Definition der POD-Basis zeigt, dass $\phi_2$ eine Lösung von $(Op^2)$ ist und $\argmax (Op^2)=\lambda_2$ gilt.
Im Detail löst $\phi_2$ die nOB1 (4.15) und für 
\[
\tilde{\psi}\sum_{i=2}^N (\tilde{\psi},\phi_i)\phi_i \in \spann\{\phi_1\}^\perp
\]
gilt:
\[
\frac{1}{n} \sum_{j=1}^n \abs{(u_j^N,\tilde{\psi})}^2 \le \lambda_2 = \frac{1}{n} \sum_{j=1}^n \abs{(u_J^N,\phi_2)}^2.
\]
Schließlich zeigen wir mittels vollständiger Induktion, dass die in 4.24 definierte POD-Basis das Optimierungsproblem $(Op^l)$ löst.
Für allgemeine $l$ betrachten wir dazu das Lagrange-Funktional: $\mathcal{L}:\underbracket{\R^N\dt{\times}\R^N}_{l-\text{mal}}\times \R^l\times \R^l \to \R$ definiert durch
\[
\mathcal{L}(\psi_1\dt{,}\psi_l,\Lambda) = \frac{1}{n} \sum_{i=1}^l\sum_{j=1}^n \abs{(u_j^N,\psi_i)}^2 + \sum_{i,j=1}^{l} \lambda_{ij}(\delta_{ij} -  (\psi_i,\psi_j)),
\]
für $\psi_1\dt{,}\psi_l\in \R^N$ und $\Lambda=((\lambda_{ij}))\in \R^{l\times l}$.
Die nOB1 für $(Op^l)$ lauten dann
\begin{align}
\diff{\mathcal{L}}{\psi_k}(\psi_1\dt{,}\psi_l,\Lambda)\delta\psi_k = 0 ~\forall \delta\psi_k\in\R^N,~k\in \{1\dt{,}l\}.
\end{align}
Aus 
\begin{align*}
\diff{\mathcal{L}}{\psi_k}(\psi_1\dt{,}\psi_l,\Lambda)\delta\psi_k &= 2\cdot\frac{1}{n} \sum_{j=1}^n (u_j^N,\psi_k)(u_j^N,\delta\psi_k) - \sum_{i=1}^l \delta_{ik}(\psi_i,\psi_k) - \sum_{i=1}^l \lambda_{ki} (\delta\psi_k,\psi_i)\\
&= 2\cdot\frac{1}{n} \sum_{j=1}^n (u_j^N,\psi_k)(u_j^N,\delta\psi_k) - \sum_{i=1}^l (\lambda_{ik}+\lambda_{ki})(\psi_i,\delta\psi_k)\\
&= \enbrace{2\cdot\frac{1}{n} \sum_{j=1}^n (u_j^N,\psi_k)u_j^N - \sum_{i=1}^l (\lambda_{ik}+\lambda_{ki})\psi_i,\delta\psi_k}
\end{align*}
und (4.16) schließen wir
\begin{align}
\frac{1}{n} \sum_{j=1}^n (u_j^N,\psi_k)u_j^N = \frac{1}{2} \sum_{i=1}^l (\lambda_{ik}+\lambda_{ki})\psi_i \text{ in } \R^N ~\forall k\in\{1\dt{,}l\},
\end{align}
oder bzw.
\begin{align}
\frac{1}{n} UU^T \psi_k = \frac{1}{2} \sum_{i=1}^l (\lambda_{ik}+\lambda_{ki})\psi_i \text{ in } \R^N \forall k\in\{1\dt{,}l\}.
\end{align}
Wir zeigen nun mittels vollständiger Induktion, dass das Eigenwert-Problem aus 4.24 gerade den nOB1 für $(Op^l)$ entspricht.
Für $l=1$ gilt: $k=1$. 
Dann folgt aus (4.17):
\[
\frac{1}{n} \sum_{j=1}^n (u_j^N,\psi_1) u_j^N = \lambda_1\psi_1 \text{ in } \R^N \text{ mit } \lambda_1 =\lambda_{11}.
\]
Wir nehmen nun an, dass für $l\ge 1$ die nOB1 durch
\begin{align}
\frac{1}{n} \sum_{j=1}^n (u_j^N,\psi_k) u_j^N = \lambda_k\psi_k \text{ in } \R^N ~\forall k\in\{1\dt{,}l\}
\end{align}
gegeben sind.
Wir wollen nun zeigen, dass die nOB1 für eine POD Basis f\"{u}r $l+1$ durch
\begin{align}
\frac{1}{n} \sum_{j=1}^n (u_j^N,\psi_k) u_j^N = \lambda_k\psi_k \text{ in } \R^N ~\forall k\in\{1\dt{,}l+1\}
\end{align}
gegeben sind.
Aus (4.17) schließen wir
\begin{align}
\frac{1}{n} \sum_{j=1}^n (u_j^N,\psi_{l+1}) u_j^N = \frac{1}{2} \sum_{i=1}^{l+1} (\lambda_{i,l+1}+\lambda_{l+1,i})\psi_i \text{ in } \R^N.
\end{align}
Da $\{\psi_i\}_{i=1}^{l+1}$ POD-Basis (orthonormal als Lösung von (4.19)) und da $R_N$ selbstadjungiert folgt
\begin{align*}
0 &= \lambda_j(\psi_{l+1},\psi_j) = (\psi_{l+1},R_N\psi_j) = (R_N\psi_{l+1},\psi_j)\\
&= \frac{1}{2} \sum_{i=1}^{l+1} (\lambda_{i,l+1}+\lambda_{l+1,i}) (\psi_i,\psi_j) = \frac{1}{2} (\lambda_{j,l+1}+\lambda_{l+1,j})
\end{align*}
f\"{u}r $j=1,...,l$. Damit folgt
\begin{align}
\lambda_{i,l+1} = \lambda_{l+1,i} \text{ für jedes } i\in\penbrace{1\dt{,}l}.
\end{align}
Einsetzen von (4.22) in (4.21) ergibt
\begin{align*}
\frac{1}{n} \sum_{j=1}^n (u_j^N,\psi_{l+1}) u_j^N &= \frac{1}{2} \sum_{i=1}^{l} \underbracket{(\lambda_{i,l+1}+\lambda_{l+1,i})}_{=0}\psi_i + \lambda_{l+1,l+1}\psi_{l+1}\\
&= \lambda_{l+1,l+1}\psi_{l+1}.
\end{align*}
Mit $\lambda_{l+1}=\lambda_{l+1,l+1}$ folgt dann (4.20).
Zusammengefasst sind die nOB1 für $(Op^l)$ durch das Eigenwert-Problem
\begin{align}
R_N\psi_i = \lambda_i\psi_i \text{ für } i=1\dt{,}l
\end{align}
gegeben.
Die in 4.24 definierte POD-Basis $\Phi_l=\penbrace{\phi_1\dt{,}\phi_l}$ löst (4.23).
Der Beweis das $\{\phi_i\}_{i=1}^l$ eine Lösung von $(Op^l)$ ist und das $\argmax (Op^l)=\sum_{i=1}^l \lambda_i$ mit den Eigenwerten aus Satz 4.24 gilt, folgt analog zum Beweis für $(Op^1)$.
\hfill $\square$

\ssect{4.33 Folgerung}{(Optimalität der OPD-Basis)}
Seien die Voraussetzungen von Satz 4.32 erfüllt und gilt $\rg(U)=d$, wobei $U=[u_1^N\dt{,}u_n^N]$.
Sei $\hat{\Psi}_d=[\psi_1\dt{,}\psi_d]\in \R^{N,d}$ Matrix mit paarweise orthonormalen Spalten und die Darstellung der Spalten von $U$ in der Basis $\{\psi_i\}_{i=1}^d$ sei gegeben durch
\[
U=\hat{\Psi}_d\cdot C_d,\text{ wobei } (C_d)_{ij} := (\psi_i,u_j^N)~1\le i\le d,~1\le j\le n.
\]
Dann gilt für jedes $l\in \{1\dt{,}d\}$:
\begin{align}
\norm{U-\hat{\Phi}_lB_l}_F \le \norm{U-\hat{Psi}_lC_l}_F,
\end{align}
wobei $\hat{\Phi}_l =[\phi_1\dt{,}\phi_d]$ mit $\phi_i$ POD-Basis aus 4.24 und $(B_l)_{ij}=(\phi_i,u_j^N)$.
Hierbei bezeichnet $\norm{.}_F$ in (4.24) die \bet{Frobeniusnorm}, welche durch
\[
\norm{A}_F = \sqrt{\sum_{i=1}^{N}\sum_{j=1}^{n}\abs{A_ij}^2}= \sqrt{\spur(A^TA)} \text{ für } A\in \R^{N\times n}
\]
gegeben ist.
Ferner bezeichnet die Matrix $\hat{\Psi}_d$ die $l\le d$ ersten Spalten von $\hat{\Psi}$, $B_l$ die ersten $l$-Zeilen von $B$ und analog für $\hat{\Phi}_l$ und $C_l$.\\

\bet{Beweis:}\\
Da die Spalten von $\hat{\Psi}_d$ orthonormal sind, folgt 
\begin{align*}
\norm{u-\hat{\Psi}_lC_l}_F^2 &= \norm{\hat{\Psi}_d(C_d-C_l^0)}_F^2\\
&\bgl{\Psi^T\Psi=\id} \norm{C_d-C_l^0}_F^2\\
&= \sum_{i=l+1}^d \sum_{j=1}^n \abs{(C_l)_{ij}}^2,
\end{align*}
wobei $C_l^0\in \R^{d\times n}$ aus $C\in \R^{d\times n}$ durch Ersetzen der letzten $d-l$ Reihen durch $0$ hervor geht.
Da wegen
\begin{align*}
\frac{1}{n} uu^Tv &= R_Nv = R_N \enbrace{\sum_{i=1}^{N}(v,\phi_i)\phi_i}\\
&= \sum_{i=1}^{N} (v,\phi_i)R_N\phi_i = \sum_{i=1}^N \lambda_i(v,\phi_i)\phi_i\\
&= \sum_{i=1}^d \lambda_i(v,\phi_i)\phi_i,
\end{align*}
folgt, dass $u_j^N\in \bild(R_N)=\spann \{\phi_i\}_{i=1}^d$.
Daher gilt $U=\hat{\Phi}_dB_d$ und damit
\begin{align}
\begin{split}
\frac{1}{n} \norm{U-\hat{\Phi}_lB_l}_F^2 &= \frac{1}{n} \norm{\hat{\Phi}_D(B_d-B_L^0)}_F^2 = \norm{B_d-B_L^0}_F^2\\
&= \frac{1}{n} \sum_{i=l+1}^d \sum_{j=1}^n \abs{(B_d)_{ij}}^2 = \frac{1}{n} \sum_{i=l+1}^d \sum_{j=1}^n \abs{(u_j^N,\phi_i)}^2\\
&= \frac{1}{n} \sum_{i=l+1}^d \sum_{j=1}^n \bigg((u_j^N,\phi_i)u_j^N,\phi_i\bigg)\\
&= \sum_{i=l+1}^d (R_N\phi_i,\phi_i) = \sum_{i=l+1}^d \lambda_i.
\end{split}
\end{align}
Damit gilt:
\[
\frac{1}{n} \norm{U}_F^2 = \frac{1}{n} \norm{\hat{\Psi}_dC_d}_F^2 = \frac{1}{n} \norm{C_d}_F^2 = \frac{1}{n} \sum_{i=1}^d \sum_{j=1}^n \abs{(C_d)_{ij}}^2
\]
und
\[
\frac{1}{n} \norm{U}_F^2 = \frac{1}{n} \norm{\hat{\Phi}_dB_d}_F^2 = \frac{1}{n} \norm{B_d}_F^2 = \frac{1}{n} \sum_{i=1}^d \sum_{j=1}^n \abs{(B_d)_{ij}}^2 = \sum_{i=1}^d \lambda_i.
\]
Da die Vektoren $\phi_1\dt{,}\phi_l$ aus Satz 4.32 des Optimierungsproblems $(Op^l)$ lösen, folgt:
\begin{align*}
\frac{1}{n} \norm{U - \hat{\Phi}_lB_l}_F^2 &= \sum_{i=l+1}^{d} \lambda_i = \sum_{i=1}^{d} \lambda_i -\sum_{i=1}^{l} \lambda_i \bgl{(4.13)} \frac{1}{n} \norm{U}_F^2 - \frac{1}{n} \sum_{i=1}^{l}\sum_{j=1}^n \abs{(u_j^N,\phi_i)}^2\\
&\stackrel{\phi_i \text{ lösen }(Op^l)}{\le} \frac{1}{n} \norm{U}_F^2 - \frac{1}{n} \sum_{i=1}^{l}\sum_{j=1}^n \abs{(u_j^N,\psi_i)}^2\\
&= \frac{1}{n} \sum_{i=1}^{d}\sum_{j=1}^n \abs{(C_d)_{ij}}^2 -  \frac{1}{n} \sum_{i=1}^{l}\sum_{j=1}^n \abs{(C_d)_{ij}}^2\\
&= \frac{1}{n} \sum_{i=l+1}^{l}\sum_{j=1}^n \abs{(C_d)_{ij}}^2 = \frac{1}{n} \norm{U-\hat{\Psi}_lC_l}_F^2.\\
\end{align*}
\hfill $\square$

\ssect{4.34 Bemerkung}{}
Folgerung 4.33 liefert direkt, dass
\[
\frac{1}{n} \sum_{j=1}^n \norm{u_j^N - \sum_{k=1}^{l} (u_j^N,\phi_k)\phi_k }^2 \le \frac{1}{n} \sum_{j=1}^n \norm{u_j^N - \sum_{k=1}^{l} (u_j^N,\psi_k)\psi_k }^2
\]
für jede andere Menge $\{\psi_i\}_{i=1}^l$ paarweise orthonormaler Vektoren.
Damit folgt aus Folgerung 4.33, dass die in 4.24 definierte POD-Basis auch das folgende Optimierungsproblem löst:
\[
\min\limits_{\psi_1\dt{,}\psi_l\in\R^N} \frac{1}{n} \sum_{j=1}^n \norm{u_j^N - \sum_{k=1}^{l} (u_j^N,\psi_k)\psi_k }^2, \text{ so dass } (\psi_i,\psi_j) = \delta_{ij} \text{ für } 1\le i,j \le l.
\]
Die nOB1 sind identisch mit denen von $(Op^l)$.

\ssect{4.35 Satz}{(Berechnung der POD-Basis über die Gram-Matrix)}
Sei $X_N$ HR von Dimension $N$ und $\{u_i^N\}_{i=1}^n\subset X_N$.
Die Matrix $\mathbb{K}\in \R^{n\times n}$ definiert durch
\begin{align}
\mathbb{K}_{ij} := (u_i^N,u_j^N)
\end{align}
heißt \bet{Gram-Matrix}.
Dann sind äquivalent:
\begin{enumerate}[(1)]
	\item $\phi\in X_N$ ist Eigenvektor zu $R_N$ aus Satz 4.24 zum Eigenwert $\lambda>0$ mit Norm1 und einer Darstellung
	\[
	\phi = \sum_{i=1}^n a_i u_i^N \text{ mit } a\in \ker (\mathbb{K})^{\perp}.
	\]
	\item $a=(a_1\dt{,}a_n)\in \R^n$ ist Eigenvektor von $\frac{1}{n} \mathbb{K}$ zu $\lambda>0$, mit Norm $\frac{1}{\sqrt{n\lambda}}$.
\end{enumerate}

\bet{Beweis:} s. \cite{Haa11}, Satz 5.7, S.53f.
\hfill $\square$

\ssect{4.36 Bemerkung}{}
Die POD kann daher entweder als teures EW-Problem für$R_N$ in $X_N$ (Komplexität $\mathcal{O}(N^3)$) oder, meist günstiger, als EW-Problem für $\mathbb{K}$ (Komplexität $\mathcal{O}(n^3)$) ermittelt werden.
Letzteres wir auch \bet{'Method of snapshots'} genannt.

\ssect{4.37 Lemma}{(Berechnung der POD-Basis mittels der Singulärwertzerlegung für $X_N=\R^N$)}
Sei $X_N0\R^N, [u_1^N\dt{,}u_n^N] =U \in \R^{N\times n}$ Snapshot-Matrix mit Rang $d$ und $U=\Psi DV^T$ eine Singulärwertzerlegung mit $\Psi\in \R^{N\times d}$ mit orthonormalen Spalten, $D=\diag(\sigma_1\dt{,}\sigma_d)\in \R^{d\times d}$ und $V\in \R^{n\times d}$ mit orthonormalen Spalten.
Falls $\sigma_1>\sigma_2\dt{>}0$ echt fallend, so ist $\Psi=\Phi$ bis auf Vorzeichen.\\

\bet{Beweis:}\\
Sei $\Psi=[\psi_1\dt{,}\psi_d]$, $\psi_i$ Eigenvektor von $R_N$ wegen
\[
R_N\psi_i = \frac{1}{n} UU^T \psi_i = \frac{1}{n} \Psi D \underbracket{V^TV}_{=\id}D \underbracket{\Psi^T\psi_i}_{e_i} = \frac{1}{n} \Psi DD e_i = \frac{1}{n} \sigma_i^2\psi_i.
\]
Die Eigenwerte $\frac{1}{n}\sigma_i^2$ sind monoton fallend, also identisch sortiert wie Spektralzerlegung von $R_N$, das heißt $\frac{1}{n} \sigma_i^2 = \lambda_i$ und $\psi_i=\phi_i$ oder $\psi_i=-\phi_i$.


\section{Approximationstheorie}


Im letzten Kapitel haben wir gesehen, dass der Greedy-Algorithmus quasi-optimale rB-räume erzeugt, indem Sinne, dass die konstruierten Räume entweder zur gleichen oder einer leicht schlechteren Konvergenzrate, wie die optimalen Räume im Sinne von Kolmogorov, führen.
Ferner konnten wir zeigen, dass die POD-Räume den Projektionsfehler minimieren und in diesemm Sinne optimal sind.

Es ist zu beachten, dass wir aus der (Quasi-)Optimalität der RB-Räume \uline{nicht} schließen können, dass die RB-Räume tatsächlich zu einer schnellen Konvergenz der RB-Approximation führen.
Tatsächlich kann die Konvergenzrate der optimalen Räume unter gewissen Umständen sehr schlecht sein.
\marginnote{Zum Beispiel, wenn der Rand parametrisiert wird.}
In diesem Kapitel wollen wir der Frage nach gehen unter welchen Umständen wir eine schnelle und idealerweise eine exponentielle Konvergenz der RB-Räume erwarten können.

\ssect{5.1 Bemerkung}{(Von welchen Faktoren hängt die Approximationsgüte der RB-Approximation ab?)}
Unter anderem beeinflussen die folgenden Faktoren die Approximationsgüte des RB-Raums:
\begin{enumerate}[(1)]
	\item \bet{Differenzierbarkeit und Regularität} der Lösungsabbildung $\gamma: P\to X,~\mu\mapsto u(\mu)$ bzgl. der Parameter.
	Von besonderer Bedeutung ist der Fall,wenn $\gamma$ analytisch ist.
	\item \bet{Parametrische Komplexität}, welche hier durch $Q_b$ und $Q_f$, also die Anzahl der Summanden in der affin parametrischen Darstellung von $b$ unf $f$ charakterisiert ist.
	Die parametrische Komplexität ist insbesondere dann kritisch, wenn $Q_b,Q_f$ sehr groß oder gar unendlich sind.
	Im letzterem Fall kann eine Approximation mit endlich vielen Summanden bestimmt werden (s. Kapitel 6). 
	Für lineare, elliptische Probleme mit einer regulären Lösungsabbildung führt ein schneller Abfall der $\mu$-abhängigen Koeffizienten $\theta_b(\mu)$ und $\theta_f(\mu)$ oder die Existenz einiger weniger dominanter Terme zu einer schnellen Konvergenz der RB-Approximation.
	\item \bet{Art des betrachteten Problems:} Für $P=1$für nicht lineare Probleme wie die Navier-Stokes Gleichungen kann es schwierig sein schnell konvergierende RB-Räume zu konstruieren, während dies für elliptische Probleme für moderates $P$ im Allgemeinen gut möglich ist.
	Probleme mit hochdimensionalen Parameterräumen stellen dagegen selbst für elliptische PDgl's eine Herausforderung dar.
	Schließlich wird im Allgemeinen die Approximationsgüte von RB-Räumen deutlich schlechter, wenn man von elliptischen oder parabolischen zu rein hyperbolischen Problemen übergeht. 
\end{enumerate}

\ssect{Satz 5.2}{(Globale Exponentielle Konvergenz im Spezialfall \cite{MaPaTu02})}
Seien $P=[\mu_{\min}\dt{,}\mu_{\max}] \subset \R^+$ mit $\mu_{\min} = \frac{1}{\sqrt{\mu_r}},\mu_{\max}= \sqrt{\mu_r},\mu_r\in\R^+$ mit $1\le \mu_r < \infty$,
\[
b(u,v;\mu) = \mu b^1(u,v) + b^2(u,v) \text{ mit } b^q(u,v) = \int_{\Omega} \nabla u\nabla v ~\forall u,v\in X,~ 1\le q\le 2
\]
und $f$ nicht parametrisch mit $\ln \mu_r > \frac{1}{2e}$ und $N_{krit} := 1+ \lceil 2e \ln \mu_r \rceil$.
Zu $N\in\N, N\ge 2$, seien $\mu_{\min} = \mu_1 \dt{<} \mu_N=\mu_{\max}$ gegeben durch
\[
\mu_i := e^{\{\ln \mu_{\min} + (i-1)\frac{\ln \mu_r}{N-1}\}}
\]
und sind daher logarithmisch äquidistant, dass heißt
\[
\ln(\mu_{i+1}-\mu_i) = \frac{\ln(\mu_{\max})-\ln(\mu_{\min})}{N-1}.
\]
Schließlich sei $X_N := \spann\penbrace{ u_h(\mu_i)}_{i=1}^N$ der zugehörige Lagrange RB-Raum.
Dann gilt:
\[
\frac{\ener{u_h(\mu)-u_N(\mu)}}{\ener{u_h(\mu)}} \le e^{\frac{N-1}{N_{krit}-1}} ~\forall \mu\in P,~N\ge N_{krit}.
\]

\bet{Beweis:} s. \cite{PatRoz2006}.
\hfill $\square$

\subsection{Reguläre Lösungsabbildungen}

\ssect{5.3 Satz}{(Lipschitz-Stetigkeit)}
Seien die Bilinearform $b(.,.;\mu)$ und die Linearform $f(.;\mu)$ Lipschitz-stetig bzgl. $\mu$ und $b$ glm. koerziv bzgl. $\mu$.
Außerdem seien $b$ und $f$ glm. beschränkt bzgl. $\mu$ (s. Def 3.3).
Dann ist die Lösung $u(\mu)$ von $P(\mu)$ definiert in Definition 3.5 Lipschitz-stetig bzgl. $\mu$, dass heißt es existiert ein $L_u>0$, so dass:
\begin{align}
\norm{u(\mu_1)-u(\mu_2)}_X \le L_u \norm{\mu_1-\mu_2}_2~\forall \mu_1,\mu_2\in P.
\end{align}

\bet{Beweis:}\\
Nach Definition 3.5 lösen $u(\mu_1)$ und $u(\mu_2)$
\[
b(u(\mu_1),v;\mu_1) = f(v;\mu_1)~\forall v\in X; b(u(\mu_2),v;\mu_2) = f(v;\mu_2)~\forall v\in X.
\]
Subtraktion der beiden Gleichungen und Addition von Null ergibt:
\[
b(u(\mu_1),v;\mu_1) - b(u(\mu_2),v;\mu_1) + b(u(\mu_2),v;\mu_1) -b(u(\mu_2),v;\mu_2) = f(v;\mu_1) -f(v;\mu_2).
\]
Wegen der Lipschitz-Stetigkeit von $b$ und $f$ erhalten wir:
\begin{align*}
b(u(\mu_1)-u(\mu_2),v;\mu_1) &= f(v;\mu_1) -f(v;\mu_2) - b(u(\mu_2),v;\mu_1) +  b(u(\mu_2),v;\mu_2)\\
&\le L_f \norm{v}_X \norm{\mu_1-\mu_2}_2 +L_b \norm{u(\mu_2)}_X \norm{v}_X \norm{\mu_1-\mu_2}_2.
\end{align*}
Teste mit $v = u(\mu_1)-u(\mu_2)$ ergibt:
\[
\alpha(\mu_1) \norm{u(\mu_1)-u(\mu_2)}_X \le L_f \norm{\mu_1-\mu_2}_2 +L_b \norm{u(\mu_2)}_X \norm{\mu_1-\mu_2}_2.
\]
Das Ausnutzen der A priori Abschätzung $\norm{u(\mu_2)}_X \le \frac{\norm{f(,;\mu_2)}_X}{\alpha(\mu_2)}$ ergibt schließlich:
\[
\norm{u(\mu_1)-u(\mu_2)}_X \le \enbrace{\frac{l_f}{\alpha(\mu_1)} + \frac{L_b \norm{f(.;\mu_2)}_{X'}}{\alpha(\mu_1)\alpha(\mu_2)}} \norm{\mu_1-\mu_2}_2.
\]
Wegen der glm. Beschränktheit und glm. Koerzivität folgt mit $L_u = \frac{L_f}{\alpha_0} + \frac{L_b \gamma_0}{\alpha_0^2}$ die Behauptung.
\hfill $\square$

\ssect{5.4 Bemerkung}{}
Die Aussage aus Satz 5.3 gilt analog für $u_h(\mu)$ als Lösung von $(P_h(\mu))$ und wie bereits in Satz 3.25 bemerkt für $u_N(\mu)$ als Lösung von $(P_N(\mu))$.

\ssect{5.5 Bemerkung}{}
Sind $b$ und $f$ affin parametrisch,so folgt die Lipschitz-Stetigkeit von $b$ und $f$ bzgl. $\mu$ aus der Lipschitz-Stetigkeit der Funktionen $\theta_b(\mu)$ und $\theta_f(\mu)$.

\ssect{5.6 Definition}{(Fréchet-Ableitung)}
Seien $X,Y$ Banachräume und sei $f:X\to Y$ eine Funktion.
Dann ist $f$ \bet{Fréchet-differenzierbar} im Punkt $x_0\in X$ genau dann, wenn eine stetige lineare Abbildung $A:X\to Y$ existiert, so dass
\[
f(x_0+h)-f(x_0) = Ah + o(\norm{h}),~h\to 0.
\]
Wenn diese Abbildung existiert, nennen wir sie \bet{Fréchet-Ableitung} von $f$ in $x_0$ und bezeichnen sie mit $f'(x_0) =: A$.

\ssect{5.7 Definition}{(Partielle Ableitung)}
Seien $X,Y,Z$ Banachräume.
Wir betrachten eine Funktion $f:X\times Y\to Z, (x,y)\mapsto f(x,y)$, wenn wir $y_0\in Y$ festhalten, ist $F(.) := f(.,y_0):X\to Z$ eine Funktion in einer Variablen $x\in X$
Die partielle Ableitung von $f$ nach $x$ ist dann analog zu den partiellen Ableitungen von Funtionen im $\R^n$ definiert:
\[
\diff{f}{x} (x_0,y_0) \equiv F'(x_0).
\]
Analog kann man $x_0\in X$ festhalten und mit Hilfe von $G(.):= f(x_0,.):Y\to Z$ die partielle Ableitng von $f$ nach $y$ definieren:
\[
\diff{f}{y} (x_0,y_0) \equiv G'(y_0).
\]

\ssect{5.8 Bemerkung}{(Höhere Ableitungen)}
Höhere Ableitungen werden durch iterative Anwendung der obigen Definitionen definiert.
 
\ssect{5.9 Satz}{}
Seien $b:X\times X\times P\to \R$ und $f:X\times P\to \R$ $k$-mal stetig differenzierbar nach $\mu$.
Seien ferner $b(.,.;\mu)$ koerziv und stetig für alle $\mu\in P$ und $f.;\mu$ stetig für alle $\mu\in P$.
Dann ist die Lösungsabbildung $\gamma$ $k$-mal stetig differenzierbar.\\

\bet{Beweisidee:} Lax-Milgram, Satz über implizite Funktionen für Banachräume (siehe z.B. [Garlet, 2013], [Riuzicka, 2004]).
\hfill $\square$

\ssect{5.10 Satz}{exponentielle Konvergenz der RB Approximation für die parametrische, stationäre WLG}
Wir betrachten die parametrische, stationäre WLG mit den Voraussetzungen aus Definition 3.7 und den folgenden zusätzlichen Annahmen:
\begin{enumerate}[(i)]
	\item $P = \prod_{i=1}^{p} P_i$ mit $P_i  =[\mu_i^{\min}\dt{,}\mu_i^{\max}]\subset \R$.
	\item $\kappa(x;\mu)$ und $q(x;\mu)$ sind unendlich oft stetig differenzierbar bzgl. $\mu$ und es gilt:
	Für alle $\mu\in P$ und für alle $i=1\dt{,}p$ existiert ein $0<\gamma_i<+\infty$, so dass
	\[
	\norm{\frac{1}{\kappa(.;\mu)} \frac{\partial^m \kappa(.;\mu)}{\partial \mu_i^m}}_{L^\infty(\Omega)} \le \gamma_i^m \cdot m!
	\]
	und
	\[
	\frac{1}{1+\norm{q(.;\mu)}_{L^2(\Omega)}} \norm{\frac{\partial^m q(.;\mu)}{\partial \mu_i^m} }_{L^2(\Omega)} \le \gamma_i^m\cdot m!.
	\]	 
\end{enumerate}
 Dann gilt die folgende Abschätzung:
\[
d_n(M,X_n) \le C\cdot \sum_{j=1}^{p} e^{-r_jm_j},
\]
mit $r_j,m_j >0$ und Konstante $C>0$.\\

\bet{Beweis:} [Quarterone, Manzoni, Megri, 2015].
\hfill $\square$

\subsection{Exponentielle Konvergenz im Falle geringer parametrischer Komplexität}

\ssect{5.11 Satz}{(Exponentielle Konvergenz im Falle dominanter Terme in der affinen Darstellung der Bilinearform [Lassik, Manzoni, Quarteroni, Rozza, 2013])}
Seien $b(u,v;\mu) = \theta_b^1(\mu)b^1(u,v) + \theta_b^2(\mu)b^2(u,v)$ und $f(v;\mu) = \sum_{q=1}^{Q_f} \theta_f^q(\mu)f^q(v)$ und seien $\mathbb{B}_h^q\in \R^{N_h\times N_h},q=1,2,~\mathbb{F}_h^q\in \R^{N_h},q01\dt{,}Q_f$ definiert durch
\[
(\mathbb{B}_h^q)_{ij} := b^q(\bar{\varphi}_j,\bar{\varphi}_i),~(\mathbb{F}_h^q)_i := f^q(\bar{\varphi}_i),~1\le i,j\le N_h,
\]
wobei $\bar{\varphi}_i,1\le i\le N_h$ die Knotenbasis aus Definition 2.50.
Wir nehmen an, dass
\begin{enumerate}[(i)]
	\item $\mathbb{B}_h^1$ ist invertierbar.
	\item $\rho\enbrace{\frac{\theta_b^2(\mu)}{\theta_b^1(\mu)}(\mathbb{B}_h^1)^{-1}\mathbb{B}_h^2}<1$ für alle $\mu\in P$, wobei $\rho$ der Spektralradius ist.
	\item $\exists \varepsilon>0: \abs{\frac{\theta_b^2(\mu)}{\theta_b^1(\mu)}} \le \frac{1-\varepsilon}{\norm{(\mathbb{B}_h^1)^{-1}\mathbb{B}^2_h}_{\mathbb{X}_h,\mathbb{X}_h^{-1}}}$, wobei $(\mathbb{X}_h,\mathbb{X}_h^{-1})$ Matrixnorm, definiert durch
	\[
	\norm{\mathbb{B}}_{\mathbb{X}_h,\mathbb{X}_h^{-1}} := \sup\limits_{\mathbb{V}\in \R^{N_h}} \frac{\norm{\mathbb{B}\mathbb{V}}_{\mathbb{X}_h^{-1}}}{\norm{\mathbb{V}}_{\mathbb{X}_h}},	
	\]
	mit $\norm{\mathbb{V}}_{\mathbb{X}_h}=\sqrt{V^T\mathbb{X}_hV}$ und die $(\mathbb{X}_h,\mathbb{X}_h^{-1})$-Norm die $L(X_h,X_h^{-1})$-Norm realisiert.
\end{enumerate}
Dann gilt die folgende Abschätzung:
\[
d_n(M,X_n) \le C\cdot e^{-an} \text{ für } C,a>0.
\]

\bet{Beweis:}\\
Sei $\mathbb{U}_h(\mu)\in \R^{N_h}$ Lösung des LGS
\[
\mathbb{B}_h(\mu) \mathbb{U}_h(\mu) = \mathbb{F}_h(\mu),
\]
wobei $\mathbb{B}_h(\mu),\mathbb{F}_h(\mu)$ definiert in Folgerung 3.28.
Dann gilt:
\[
\mathbb{U}_h(\mu) = \enbrace{I + \frac{\theta_b^2(\mu)}{\theta_b^1(\mu)}(\mathbb{B}_h^1)^{-1}\mathbb{B}_h^2}^{-1} (\theta_b^1(\mu)\mathbb{B}_h^1)^{-1} \enbrace{\sum_{q=1}^{Q_f}\theta_f^q(\mu)\mathbb{F}_h^q}.
\]
Ausnutzen von Annahme (ii) und der Neumannreihe: $\sum_{k=0}^{\infty} \mathbb{B}^k = (I-\mathbb{B})^{-1}$ liefert:
\[
\mathbb{U}_h(\mu) = \sum_{k=0}^{\infty} \sum_{q=1}^{Q_f} \frac{(-1)^k (\theta_b^2(\mu))^k \theta_f^q(\mu)}{(\theta_b^1(\mu))^{k+1}} \bigg((\mathbb{B}_h^1)^{-1}\mathbb{B}_h^2\bigg)^k (\mathbb{B}_h^1)^{-1} \mathbb{F}_h^q.
\]
Nun führen wir die Basisvektoren
\[
\mathbb{A}_{k,q} = \bigg((\mathbb{B}_h^1)^{-1}\mathbb{B}_h^2\bigg)^k (\mathbb{B}_h^1)^{-1} \mathbb{F}_h^q
\]
ein und schreiben
\begin{align}
\mathbb{U}_h(\mu) = \sum_{k=0}^{\infty} \sum_{q=1}^{Q_f} \frac{(-1)^k (\theta_b^2(\mu))^k \theta_f^q(\mu)}{(\theta_b^1(\mu))^{k+1}} \mathbb{A}_{k,q}.
\end{align}
Ferner bezeichnen wir mit 
\[
X_n^{\mathbb{A}} := \spann \penbrace{\mathbb{A}_{k,q} ~|~k=0\dt{,}m-1,~q=1\dt{,}Q_f} \subset \R^{N_h}
\]
den $n$-dimensionalen Unterraum, welcher durch die ersten $n$ Basisvektoren aufgespannt wird, wobei $n=Q_f\cdot m$.

Dann gilt:
\begin{align*}
d_n(M,X_h) &= \inf\limits_{X_n\subset X_h, \dim X_n=n} \sup\limits_{\mu\in P} \inf\limits_{\mathbb{V}\in \R^{N_h}} \norm{\mathbb{U}_h(\mu)- \mathbb{V}}_{\mathbb{X}_h}\\
&\le \sup\limits_{\mu\in P} \inf\limits_{\mathbb{V}_n\in X_n^{\mathbb{A}}}  \norm{\mathbb{U}_h(\mu)- \mathbb{V}_n}_{\mathbb{X}_h}\\
&\le \sup\limits_{\mu\in P} \norm{\mathbb{U}_h(\mu)- \sum_{k=0}^{m-1} \sum_{q=1}^{Q_f} \frac{(-1)^k (\theta_b^2(\mu))^k \theta_f^q(\mu)}{(\theta_b^1(\mu))^{k+1}} \mathbb{A}_{k,q} }_{\mathbb{X}_h}\\
&= \sup\limits_{\mu\in P} \norm{\sum_{k=m}^{\infty} \sum_{q=1}^{Q_f} \frac{(-1)^k (\theta_b^2(\mu))^k \theta_f^q(\mu)}{(\theta_b^1(\mu))^{k+1}} \mathbb{A}_{k,q} }_{\mathbb{X}_h}\\
&\le \sup\limits_{\mu\in P} \sum_{k=m}^{\infty} \sum_{q=1}^{Q_f} \abs{ \frac{(\theta_b^2(\mu))^k \theta_f^q(\mu)}{(\theta_b^1(\mu))^{k+1}} } \norm{\mathbb{A}_{k,q}}_{\mathbb{X}_h}\\
&= \sup\limits_{\mu\in P} \sum_{k=m}^{\infty} \sum_{q=1}^{Q_f} \abs{ \frac{(\theta_b^2(\mu))^k \theta_f^q(\mu)}{(\theta_b^1(\mu))^{k+1}} } \norm{ \bigg((\mathbb{B}_h^1)^{-1}\mathbb{B}_h^2\bigg)^k (\mathbb{B}_h^1)^{-1} \mathbb{F}_h^q }_{\mathbb{X}_h}\\
&\le Q_f \sup\limits_{\mu,q} \penbrace{ \abs{ \frac{\theta_f^q(\mu)}{\theta_b^1(\mu)}} \norm{(\mathbb{B}_h^1)^{-1} \mathbb{F}_h^q}_{\mathbb{X}_h} }  \cdot \sum_{k=m}^{\infty} \abs{ \frac{(\theta_b^2(\mu))^k}{(\theta_b^1(\mu))^k} } \norm{(\mathbb{B}_h^1)^{-1}\mathbb{B}_h^2}^k_{\mathbb{X}_h,\mathbb{X}_h^{-1}}\\
&\stackrel{(ii)}{\le} Q_f \sup\limits_{\mu,q} \penbrace{ \abs{ \frac{\theta_f^q(\mu)}{\theta_b^1(\mu)}} \norm{(\mathbb{B}_h^1)^{-1} \mathbb{F}_h^q}_{\mathbb{X}_h} }  \cdot \sum_{k=m}^{\infty} (1-\varepsilon)^{k-m+m}\\
&\stackrel{l=k-m}{=} Q_f \sup\limits_{\mu,q} \penbrace{ \abs{ \frac{\theta_f^q(\mu)}{\theta_b^1(\mu)}} \norm{(\mathbb{B}_h^1)^{-1} \mathbb{F}_h^q}_{\mathbb{X}_h} }  \cdot (1-\varepsilon)^m \sum_{l=0}^{\infty} (1-\varepsilon)^l\\
&= Q_f \sup\limits_{\mu,q} \penbrace{ \abs{ \frac{\theta_f^q(\mu)}{\theta_b^1(\mu)}} \norm{(\mathbb{B}_h^1)^{-1} \mathbb{F}_h^q}_{\mathbb{X}_h} }  \cdot (1-\varepsilon)^m \frac{1}{1-(1-\varepsilon)}\\
&= \frac{Q_f}{\varepsilon} \sup\limits_{\mu,q} \penbrace{ \abs{ \frac{\theta_f^q(\mu)}{\theta_b^1(\mu)}} \norm{(\mathbb{B}_h^1)^{-1} \mathbb{F}_h^q}_{\mathbb{X}_h} }  \cdot \exp \enbrace{\frac{\ln (1-\varepsilon)n}{Q_f}}
\end{align*}
Indem wir $a = -\frac{\ln (1-\varepsilon)n}{Q_f}$ und $C = \frac{Q_f}{\varepsilon} \sup\limits_{\mu,q} \penbrace{ \abs{ \frac{\theta_f^q(\mu)}{\theta_b^1(\mu)} \norm{(\mathbb{B}_h^1)^{-1} \mathbb{F}_h^q}_{\mathbb{X}_h} } }$ wählen, folgt die Behauptung.
\hfill $\square$

\ssect{5.12 Bemerkung}{}
Gilt $b^2(u,v)=0$ und damit $\mathbb{B}_h^2 =0$ in Satz 5.11, so erhalten wir die folgende Darstellung für $\mathbb{U}_h(\mu)$:
\[
\mathbb{U}_h(\mu) = \sum_{q=1}^{Q_f} \frac{\theta_f^q(\mu)}{\theta_b^1(\mu)} \mathbb{A}_{0,q}.
\]
Der Greedy-Algorithmus würde dann eine Basis mit $N \approx Q_f$ konstruieren.
Wir erhalten schließlich noch die allgemeine Aussage:

\ssect{5.13 Satz}{}
Sei $u(\mu)$ eine Lösung von $(P(\mu)), M_e := \penbrace{u(\mu)~|~\mu\in P}$ und $b$ affin parametrisch.
Dann gilt:
\[
d_n(M_e,X) \le C\cdot e^{-cn^{\frac{1}{Q_b}}},
\]
mit Konstanten $c,C>0$.\\

\bet{Beweis:} [Ohlberger, Rave, 2015].
\hfill $\square$

\section{Empirische Interpolation}

\subsection{Die empirische Interpolationsmethode}
Unterkapitel 3.3 zeigt, dass affin parametrische (Bi-)Linearformen unabdingbar für eine Offlin-/Onlinezerlegung des RB-Modells sind.
Gesucht ist daher ein Approximationsverfahren für parametrische Funktionen $g:\Omega\times P\to \R$ der Form
\begin{align}
g(x;\mu) \approx I_m[(g(.;\mu))](x) = \sum_{m=1}^M \theta_g^m(\mu)z_m(x),
\end{align}
mit Skalarenfunktionen $\theta_g^m(\mu)$ und einer sogenannten 'kollateralen Reduzierten Basis' $Z_m=\{z_m\}_{m=1}^M$.
Da wir in Unterkapitel 3.3 und in Kapitel 5 gesehen haben, dass die Anzahl der Summanden in der affin parametrischen Darstellung sowohl den Rechenaufwand in der Offline-/Online-Phase, als auch das Konvergenzverhalten der RB-Approximation beeinflussen kann.
Daher ist es wichtig eine Approximation zu finden, für die $M$ in (6.1) möglichst klein ist.
Daher sind zum Beispiel Chebyshev oder Legendre Polynome \marginnote{Polynome schlecht für komplexe Gebiete.} als kollaterale Basis nur bedingt geeignet.

Stattdessen wählen wir wieder einen 'Snapshot basierten Ansatz', dass heißt $Z_M\subset \spann \penbrace{g(.;\mu)~|~\mu\in S_{\train}\subset P}$.
Die in [Barrault, Meday, Nguyen, Patera, 2004] eingeführte \bet{Empirische Interpolationmethode} ist eine Möglichkeit eine solche Approximation zu bestimmen.

\ssect{6.1 Definition}{(Empirische Interpolationsmethode)}
Sei $G_{\train} := \penbrace{g(.;\mu)~|~\mu\in S_{\train}} \subset G := \penbrace{g(.;\mu)~|~\mu\in P}\subset C^0(\bar{\Omega})$ mit $S_{\train}\subset P$ Menge zu interpolierender Funktionen und sei $\varepsilon$ eine vorgegebene Toleranz und $M_{\max}$ eine vorgegebene Anzahl von Iterationsschritten.
Für $M\le M_{\max},M\le \dim (\spann (G_{\train}))$ definieren wir rekursiv die \bet{Interpolationspunktmenge} $T_M := \{t^1\dt{,}t^M\}\subset \bar{\Omega}$ und die kollaterale Reduzierte Basis $Z_M := \{z_1\dt{,}z_M\}\subset \spann (G_{\train})$ wie folgt:

Initialisiere:
$M=0;~e_0=\varepsilon + 1;~ I_0[g(.;\mu)](x) \equiv 0;~ \mu^1 = \argmax_{\mu\in S_{\train}} \norm{g(.;\mu)}_{C^0(\bar{\Omega})};~ T_0=\emptyset;~Z_0=\emptyset$.

\texttt{while} $M<M_{\max}$ \texttt{and} $e_M > \varepsilon$:

\qquad $M \leftarrow M+1$\\
\qquad $r_M(x) = g(x;\mu^M) - I_{M-1}[g(.;\mu)](x)$\\
\qquad $t^M = \argmax_{x\in \bar{\Omega}} \abs{r_M(x)}$\\
\qquad $T_M = T_{M-1} \cup \{t^M\}$\\
\qquad $z_M(x) = r_M(x) / r_M(t^M)$\\
\qquad $Z_M = Z_{M-1} \cup \{z_M\}$\\
\qquad $e_M = \max_{\mu\in S_{\train}} \norm{g(.;\mu)}_{C^0(\bar{\Omega})}$\\
\qquad $\mu^{M+1} = \argmax_{\mu\in S_{\train}} \norm{g(.;\mu)}_{C^0(\bar{\Omega})}$\\
\texttt{end}\\

Hierbei bestimmen wir die Koeffizienten $\sigma_j^{M-1}(\mu^M)$ von $I_{M-1}[g(.;\mu^M)] := \sum_{j=1}^{M-1} \sigma_j^{M-1}(\mu^M) z_j$ durch lösen des LGS
\begin{align}
\sum_{j=1}^{M-1} \sigma_j^{M-1}(\mu^M) z_j(t^i) = g(t^i;\mu^M),~ 1\le i\le M-1.
\end{align}
Für allgemeine $\mu\in P$ definieren wir
\begin{align}
I_M[g(.;\mu)]:= \sum_{j=1}^{M} \sigma_j^{M}(\mu) z_j,
\end{align}
wobei
\begin{align}
\sum_{j=1}^{M} \sigma_j^{M}(\mu) z_j = g(t^i;\mu),~ 1\le i \le M.
\end{align}
Schließlich definieren wir die Matrix $\mathbb{B}^M\in \R^{M\times M}$ durch
\begin{align}
B_{ij}^M = z_j(t^i),~ 1\le i,j \le M.
\end{align}
Wir zeigen nun, dass die Konstruktion der Interpolationspunkte $\{t^i\}_{i=1}^M$ wohldefiniert ist und das $\{z_m\}_{m=1}^M$ linear unabhängig sind.
Dazu benötigen wir das folgende Resultat:

\ssect{6.2 Lemma}{}
Unter der Annahme, dass für $W_{M-1} := \spann \{z_1\dt{,}z_{M-1}\} ~ \dim (W_{M-1}) = M-1$ gilt und $\mathbb{B}^{M-1}$ invertierbar ist, gilt 
\[
I_{M-1}[v] = v ~\forall v\in W_{M-1},
\]
wobei 
\[
I_{M-1}[v] = \sum_{j=1}^{M-1} \sigma_j^{M-1} z_j,
\]
und $\sigma_j^{M-1}$ Lösung von $\sum_{j=1}^{M-1} \sigma_j^{M-1} z_j(t^i) = v(t^i),~1\le i\le M-1$.
Mit anderen Worten ist die Interpolation also exakt für alle $v\in W_{M-1}$.\\

\bet{Beweis:}\\
Da nach der Annahme $Z_{M-1} = \penbrace{z_1\dt{,}z_{M-1}}$ Basis von $W_{M-1}$ läst sich jedes $v\in W_{M-1}$ darstellen als
\[
v(x) = \sum_{j=1}^{M-1} \gamma_j^{M-1} z_j(x),
\]
mit entsprechenden Koeffizienten $\gamma_j^{M-1}\in \R$.

Insbesondere gilt in den Interpolationspunkten $t^1\dt{,}t^{M-1}$:
\[
v(t^i) = \sum_{j=1}^{M-1} \gamma_j^{M-1} z_j(t^i),~ 1\le i \le M-1.
\]
Da $B_{ij}^{M-1} = z_j(t^i)$, folgt aus der Invertierbarkeit von $\mathbb{B}^{M-1}$, dass $\sigma_j^{M-1}=\gamma_j^{M-1}$ für $j=1\dt{,}M-1$ und daher $I_{M-1}[v]=v$.
\hfill $\square$

\ssect{6.3 Satz}{}
Sei $M_{\max}< \dim(\spann (G_{\train}))$.
Dann gilt für jedes $M\le M_{\max}$, dass $W_M:=\spann\penbrace{z_1\dt{,}z_M}$ die Dimension $M$ hat.
Ferner ist die Matrix $\mathbb{B}^M$ eine untere Dreiecksmatrix mit 1 auf der Diagonalen und daher invertierbar.
Schließlich sind die Interpolationspunkte $t^1\dt{,}t^M$ paarweise disjunkt.\\

\bet{Beweis:}\\
Durch vollständige Induktion:\\
Es gilt offensichtlich, dass $W_1=\spann\{z_1\}$ Dimension 1 hat und dass $\mathbb{B}^1=1$ invertierbar ist.

Wir nehmen an, dass $W_{M-1} = \spann\{z_1\dt{,}z_{M-1}\}$ die Dimension $M-1$ hat und die Matrix $\mathbb{B}^{M-1}$ invertierbar ist.
Es ist dann zu zeigen, dass
\begin{enumerate}[(i)]
	\item $\dim W_M = M$ mit $W_M:=\spann\penbrace{z_1\dt{,}z_M}$,
	\item $\mathbb{B}^M$ ist invertierbar.\\
	(Die Aussage über die Disjunktheit der Interpolationspunkte folgt dann direkt.)
\end{enumerate}

\bet{Beweis (i):}
\begin{align*}
e_{M-1} &= \max\limits_{\mu\in S_{\train}} \norm{g(.;\mu) - I_M[g(.;\mu)]}_{C^0(\bar{\Omega})}\\
&\ge \max\limits_{\mu\in S_{\train}} \inf\limits_{v\in W_{M-1}} \norm{g(.;\mu) - v}_{C^0(\bar{\Omega})}\\
&\ge \inf\limits_{W \subset C^0(\bar{\Omega}), \dim W = M_{\max}} \max\limits_{\mu\in S_{\train}} \inf\limits_{v\in W} \norm{g(.;\mu) - v}_{C^0(\bar{\Omega})}\\
&= d_{M_{\max}} ( G_{\train}, C^0(\bar{\Omega})) > 0,
\end{align*}
da $M_{\max} < \dim (\spann (G_{\train}))$ vorausgesetzt ist.
Falls nun $\dim W_M\neq M$ gilt, so folgt $g(.;\mu^M)\in W_{M-1}$ und damit nach Lemma 6.2, dass $e_{M-1} = 0~\lightning$.
Dies zeigt, dass $\dim W_M=M$ und
\begin{align*}
\norm{r_M}_{C^0(\bar{\Omega})} &= \norm{g(.;\mu^M) - I_{M-1}[g(.;\mu^M)]}_{C^0(\bar{\Omega})}\\
&= \max\limits_{\mu\in S_{\train}} \norm{g(.;\mu) - I_{M-1}[g(.;\mu)]}_{C^0(\bar{\Omega})}\\
&= e_{M-1} > 0.
\end{align*}

\bet{Beweis (ii):} Es gilt:
\[
B_{ij} = z_j(t^i) = \frac{r_j(t^i)}{r_j(t^j)} = \frac{g(t^i; \mu^j) - \sum_{l=1}^{j-1} \sigma_l^{j-1} z_l(t^i) }{r_j(t^j)}.
\]
Damit folgt:
\begin{enumerate}[(1)]
	\item $B_{ij}^M = 1$ für $i=j$.
	\item $B_{ij}^M = 0$ für $i<j$, da nach Konstruktion
	\[
	g(t^i; \mu^j) = \sum_{l=1}^{j-1} \sigma_l^{j-1} z_l(t^i) \text{ für } 1\le i\le j-1.
	\]
	\item $\abs{B_{ij}^M}\le 1$, da $t^j= \argmax_{x\in \bar{\Omega}} \abs{r_j(x)}$.
\end{enumerate}
Daher ist $\mathbb{B}^M$ untere Dreiecksmatrix mit 1 auf der Diagonalen und daher invertierbar.
Dies zeigt auch, dass $t^M \neq t^i$ für $i=1\dt{,}M-1$.
Denn angenommen, dass $t^M = t^i$ für ein $i\in \penbrace{1\dt{,}M-1}$, dann folgt aus der Definition von $t^M$:
\begin{align*}
\max\limits_{x\in \bar{\Omega}} \abs{r_M(x)} &= \abs{r_M(t^M)} = \abs{r_M(t^i)}\\
&= \abs{g(t^i;\mu^M) - \sum_{j=1}^{M-1} \sigma_j^{M-1}(\mu^M) z_j(t^i)}\\
&= 0 ~\lightning.
\end{align*}
\hfill $\square$

\subsection{Praktische Implementierung}
Da die Berechnung von $t^M$ in der in Definition 6.1 definierten Empirischen Interpolationsmethode (EIM) sehr teuer ist, bestimmt man im Allgemeinen nicht das Maximum von $\abs{r_M(x)}$ über $\bar{\Omega}$, sondern über eine endliche Teilmenge $\Omega_h := \{x^k\}_{k=1}^{N_h}$.
Im Kontext von Finiten Elementen können die $x^k$ zum Beispiel Quadraturpunkte oder die Ecken des Gitters sein.
Die Kosten zur Berechnung des Maximums von $\abs{r_M(x)}$ skalieren dann linear in der Anzahl der Punkte $x^k$.
In diesem Setting können wir auch eine algebraische Version der EIM angeben.

Dazu führen wir zunächst die Abbildung $\mathbb{G}:P\to \R^{N_h}$ ein, welche wie folgt definiert ist
\[
(\mathbb{G}(\mu))_{i} := g(x^i;\mu),~i=1\dt{,}N_h,~\mu\in P.
\]
Ferner bezeichnen wir mit $\Z\in\R^{N_h\times M}$ die Matrix
\[
(\Z)_{ik} := z_k(x^i),~i=1\dt{,}N_h,~k=1\dt{,}M.
\]
Deren Spalten die diskreten Repräsentanten der kollateralen Basisfunktionen enthalten.

Schließlich bezeichnen wir mit $J_M:=\{i_1\dt{,}i_M\}$ eine Menge von Interpolationsindizes, so dass $\{t^1\dt{,}t^M\} = \{x^{i_1}\dt{,}x^{i_M}\}$ und führen die Matrix $\mathbb{P} = [e_{i_1}\dt{,}e_{i_M}]\in \R^{N_h\times M}$ ein.
Hierbei bezeichnen $e_{i_j}$ die Einheitsvektoren, welche in der $i_j$-ten Zeile eine 1 haben.
Dann gilt zum Beispiel:
\[
\Pw^T \mathbb{G}(\mu) = \bigg( g(t^1;\mu)\dt{,} g(t^M;\mu) \bigg)^T
\]
und analog für die kollaterale Basen.
Die diskrete empirische Interpolierende $I_M[\mathbb{G}(\mu)]$ ist dann gegeben durch
\begin{align}
I_M[\mathbb{G}(\mu)] = \Z \sigma(\mu),
\end{align}
wobei $\sigma(\mu) = (\sigma_1(\mu)\dt{,}\sigma_M(\mu))^T \in \R^M$ Lösung des folgenden linearen Gleichungssystems:
\begin{align}
\Pw^T \mathbb{G}(\mu) = (\Pw^T\Z)\sigma(\mu).
\end{align}
Die diskrete empirische Interpolierende können wir damit auch wie folgt schreiben:
\begin{align}
I_M[\mathbb{G}(\mu)] = \Z (\Pw^T\Z)^{-1} \Pw^T \mathbb{G}(\mu).
\end{align}
Der zur \bet{Offline-Phase} der EIM gehörende Algorithmus kann dann wie folgt formuliert werden, wobei $\varepsilon,M_{\max}$ wie in Definition 6.1:\\
Input: $S_{\train}, \mathbb{G}, \varepsilon, M_{\max}$\\
Output: $\Z, J_M$\\
Initialisiere: $M=0, e_0= \varepsilon+1, \mu^1=\argmax_{\mu\in S_{\train}} \norm{\mathbb{G}(\mu)}_{\infty}, \R=\mathbb{G}(\mu^1), \Z=[], J_0=\emptyset, \Pw=[]$\\
\texttt{while} $M<M_{\max}$ \texttt{and} $e_M>\varepsilon$:
\qquad $M \leftarrow M+1$\\
\qquad $i_M = \argmax_{i=1\dt{,}N_h}\abs{\R_i}$\\
\qquad $\bar{\Z} = \frac{\R}{\R_{i_m}}$, $\Z \leftarrow [\Z,\bar{\Z}_M]$\\
\qquad $J_M = J_{M-1} \cup \{i_m\}$, $\Pw \leftarrow [\Pw, \bar{e}_{i_m}]$\\
\qquad $e_M = \max_{\mu\in S_{\train}} \norm{\mathbb{G}(\mu) - \Z(\Pw^T\Z)^{-1}\Pw^T\mathbb{G}(\mu)}_{\infty}$\\
\qquad $\mu_{M+1} = \argmax_{\mu\in S_{\train}} \norm{\mathbb{G}(\mu) - \Z(\Pw^T\Z)^{-1}\Pw^T\mathbb{G}(\mu)}_{\infty}$\\
\qquad $\R = \mathbb{G}(\mu^{M+1}) -\Z(\Pw^T\Z)^{-1}\Pw^T\mathbb{G}(\mu^{M+1})$\\
\texttt{end}

\ssect{6.4 Bemerkung}{}
Zur Lösung des LGS (6.7) werden $\mathbb{O}(M^2)$ Rechenschritte benötigt, da $\mathbb{B}^M = \Pw^T\Z$ eine untere Dreiecksmatrix ist.
Der obige Algorithmus erfordert in jeder Iteration die Auswertung von $\mathbb{G}(\mu)$ für alle $\mu\in S_{\train}$.
Ist diese Auswertung teuer kann es Sinn ergeben vor dem Beginn der \texttt{while}-Schleife einmal die Matrix
\[
\mathbb{S} = [\mathbb{G}(\mu^1)\dt{,}\mathbb{G}(\mu^{n_{\train}})] \in \R^{N_h\times n_{\train}}
\]
zu assemblieren und zu speichern.
Dies ist je nach verfügbarem Speicher allerdings nur für moderate $N_h$ und $N_{\train}$ möglich.

Schließlich verfahren wir in der \bet{Online-Phase} für die EIM wie folgt:

Input: $\mu, \mathbb{B} = (\Pw^T\Z), T_M$\\
Output: $\sigma(\mu)$\\
\begin{itemize}
	\item Werte $g(.;\mu)$ in den Interpolationspunkten aus und assembliere dadurch
	\[
	(g(t^1;\mu)\dt{,}g(t^M;\mu))^T = \mathbb{B}\sigma(\mu).
	\]
	\item Löse $(g(t^1;\mu)\dt{,}g(t^M;\mu))^T = \mathbb{B}\sigma(\mu)$.
\end{itemize}

\ssect{6.5 Bemerkung}{}
Da $(\Pw^T\Z)$ eine untere Dreiecksmatrix ist benötigen wir in der Online-Phase $\mathbb{O}(M^2)$ Rechenschritte.
Insbesondere ist die Komplexität unabhängig von $N_h$.

\ssect{6.6 Bemerkung}{}
ALternativ kann $Z_M$ in einem ersten Schritt mittels POD bestimmt werden.
Die Interpolationspunkte werden anschließend so bestimmt, dass sie das Residuum maximieren.
Da die Basisfunktionen POD-Basisfunktionen sind und nicht iterativ gleich dem skalierten Residuum gewählt werden, sind sie zwar orthonormal, der Rechenaufwand zur Bestimmung der Koeffizienten $\sigma(\mu)$ beträgt aber $\mathbb{O}(M^3)$.
Diesen Ansatz findet man in der Literatur unter 'Discret Empirial Interpolation Method' [Chaturantabut, Sorensen, 2010].

\subsection{Fehlerabschätzungen}
Für analytische Untersuchungen führen wir die nodale Basis $\xi_M \subset \spann(Z_M)$ ein, wobei $\xi_m(t^i) =\delta_{im},~1\le i,m\le M$.
Die Existenz und Eindeutigkeit dieser nodalen Basis sichert das folgende Lemma.

\ssect{6.7 Lemma}{}
Für jedes $M$-Tupel $(a_i)_{i=1\dt{,}M}$ reeller Zahlen existiert genau ein Element $w\in W_M$, so dass $w(t^i)=a_i$ für alle $i=1\dt{,}M$.\\

\bet{Beweis:}\\
Folgt direkt aus der Invertierbarkeit von $\mathbb{B}^M$.
\hfill $\square$\\

Darauf basierend erhalten wir die folgende A priori Fehlerabschätzung für die EIM.

\ssect{6.8 Satz}{(A priori Fehlerabschätzung)}
Sei $I_M:C^0(\bar{\Omega}) \to W_M = \spann\{\xi_i\}_{i=1}^M \subset C^0(\bar{\Omega})$ Interpolationsoperator zu den Punkten $\{t^i\}_{i=1}^M\subset \bar{\Omega}$ und $\{\xi_i\}_{i=1}^M$ nodale Basis, dass heißt $\xi_i(t^j)=\delta_{ij}, ~1\le i,j\le M,~I_M[g(.;\mu)] = \sum_{i=1}^{M} g(t^i;\mu)\xi_i$.
Dann ist 
\[
\Lambda_M := \max\limits_{x\in\bar{\Omega}} \sum_{i=1}^M \abs{\xi_i(x)}
\]
die \bet{Lebesgue-Konstante} der Interpolation.
Es gilt:
\begin{enumerate}[(1)]
	\item 
	\[
	\norm{g(.;\mu) - I_M[g(.;\mu)]}_{C^0(\bar{\Omega})} \le (1-\Lambda_M) \inf\limits_{w\in W_M} \norm{g(.;\mu) - w}_{C^0(\bar{\Omega})}
	\]
	für $g(.;\mu)\in C^0(\bar{\Omega})$.
	\item Für die Lebesgue-Konstante gilt die Abschätzung
	\[
	\Lambda_M \le 2^M -1.
	\]
\end{enumerate}

\bet{Beweis:}
\begin{enumerate}
	\item Sei $g(.;\mu[(1)])\in C^0(\bar{\Omega}),x\in \bar{\Omega}$ und $v\in W_M$.
	Dann gilt
	\begin{align*}
	\abs{g(x;\mu) - I_M[g(.;\mu)](x)} &\le \abs{g(x;\mu) - v(x)} + \abs{v(x) - I_M[g(.;\mu)](x)}\\
	&\bgl{6.2} \abs{g(x;\mu) - v(x)} + \abs{I_M[v](x) - I_M[g(.;\mu)](x)}\\
	&= \abs{g(x;\mu) - v(x)} + \abs{\sum_{j=1}^M(v(t^j)-g(t^j;\mu)) \xi_j(x)}\\
	&\le \norm{g(.;\mu) - v}_{C^0(\bar{\Omega})} + \max\limits_{x\in \bar{\Omega}} \sum_{j=1}^M \abs{\xi_j(x)} \norm{g(.;\mu) - v}_{C^0(\bar{\Omega})}\\
	&= \norm{g(.;\mu) - v}_{C^0(\bar{\Omega})} + \Lambda_M \norm{g(.;\mu) - v}_{C^0(\bar{\Omega})}
	\end{align*}
	\item Zunächst bemerken wir,dass aus $z_m(x) = \sum_{j=1}^M z_m(t^j)\xi_j(x)$ die Darstellung
	\[
	z_m(x) = \sum_{j=1}^M \mathbb{B}_{jm}^M \xi_j(x)
	\]
	folgt.
	Da $\mathbb{B}_{mm}^M=1$, folgt
	\begin{align*}
	\abs{\xi_m(x)} &= \abs{z_m(x) - \underbracket{\sum_{j=1}^{m-1} \mathbb{B}_{jm}^M\xi_j(x)}_{=0} - \sum_{j=m+1}^{M} \mathbb{B}_{jm}^M\xi_j(x)}\\
	&= \abs{z_m(x) - \sum_{j=m+1}^{M} \mathbb{B}_{jm}^M\xi_j(x)}\\
	&\le 1 + \sum_{j=m+1}^M \abs{\xi_j(x)},~ 1\le m\le M-1.
	\end{align*}
	Da $\abs{\xi_M(x)}=\abs{z_M(x)}\le 1$ gilt zum Beispiel
	\[
	\abs{\xi_{M-1}(x)} \le 1+1;~ \abs{\xi_{M-2}(x)}\le 1+1+2;~ \abs{\xi_{M-3}(x)}\le 1+1+2+4
	\]
	und damit
	\[
	\abs{\xi_{M+1-m}(x)} \le 2^{m-1},~ 1\le m\le M.
	\]
	Daher gilt
	\begin{align*}
	\sum_{m=1}^M \abs{\xi_m(x)} &= \sum_{m=1}^M \abs{\xi_{M+1-m}(x)} \le \sum_{m=1}^M 2^{m-1}\\
	&= \sum_{m=0}^{M-1} 2^m = 2^M-1 ~ \text{(geometrische Reihe)}
	\end{align*}
	\hfill $\square$
\end{enumerate}

\ssect{6.9 Bemerkung}{}
Obige Abschätzung ist sehr pessimistisch; in der Praxis werden meist sehr viel bessere Lebesgue-Konstanten beobachtet.
Allerdings ist die Schranke scharf, dass heißt es existieren Beispiele mit $\Lambda_M=2^M-1$ (vgl. [Maday, Nguyen, Patera, Pan, 2009]).

Die Abschätzung in 6.8 macht aber eine Aussage über die Stabilität der EI in dem Sinne, dass wir Konvergenz der Approximation für $M\to \infty$ erhalten, falls der Bestapproximationsfehler schneller als $2^M$ fällt, dass für langsame Konvergenzraten die EI aber für große $M$ instabil werden kann.

\ssect{6.10 Satz}{(Exponentielle Konvergenz des Interpolationsfehlers [Maday, Nguyen, Patera, Pan, 2009])}
Falls eine Folge von endlich dimensionalen Unterräumen $Y_1\subset Y_2\dt{\subset}
Y_M\dt{\subset} \spann(G)$ mit $\dim Y_M=M$ gibt und eine Konstante $c>0$ und ein $\alpha>\ln(4)$ existiert, so dass gilt
\[
\sup\limits_{\mu\in P} \inf\limits_{v\in Y_M} \norm{g(.;\mu) -v}_{C^0(\bar{\Omega})} \le c\cdot e^{-(\alpha-\ln(4))M}.
\]

\bet{Beweis:} siehe [MNPP, 2009].

\ssect{6.11 Satz}{A posteriori Fehlerabschätzung für die EIM}
Seien $I_M,I_{M'}:C^0(\bar{\Omega}) \to \spann(G_{\train})$ EI-Operatoren für $M'>M,~ Z_M\subset Z_{M'}:=\{z_i\}_{i=1}^{M'},~T_M\subset T_{M'}:= \{t^i\}_{i=1}^{M'}$.
Ferner seien für $g(.;\mu)\in G$ die Koeffizienten $\sigma_j^{M'}(\mu)$ von $I_{M'}[g(.;\mu)]= \sum_{j=1}^{M'} \sigma_j^{M'}(\mu)z_j$ Lösung des LGS
\[
\sum_{j=1}^{M'} \sigma_j^{M'}(\mu) z_j(t^i)= g(t^i;\mu),~ 1\le i\le M'.
\]
Falls $g(.;\mu)\in W_{M'} := \spann \penbrace{z_1\dt{,}z_{M'}}$, so gilt die folgende A posteriori Fehlerabschätzung:
\begin{align}
\norm{g(.;\mu) - I_M[g(.;\mu)]}_{C^0(\bar{\Omega})} \le \Delta_M^{M'}(\mu),
\end{align}
wobei
\begin{align}
\Delta_M^{M'}(\mu):= \sum_{i=M+1}^{M'} \abs{\sigma_i^{M'}(\mu)}.
\end{align}

\bet{Beweis:}\\
Nach der Definition der EIM gilt:
\[
I_M[g(.;\mu)] = \sum_{i=1}^M \sigma_i^Mz_i \text{ mit } \sum_{i=1}^M \sigma_i^Mz_i(t^j) = g(t^j;\mu),~j = 1\dt{,}M.
\]
Da $\mathbb{B}^M$ linke untere Dreiecksmatrix und oberer linker Block von $\mathbb{B}^{M'}$, folgt $\sigma_j^M(\mu) = \sigma_j^{M'}(\mu)$ für $j=1\dt{,}M,~\mu\in P$.
\begin{align*}
g(x;\mu) - I_M[g(.;\mu)](x) &= I_{M'}[g(.;\mu)](x) - I_M[g(.;\mu)](x)\\
&= \sum_{j=1}^{M'} \sigma_j^{M'}z_j - \sum_{j=1}^M \sigma_j^M z_j\\
&= \sum_{j=M+1}^{M'} \sigma_j^{M'}z_j.
\end{align*}
Wegen $\norm{z_j}_{C^0(\bar{\Omega})}=1$ folgt dann die Aussage.
\hfill $\square$

\ssect{6.12 Bemerkung}{}
Da im Allgemeinen nicht $g(.;\mu)\in W_{M'}$ gilt, ist $\Delta_M^{M'}(\mu)$ im Allgemeinen keine rigorose obere Schranke für den Interpolationsfehler.

Falls letztere sehr schnell gegen 0 konvergiert für $M\to \infty$, erwarten wir, dass die Effektivität $\Delta_M^{M'}(\mu)/\norm{g(.;\mu) - I_M[g(.;\mu)]}_{C^0(\bar{\Omega})}$ aber zumindest nahe bei 1 liegt.
In praktischen Anwendungen zeigt es sich, dass eine Wahl von $M'-M\approx 10$ häufig einen verlässlichen Schätzer liefert.
Eine rigorose, aber sehr teure Fehlerschranke wurde in [Eftang, Grepl, Patera, 2010] hergeleitet.

\subsection{Anwendungen für lineare RB Methoden}
Wir betrachten wieder das Modellproblem der parametrischen stationären WLG aus Definition3.7 und nehmen zusätzlich an, dass $\kappa(.;\mu),q(.;\mu)\in C^0(\bar{\Omega})$ für alle $\mu\in P$.
Falls die Datenfunktionen $\kappa(.;\mu)$ und $q(.;\mu)$ nicht affin parametrisch sind, so können wir sie mit empirischen Interpolierenden approximieren.
\[
\kappa(x;\mu) \approx I_{M_\kappa}[\kappa(.;\mu)](x) = \sum_{j=1}^{M_\kappa} \sigma_j^\kappa(\mu) z_j^\kappa
\]
und
\[
q(x;\mu) \approx I_{M_q}[q(.;\mu)](x) = \sum_{j=1}^{M_q} \sigma_j^q(\mu) z_j^q.
\]
Die entsprechende Bilinearform und Linearform definieren wir dann wie folgt:
\begin{align}
b_M(u,v;\mu) := \int_{\Omega} I_{M_\kappa}[\kappa(.;\mu)] \nabla u \nabla v, ~\forall u,v\in X,\\
f_M(v;\mu) := \int_{\Omega} I_{M_q}[q(.;\mu)] v, ~\forall v\in X,
\end{align}
wobei wir zur Vereinfachung bei $f$ und $b$ den Doppelindex durch $M$ ersetzen.

\ssect{6.13 Definition}{(FEM für parametrische Variationprobleme mit EIM Approximation)}
Seien die Voraussetzungen aus Definition 3.9 erfüllt.
Ferner seien $b_M:X\times X\times P\to \R$ und $f_M:X\times P\to \R$ Bilinearform und Linearform, welche aus $b$ und $f$ durch eine Approximation der Datenfunktionen mittels EIM hervorgegangen sind.
Zusätzlich nehmen wir an, dass $b(.,.;\mu)$ stetig auf $X_h\times X_h$ und koerziv auf $X_h$ für alle $\mu\in P$, das heißt
\[
\exists \alpha_h^M(\mu): b_M(v,v;\mu) \ge \alpha_h^M(\mu) \norm{v}_X^2 ~\forall v\in X_h
\]
und das $f(.;\mu)$ stetig auf $X_h$.
Zu $\mu\in P$ heißt $u_h^M(\mu)\in X_h$ Lösung des FEM für das parametrische Variationsproblem mit EIM Approximation, falls gilt:
\begin{align}
b_M(u_h^M,v;\mu) = f(v;\mu),~\forall v\in X_h.
\end{align}

\ssect{6.14 Bemerkung}{}
Die Koerzivität von $b$ muss sich nicht notwendigerweise auf $b_M$ übertragen, kann aber im diskreten Fall leicht verifiziert werden.

\ssect{6.15 Definition}{(RB Modell mit EIM Approximation)}
Seien die Voraussetzungen von Definition 3.13 und 6.13 erfüllt.
Zu $\mu\in P$ heißt $u_N^M(\mu)\in X_N$ RB-Lösung des RB Modells mit EIM Approximation, falls gilt:
\[
b_M(u_N^M(\mu),v;\mu) = f_M(v;\mu)~\forall v\in X_N.
\]

\ssect{6.16 Bemerkung}{}
Ähnlich zu Satz 3.19 und Satz 3.35 erhält man A priori und A posteriori Fehlerabschätzungen, weelche allerdings zusätzliche Terme für die Abschätzung des Interpolationsfehlers enthalten.
Für Details siehe zum Beispiel [Quateroni, Manzoni, Negri, 2015] und zu A posteriori Fehlerschätzern insbesondere [Tonn, 2011, Kap. 4.5].

\ssect{6.17 Bemerkung}{}
Falls die Koeffizientenfunktionen keine Punktauswertungen erlauben, kann die Generalized Empirial Interpolation verwendet werden [Maday, Mala, 2013] oder gegebenenfalls zunächst eine Projektion in den Finite Elemente Raum vorgenommen werden.

\newpage
\section{Lokalisierte Modellreduktion}

\begin{itemize}
	\item[Ziele:]
	\item ermögliche topologische Flexibilität in der Online-Phase
	\item erlaube lokale Änderungen der Geometrie oder der Parametermenge in der Online-Phase (Online-adaptive Modellreduktion)
	\item reduziere Anzahl der Parameter
	\item[Ansatz:]
	\item Zerlege das Rechengebiet in Teilgebiete, wende in den Teilgebieten RB Methoden an und erhalte dadurch eine reduzierte Lösung auf dem globalen Begiet $\Omega$.
	\item Stichwort: Legos
\end{itemize}

\subsection{Einführung in Gebietszerlegungsmethoden}
In diesem Unterkapitel betrachten wir das folgende Modellproblem:
Finde $u$, so dass
\begin{align}
-\Delta u = q \text{ in }\Omega;~\nabla u\cdot n = 0 \text{ auf }\Sigma_N;~ u=0 \text{ auf } \Sigma_D,
\end{align}
wobei $q\in L^2(\Omega), \partial \Omega = \bar{\Sigma}_N \cup \bar{\Sigma}_D$ und $n$ äußere Normale auf $\Omega\subset \R^2$.
Zunächst zerlegen wir $\omega$ in zwei nicht-überlappende Teilgebiete $\Omega_1$ und $\Omega_2$, so dass $\bar{\Omega} = \bar{\Omega}_1 \cup \bar{\Omega}$.
Das Interface bezeichnen wir mit $\Gamma:= \bar{\Omega}_1 \cap \bar{\Omega}$.

\subsubsection{Das Modellproblem auf zerlegtem Gebiet und die Steklov-Poincaré Interface Gleichung}
Wir bezeichnen mit $u_i$ die \bet{Restriktion} der Lösung $u$ von (7.1) auf $\Omega_i, i=1,2$ und mit $n_i$ die äußere Normale auf $\partial\Omega_i\cap\Gamma$.
Dann kann das Modellproblem (7.1) äquivalent auf dem zerlegten Gebiet wie folgt formuliert werden:
\begin{align}
\begin{split}
-\Delta u_1 &= q\text{ in } \Omega_1;~\nabla u_1\cdot n = 0 \text{ auf }\Sigma_N\cap \partial\Omega_1;~ u=0 \text{ auf } \Sigma_D\cap \partial\Omega_1,\\
-\Delta u_2 &= q\text{ in } \Omega_2;~\nabla u_2\cdot n = 0 \text{ auf }\Sigma_N\cap \partial\Omega_2;~ u=0 \text{ auf } \Sigma_D\cap \partial\Omega_2,\\
u_1 &= u_2 \text{ auf } \Gamma; ~\nabla u_1 \cdot n_1 = - \nabla u_2\cdot n_2 \text{ auf } \Gamma \text{ 'transmission conditions'}.
\end{split}
\end{align}

\begin{figure}[ht]
\begin{minipage}{7.5cm}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\clip(-1.,-1.) rectangle (7.,4.);
\fill[color=black,fill=black,fill opacity=0.1] (0.,0.) -- (6.,0.) -- (6.,3.) -- (0.,3.) -- cycle;
\draw [color=black] (0.,0.)-- (6.,0.);
\draw [color=black] (6.,0.)-- (6.,3.);
\draw [color=black] (6.,3.)-- (0.,3.);
\draw [color=black] (0.,3.)-- (0.,0.);
\draw[color=black] (3.,1.5) node {\Large$\Omega$};
\draw[color=black] (3.06,-0.16) node[below] {$\Sigma_N$};
\draw[color=black] (6.38,1.66) node {$\Sigma_D$};
\draw[color=black] (3.06,3.48) node {$\Sigma_N$};
\draw[color=black] (-0.26,1.66) node {$\Sigma_D$};
\end{tikzpicture}
\caption{Gebiet $\Omega$}
\end{minipage}
\begin{minipage}{7.5cm}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\clip(-1.,-1.) rectangle (7.,4.);
\fill[color=black,fill=black,fill opacity=0.1] (0.,0.) -- (6.,0.) -- (6.,3.) -- (0.,3.) -- cycle;
\draw [color=black] (0.,0.)-- (6.,0.);
\draw [color=black] (3.,0.)-- (3.,3.);
\draw [color=black] (6.,0.)-- (6.,3.);
\draw [color=black] (6.,3.)-- (0.,3.);
\draw [color=black] (0.,3.)-- (0.,0.);
\draw[color=black] (1.5,1.5) node {\Large$\Omega_1$};
\draw[color=black] (4.5,1.5) node {\Large$\Omega_2$};
\draw[color=black] (3.3,1.5) node {$\Gamma$};
\draw[color=black] (3.06,-0.16) node[below] {$\Sigma_N$};
\draw[color=black] (6.38,1.66) node {$\Sigma_D$};
\draw[color=black] (3.06,3.48) node {$\Sigma_N$};
\draw[color=black] (-0.26,1.66) node {$\Sigma_D$};
\end{tikzpicture}
\caption{Gebietszerlegung von $\Omega$}
\end{minipage}
\end{figure}

Beachte das die Äquivalenz im schwachen Sinne zu verstehen ist ($\to$ 7.1.2).
Wir können (7.2) weiter zu einem Problem mit einer gesuchten Funktion auf dem Interface $\Gamma$ umformulieren.
Bezeichne dazu mit $\lambda$ den unbekannten und gesuchten Wert von $u$ auf $\Gamma$.
Wir betrachten die beiden Probleme
\begin{align}
\begin{split}
-\Delta w_i &= q \text{ in }\Omega_i; ~\nabla w_i\cdot n =0 \text{ auf } \Sigma_N\cap \partial\Omega_i;\\
w_i &= 0 \text{ auf } \Sigma_D\cap \partial\Omega_i;~ w_i=\lambda \text{ auf } \Gamma,~i=1,2.
\end{split}
\end{align}
Als nächstes zerlegen wir die $w_i$ in Funktionen, welche die Daten, also $q$ repräsentieren und Funktionen, welche die gesuchte Funktion $\lambda=u|_{\Gamma}$ repräsentieren.

Betrachte dazu $w_i=u_i^0 + u_i^q$, wobei  $u_i^0$ und $u_i^q,1=1,2$, als Lösungen der folgenden Probleme definiert sind:
\begin{align}
-\Delta u_i^0 &= 0\text{ in } \Omega_i;~\nabla u_i^0 = 0 \text{ auf }\Sigma_N\cap \partial\Omega_i;~ u_i^q=0 \text{ auf } \Sigma_D\cap \partial\Omega_i;~ u_i^0=\lambda \text{ auf } \Gamma
\end{align}
und 
\begin{align}
-\Delta u_i^q &= 0\text{ in } \Omega_i;~\nabla u_i^q = 0 \text{ auf }\Sigma_N\cap \partial\Omega_i;~ u_i^q=0 \text{ auf } \Sigma_D\cap \partial\Omega_i;~ u_i^q=\lambda \text{ auf } \Gamma,
\end{align}
mit $i=1,2$.

Da $u_i^0$ \bet{harmonische Fortsetzung} von $\lambda$ nach $\Omega_i$ bezeichnen wir $u_i^0$ von nun an mit $H_i\lambda$.
Ferner schreiben wir $R_iq$ für die Repräsentanten der Daten $u_i^q$.
Indem wir (7.3) mit (7.4) vergleichen stellen wir fest, dass
\begin{align}
w_i=u_i \text{ für } i=1,2 \Leftrightarrow \nabla w_1n_1 = -\nabla w_2n_2 \text{ auf } \Gamma.
\end{align}
Letztere Bedingung bedeutet, dass die gesuchte Funktion $\lambda=u|\Gamma$ die \bet{Steklov-Poincaré Interface Gleichung} erfüllen muss:
\begin{align}
S\lambda = \chi \text{ auf } \Gamma,
\end{align}
wobei
\begin{align}
\chi := -\nabla R_1qn_1 - \nabla R_2qn_2 = - \sum_{i=1}^2 \nabla R_iqn_i
\end{align}
und der \bet{Steklov-Poincaré Operator} $S$ für Funktionen $\eta$ auf $\Gamma$ formal wie folgt definiert ist:
\begin{align}
S_{\eta} := \nabla H_1\eta n_1 + \nabla H_2\eta n_2 = \sum_{i=1}^2 \nabla H_i\eta n_i.
\end{align}

\subsubsection{Schwache Formulierung des Modellproblems auf zerlegtem Gebiet}

\ssect{7.1 Definiton}{}
Sei $q\in L^2(\Omega),\Omega,\Sigma_N,\Sigma_D$ wie in Abbildung 7.1 und $\partial \Omega=\bar{\Sigma}_N\cup \bar{\Sigma}_D$.
Dann heißt $u\in X:=\penbrace{v\in H^1(\Omega)~|~v=0 \text{ auf }\Sigma_D}$ schwache Lösung des Modellproblems (7.1), falls gilt
\[
b(u,v)=f(v)~\forall v\in X,
\]
mit 
\[
b(u,v) := \int_{\Omega} \nabla u\nabla v,~ f(v):= \int_{\Omega} qv ~\forall u,v\in X.
\]
Um eine schwache Formulierung des Modellproblems auf zerlegtem Gebiet herleiten zu können benötigen wir zunächst eine adäquate Charakterisierung des Spurraums auf $\Gamma$ mittels Sobolevräumen mit gebrochenem Exponenten.

\ssect{7.2 Definition}{}
Seien $\Gamma$ und $\Omega_i\subset \R^2$ wie in Abbildung 7.2, $i=1,2$.
Für $v\in L^2(\Omega)$ definieren wir die \bet{slobedeckij-Halbnorm} als 
\[
[v]_{\frac{1}{2},2,\Gamma} := \enbrace{\int_\Omega \int_\Omega \frac{\abs{v(x)-v(y)}^2}{\norm{x-y}^3}\dint x\dint y}^{\frac{1}{2}},
\]\marginnote{$H^{\frac{1}{2}}(\Omega)$ nicht immer gleich definiert und auch nicht auf allen Gebieten äquivalent.}
den Raum $H^{\frac{1}{2}(\Omega)}$ als
\begin{align}
H^{\frac{1}{2}}(\Gamma) := \penbrace{v\in L^2(\Gamma)~|~ [v]_{\frac{1}{2},2,\Gamma} < \infty}
\end{align}
mit zugehöriger Norm
\begin{align}
\norm{v}_{H^{\frac{1}{2}}(\Gamma)} := \enbrace{\norm{v}_{L^2(\Omega)}^2 + [v]_{\frac{1}{2},2,\Gamma}}^{\frac{1}{2}}.
\end{align}
Ferner definieren wir durch
\begin{align}
(u,v)_{H^{\frac{1}{2}}(\Gamma)} := (u,v)_{L^2(\Omega)} + \int_\Omega \int_\Omega \frac{(u(x)-u(y))(v(x)-v(y))}{\norm{x-y}^3}\dint x\dint y
\end{align}
ein Skalarprodukt $H^{\frac{1}{2}}(\Gamma)$.

\ssect{7.3 Satz}{}
Unter den Voraussetzungen aus Definition 7.2 ist $H^{\frac{1}{2}}(\Gamma)$ mit der in (7.11) definierten Norm und dem in (7.12) definierten Skalarprodukt eine Hilbertraum.\\

\bet{Beweis:} [Adams, 1975], [Grisvard, 2011].
\hfill $\square$

\ssect{7.4 Spursatz}{}
Seien $\Gamma, \Omega_i,i=1,2$ wie in Abbildung 7.2.
Dann gilt für $\Omega_1$ (und analog für $\Omega_2$):
\begin{enumerate}[(1)]
	\item Es existiert eine eindeutige lineare, stetige Abbildung
	\[
	\mathcal{T}: H^1(\Omega_1) \to H^{\frac{1}{2}}(\Gamma), \text{ so dass } \mathcal{T}v=v|_\Gamma
	\]
	für alle $v\in H^1(\Omega)\cap C^0(\Omega_1)$.
	Ferner gilt die folgende Abschätzung
	\begin{align}
	\norm{\mathcal{T}v}_{H^{\frac{1}{2}}(\Gamma)} \le c_t\cdot \norm{v}_{H^1(\Omega_1)},
	\end{align}
	mit der \bet{Spurkonstanten} $c_t>0$.
	\item Es existiert eine lineare, stetige Abbildung $\mathcal{F}: H^{\frac{1}{2}}(\Gamma)\to H^1(\Omega)$, so dass
	\begin{align}
	\mathcal{T}\mathcal{F}\varphi = \varphi ~ \forall \varphi\in H^{\frac{1}{2}}(\Gamma).
	\end{align}
\end{enumerate}

\bet{Beweis:} [Adams, 1975], [Grisvard, 2011].
\hfill $\square$

\ssect{7.5 Bemerkung}{}
Betrachtet man auf dem ganzen Rand von $\Omega$ homogene Dirichletrandwerte und gilt wie in Abbildung 7.2 $\partial\Omega\cap \Gamma \neq \emptyset$, so muss auf $\Gamma$ ein andere Spurraum als $H^{\frac{1}{2}}(\Gamma)$ betrachtet werden.
Für Details siehe zum Beispiel [Quateroni, Valli 2005].\\

Nun können wir die schwache Formulierung des Modellproblems auf zerlegtem Gebiet herleiten.

\ssect{7.6 Lemma}{}
Seien $q\in L^2(\Omega),\Omega,\Omega_i,\Sigma_N,\Sigma_D,\Gamma$ wie in Abbildung 7.2, $\partial\Omega=\bar{\Sigma_D}\cup \bar{\Sigma_N}$ und seien
\[
b_i(u_i,v_i) := \int_{\Omega_i} \nabla u_i\nabla v_i ~\forall u_i,v_i\in H^1(\Omega_i),
\]
\[
f_i(v_i):= \int_{\Omega_i} q|_{\Omega_i} v_i ~\forall v_i\in H^1(\Omega_i),
\]
\[
X_i:= \penbrace{v_i\in H^1(\Omega_i)~|~v_i=0\text{ auf }\Sigma_D\cap\Omega_i},
\]
\[
X_i^0:=\penbrace{v_i\in H^1(\Omega_i)~|~v_i=0\text{ auf }(\Sigma_D\cap\Omega_i)\cup\Gamma}, ~i=1,2.
\]
Dann lässt sich die schwache Formulierung des Modellproblems (7.1) aus Definition 7.1 äquivalent wie folgt formulieren:
Finde $u_1\in X_1,u_2\in X_2$, so dass
\begin{align}
\begin{split}
b_1(u_1,v_1)=f_1(v_1) ~\forall v_1\in X_1^0\\
b_2(u_2,v_2)=f_2(v_2) ~\forall v_2\in X_2^0\\
u_1=u_2 \text{ auf }\Gamma\\
b_2(u_2,\mathcal{F}_2\eta) = f_2(\mathcal{F}_2\eta) + f_1(\mathcal{F}_1\eta) - b_1(u_1,\mathcal{F}_1\eta) ~\forall\eta\in H^{\frac{1}{2}}(\Gamma),
\end{split}
\end{align}
wobei $\mathcal{F}_i:H^{\frac{1}{2}}(\Gamma)\to X_i$ Fortsetzungsoperatoren wie in Satz 7.4.\\

\bet{Beweis:}\\
Sei zunächst $u$ die schwache Lösung aus Definition 7.1.
Wir setzen $u_i:=u|_{\Omega_i},i=1,2$ und folgern zunächst, dass $u_i\in X_i$ wegen $u\in X$.
Ferner gelten (7.15)\textsubscript{1} und (7.15)\textsubscript{2}, da wir zum Beispiel alle Testfunktionen $v_1\in X_1^0$ durch 0 auf $\Omega_2$ fortsetzen können und dadurch zulässige Testfunktionen in $X$ erhalten (folgt aus Satz 7.4, weil die Spur auf $\Gamma$ Null ist).

Weiterhin liegt für jedes $\eta\in H^{\frac{1}{2}}(\Gamma)$ die Funktion
\begin{align}
\mathcal{F}_\eta := \left\{\begin{array}{lc} \mathcal{F}_1\eta & \text{in }\Omega_1,\\ \mathcal{F}_2\eta & \text{in } \Omega_2 \end{array}\right.
\end{align}
in $X$, was $b(u,\mathcal{F}_\eta) = f(\mathcal{F}_\eta)$ und weiter (7.15)\textsubscript{4} impliziert.

Für (7.15)\textsubscript{3} zeigen wir zunächst: aus $u_i\in X_i$ und $u_1=u_2$ auf $\Gamma$ folgt $u\in X$.
Wir wissen: $u_i\in X_i$ und $u\in X$.
Wie wir sehen werden impliziert die Annahme $u_1\neq u_2$ auf $\Gamma$, dass $u\notin H^1(\Omega)$ und damit einen Widerspruch zu $u\in X$.

Gelte also $u_i\in X_i\subset H^1(\Omega_i)$ und $u_1=u_2$ auf $\Gamma$.
\begin{align*}
\int_{\Omega} u D_j \varphi &=  \int_{\Omega_1} u|_{\Omega_1} D_j \varphi + \int_{\Omega_2} u|_{\Omega_2} D_j \varphi\\
&= \int_{\Omega_1} u_1 D_j \varphi + \int_{\Omega_2} u_2 D_j \varphi\\
&= - \int_{\Omega_1} D_j u_1 \varphi - \int_{\Omega_2} D_j u_2 \varphi + \int_\Gamma u_1 \varphi (n_1)_j + \int_\Gamma u_2 \varphi (n_2)_j\\
&= - \int_{\Omega_1} D_j u_1 \varphi - \int_{\Omega_2} D_j u_2 \varphi + \int_\Gamma \underbracket{(u_1-u_2)}_{=0} \varphi (n_1)_j\\
&= - \int_{\Omega} D_j u \varphi, ~\varphi \in C_0^\infty(\Omega).
\end{align*}
Seien nun $u_i,i=1,2$ Lösungen von (7.15). 
Wir setzen
\[
u := \left\{\begin{array}{lc} u_1 & \text{in } \Omega_1,\\ u_2 & \text{in } \Omega_2, \end{array}\right.
\]
und folgern aus $u_i\in X_i$ und (7.15)\textsubscript{3}, dass $u\in X$.
Für beliebiges $v\in X$ gilt: $\eta:= \mathcal{T}_\Gamma v \in H^{\frac{1}{2}}(\Gamma)$. Sei $\mathcal{F}_\eta$ wie in (7.16).
Da $(v|_{\Omega_i} - \mathcal{F}_i\eta)\in X_i^0$ folgt aus (7.15)\textsubscript{1}, (7.15)\textsubscript{2} und (7.15)\textsubscript{3}, dass
\begin{align*}
b(u,v) &= \sum_{i=1}^{2} b_i(u_i,v|_{\Omega_i}) = \sum_{i=1}^{2} b_i(u_i,v|_{\Omega_i} - \mathcal{F}_i\eta) + b_i(u_i,\mathcal{F}_i\eta)\\
&= \sum_{i=1}^2 f_i(v|_{\Omega_i} - \mathcal{F}_i\eta) + f_i(\mathcal{F}_i\eta)\\
&= \sum_{i=1}^2 f_i(v|_{\Omega_i}) = f(v) ~\forall v\in X.
\end{align*}
\hfill $\square$

\ssect{7.7 Bemerkung}{}
(7.15)\textsubscript{4} ist die schwache Formulierung der Neumann-Bedingung (7.2)\textsubscript{8}.

Nächstes Ziel: Herleitung der Steklov-Poincaré Interface Gleichung im schwachen Sinne.

Dazu starten wir von der schwachen Neumann-Bedingung (7.15)\textsubscript{4}, wobei wir wie oben die $u_i$ zerlegen: $u_i = H_i\lambda + R_i q$, jetzt mit $H_i\lambda\in X_i$ und $R_i q \in X_i^0$, wobei $H_i\lambda, R_i q$ Lösungen von
\begin{align}
b_i(H_i\lambda,v_i) = 0~ \forall v_i\in X_i^0;~ H_i\lambda = \lambda \text{ auf } \Gamma,\\
b_i(R_i q, v_i) = f_i(v_i) ~ \forall v_i\in X_i^0.
\end{align}
Einsetzen in (7.15)\textsubscript{4} liefert
\begin{align}
\sum_{i=1}^2 b_i(H_i\lambda,\mathcal{F}_i\eta) = \sum_{i=1}^2 \bigg[ f_i(\mathcal{F}_i\eta) - b_i(R_i q,\mathcal{F}_i\eta) \bigg] ~\forall \eta\in H^{\frac{1}{2}}(\Gamma).
\end{align}
Da $H_i\lambda$ Lösung von (7.17) gilt
\begin{align}
\norm{H_i\lambda}_{H^1(\Omega_i)} \le c\cdot \norm{\lambda}_{H^{\frac{1}{2}}(\Gamma)},
\end{align}
(siehe zum Beispiel [Lions, Magenes 1972]) und wir können $\mathcal{F}_i\eta = H_i\eta$ in (7.19) wählen.
Wir erhalten die Steklov-Poincaré Interface Ungleichung im schwachen Sinne:

Finde $\lambda\in H^{\frac{1}{2}}(\Gamma)$, so dass
\begin{align}
\sum_{i=1}^2 b_i(H_i\lambda,H_i\eta) = \sum_{i=1}^2 \bigg[ f_i(H_i\eta) - b_i(R_i q, H_i \eta) \bigg] ~\forall \eta\in H^{\frac{1}{2}}(\Gamma).1
\end{align}
Zum Nachweis der Existenz und Eindeutigkeit einer Lösung $\lambda$ von (7.21) benötigen wir das folgende Lemma.

\ssect{7.8 Lemma}{}
Seien $\Omega,\Omega_i,\Sigma_N,\Sigma_D, \Gamma$ wie in Abbildung 7.2 und gelte $\partial\Omega= \bar{\Sigma_D}\cup \bar{\Sigma_N}$.
Dann ist für alle $\xi,\eta\in H^{\frac{1}{2}}(\Gamma)$ durch
\begin{align}
(\xi,\eta)_\Gamma := \sum_{i=1}^2 b_i(H_i\xi,H_i\eta)
\end{align}
ein Skalarprodukt auf $H^{\frac{1}{2}}(\Gamma)$ gegeben,wobei $H_i\xi$ Lösung von (7.17).
Ferner ist $\chi: H^{\frac{1}{2}}(\Gamma) \to \R$ definiert durch
\begin{align}
\chi(\eta) := \sum_{i=1}^2 [f_i(H_i\eta) - b_i(R_iq,H_i\eta)] ~ \forall \eta\in H^{\frac{1}{2}}(\Gamma)
\end{align}
ein lineares, stetiges Funktional.\\

\bet{Beweis:}\\
Linearität und Stetigkeit von $(.,.)_\Gamma$ folgen direkt da $b_i$ symmetrisch und $H_i$ lineare Operatoren.
Damit bleibt zu zeigen, dass $(.,.)_\Gamma$ positiv definit ist.

Mit der Poincaré-Friedrich Ungleichung folgt:
\begin{align*}
\sum_{i=1}^2 b_i(H_i\xi,H_i\xi) &= \sum_{i=1}^2 \int_{\Omega_i} \nabla H_i\xi \nabla H_i\xi = \sum_{i=1}^2 \norm{\nabla H_i\xi}_{L^2(\Omega)}\\
&\ge \sum_{i=1}^2 \frac{1}{1+ c_{p,i}^2} \norm{H_i\xi}_{H^1(\Omega)}^2.
\end{align*}
Aus dem Spursatz 7.4 können wir dann folgern, dass:
\[
\sum_{i=1}^2 b_i(H_i\xi,H_i\xi) \ge \sum_{i=1}^2 \frac{1}{c_{t,i}^2(1+ c_{p,i}^2)} \norm{\mathcal{T}H_i\xi}_{H^{\frac{1}{2}}(\Gamma)}^2.
\]
Damit folgt:
\begin{align}
\sum_{i=1}^2 b_i(H_i\xi,H_i\xi) \ge c\cdot \norm{\xi}_{H^{\frac{1}{2}}(\Omega)}^2 \ge 0.
\end{align}
Schließlich zeigen wir dann noch:
\[
(\xi,\xi)_\Gamma = 0 \Leftrightarrow \xi=0.
\]
Sei zunächst $(\xi,\xi)_\Gamma = 0$.
Dann folgt aus (7.24), dass
\[
\norm{\xi}_{H^{\frac{1}{2}}(\Omega)} = 0 \text{ und damit } \xi = 0.
\]
Sei umgekehrt $\xi=0$.
Dann folgt aus der Definition von $H_i\xi$, dass $H_i\xi = 0$ fast überall auf $\Omega_i$ und damit $(\xi,\xi)_\Gamma=0$.
Damit folgt positiv definit.
Die Linearität von $\chi$ folgt wieder direkt.
Zum Nachweis der Stetigkeit wenden wir die Poincaré-Friedrich Ungleichung und (7.20) an:
\begin{align*}
\abs{\chi(\eta)} &\le \sum_{i=1}^2 \cenbrace{\int_{\Omega_i} \abs{q|_{\Omega_i} H_i\eta} + \int_{\Omega_i} \abs{\nabla R_i q \nabla H_i\eta} }\\
&\le \sum_{i=1}^2 \cenbrace{c_{p,i} \norm{q}_{L^2(\Omega_i)} \norm{\nabla H_i\eta}_{L^2(\Omega_i)} + \norm{\nabla R_i q}_{L^2(\Omega_i)} \norm{\nabla H_i\eta}_{L^2(\Omega_i)} }\\
&\stackrel{(7.18)}{\le} \sum_{i=1}^2 \cenbrace{\enbrace{2c_{p,i} \norm{q}_{L^2(\Omega_i)}}\cdot \norm{\nabla H_i q}_{L^2(\Omega_i)} }\\
&\stackrel{(7.20)}{\le} c\cdot \norm{\eta}_{H^{\frac{1}{2}}(\Gamma)}.
\end{align*}
\hfill $\square$

\ssect{7.9 Satz}{}
Die Steklov-Poincaré Interface Gleichung (7.21) besitzt unter den Voraussetzungen von Lemma 7.8 eine eindeutige Lösung $\lambda\in H^{\frac{1}{2}}(\Gamma)$.\\

\bet{Beweis:}\\
Folgt direkt aus Lemma 7.8 und dem Rieszschem Darstellungssatz.
\hfill $\square$

\ssect{7.10 Bemerkung}{}
Der Steklov-Poincaré Operator $s:H^{\frac{1}{2}}(\Gamma) \to (H^{\frac{1}{2}}(\Gamma))'$ kann dann wie folgt charakterisiert werden:
\[
(s\xi)(\eta) := \sum_{i=1}^2 b_i(H_i\xi,H_i\eta).
\]
Der (formale) Zusammenhang zwischen (7.7) und (7.21) ergibt sich durch Multiplikation von beiden Seiten in (7.7) mit einer Testfunktion, Integration über $\Gamma$, partielle Integration und Ausnutzen von (7.4) und (7.3).

\subsection{Port reduced Static Condensation Reduced Basis Element Method (PR-SCRBE)}
Wir betrachten das parametrische Variationsproblem $(P(\mu))$ aus Definition 3.5.
Zusätzlich nehmen wir an, dass $b(.,.;\mu)$ symmetrisch und dass $b$ und $f$ affin parametrisch sind.
Wir betrachten wieder das Gebiet $\Omega$ wie in Abbildung 7.1 und definieren daher $X:=\penbrace{v\in H^1(\Omega)~|~v=0 \text{ auf }\Sigma_D}$.

\ssect{7.11 Definition}{}
Seien $b,f,X$ wie im Beginn dieses Unterkapitels definiert, $\mathcal{T}_h$ zulässige Triangulierung von $\Omega$, so dass für alle $T_j\in \mathcal{T}_h$ gilt: $S:=T_j\cap \bar{\Gamma}$ ist entweder $(2-k)$-dimensionaler Seitensimplex von $T_j$ für ein $k\in \{1,2\}$ oder es gilt $S=\emptyset$.
Schließlich sei $X_h\subset X$ zulässiger (linearer) FE Raum.
Zu $\mu\in P$ heißt $u_h(\mu)\in X_h$ Lösung des (linearen) FE Verfahrens für das parametrische Variationsproblem vom Beginn des Unterkapitels, falls gilt:
\begin{align}
b(u_h(\mu),v_h;\mu) = f(v_h;\mu) ~ \forall v_h\in X_h.
\end{align}
Nun können wir analog zum kontinuierlichem Fall eine äquivalente Formulierung auf zerlegtem Gebiet angeben.

\ssect{7.12 Lemma}{}
Seien $\Omega,\Omega_i,\Sigma_D,\Sigma_N,\Gamma$ wie in Abbildung 7.2, $\partial\Omega=\Sigma_N\cup\Sigma_D$, $\mathcal{T}_h$ wie in 7.11 und seien $P_i$ Parametermengen assoziert mit $\Omega_i,i=1,2$.
Ferner seien $b_i(.,.;\mu_i): H^1(\Omega_i)\times H^1(\Omega_i) \to \R$ und $f_i(.;\mu_i):H^1(\Omega_i)\to \R$ für $\mid_i\in P_i$ mittels Einschränkung des Integrationsgebiets auf $\Omega_i$ definiert.
Schließlich definieren wir:
\begin{align}
\Lambda_h := \penbrace{v_h|_\Gamma~|~v_h\in X_h},
\end{align}
\[
X_{i,h} := \penbrace{v_h|_{\Omega_i} ~|~v_h\in X_h},~ X_{i,h}^0 := \penbrace{v_h\in X_{i,h}~|~v_h|\Gamma = 0}.
\]
Dann lässt sich das FE Problem aus Definition 7.11 äquivalent wie folgt definieren:

Finde $u_{1,h}(\mu)\in X_{1,h}, u_{2,h}\in X_{2,h}$, so dass
\begin{align}
\begin{split}
b_1(u_{1,h}(\mu),v_{1,h};\mu_1) &= f_1(v_{1,h};\mu_1)~\forall v_{1,h}\in X_{1,h}^0,\\
b_2(u_{2,h}(\mu),v_{2,h};\mu_2) &= f_2(v_{2,h};\mu_2)~\forall v_{2,h}\in X_{2,h}^0,\\
u_{1,h}(\mu) &= u_{2,h}(\mu) \text{ auf } \Gamma\\
\sum_{i=1}^2 b_i(u_{i,h}(\mu),\mathcal{F}_{i,h}\eta_h;\mu_i) &= \sum_{i=1}^2 f_i(\mathcal{F}_{i,h}\eta_h;\mu_i) ~\forall \eta_h\in \Lambda_h,
\end{split}
\end{align}
wobei $\mathcal{F}_{i,h}: \Lambda_h \to X_{i,h}$ lineare, stetige, diskrete Fortsetzungsoperatoren sind.\\

\bet{Beweis:}\\
Analog zum Beweis von Lemma 7.6.

\subsubsection{Static condensation}
Als letztes leiten wir wie im Kontinuierlichem Fall eine Gleichung für die unbekannte Spur $\lambda_h(\mu) = u_h(\mu)|_\Gamma$ her.
Wieder zerlegen wir $u_{i,h}(\mu) = H_{i,h}(\mu_i)\lambda_h(\mu_i) + R_{i,h}^f(\mu_i)$, wobei $H_{i,h}(\mu_i)\in X_{i,h}$ und $R_{i,h}^f(\mu_i)\in X_{i,h}^0$ Lösungen von 
\begin{align}
\begin{split}
b_i(H_{i,h}(\mu_i)\xi_h, v_{i,h};\mu_i) &= 0 ~ \forall v_{i,h}\in X_{i,h}^0,\\
H_{i,h}(\mu_i)\xi_h &= \xi_h \text{ auf } \Gamma,
\end{split}
\end{align}
für alle $\xi_h\in \Lambda_h$ und 
\begin{align}
b_i(R_{i,h}^f(\mu_i), v_{i,h};\mu_i) = f_i(v_{i,h};\mu_i) ~\forall v_{i,h}\in X_{i,h}^0,
\end{align}
für $i=1,2$.

Die $H_{i,h}(\mu_i)$ sind also diskrete harmonische Fortsetzungen von Funktionen in $\Lambda_h$ nach $\Omega_i$.
Sie haben folgende Eigenschaft.

\ssect{7.13 Lemma}{}
Seien $H_{i,h}(\mu_i)\in X_{i,h}$ Lösungen von (7.27) und seien die Voraussetzungen von Definition 7.11 und Lemma 7.12 erfüllt, dann existieren Konstanten $c_1,c_2(\mu_i) > 0$, welche von $\Omega_i$, aber nicht von der Gitterweite $h$ abhängen, so dass gilt:
\begin{align}
c_1 \norm{\xi_h}_{H^{\frac{1}{2}}(\Gamma)} \le \norm{H_{i,h}(\mu_i)\xi_h}_{H^1(\Omega_i)} \le c_2(\mu_i) \norm{\xi_h}_{H^{\frac{1}{2}}(\Gamma)} ~\forall \xi_h\in \Lambda_h,~ i=1,2.
\end{align}

\bet{Beweis:}

[Quateroni, Valti, Theorem 4.13, Seite 105f]. \\

Schließlich erhalten wir die diskrete, parametrische Steklov-Poincaré Interface Gleichung:

Finde $\lambda_h(\mu)\in \Lambda_h$:
\begin{align}
\sum_{i=1}^2 b_i(H_{i,h}(\mu_i)\cdot \lambda_h(\mu), H_{i,h}(\mu_i)\eta_h; \mu_i) = \sum_{i=1}^2 [ f_i(H_{i,h}(\mu_i)\eta_h; \mu_i) - b_i(R_{i,h}(\mu_i), H_{i,h}(\mu_i)\eta_h ;\mu_i) ] ~\forall\eta_h \in \Lambda_h.
\end{align}
Wegen Lemma 7.13 lässt sich die Existenz einer eindeutigen Lösung von (7.30) wie im parameter-unabhängigen, kontinuierlichem Fall nachweisen ( $\rightarrow$ 7.8 Lemma, Satz 7.9).

Bevor wir die RB-Methoden anwenden, führen wir noch parameter-unabhängige, diskrete, harmonische Fortsetzungen ein.
Dazu sei zunächst
\begin{align}
\{\xi_k\}_{k=1}^{\mathcal{N}_\Gamma} \text{ Basis von } \Lambda_h \text{ wobei } \mathcal{N}_\Gamma := \dim(\Lambda_h).
\end{align}
Ferner sei $\bar{\mu} = (\bar{\mu}_1,\bar{\mu}_2)\in P_1\times P_2$ Referenzparameter.
Dann definieren wir
\begin{align}
\chi_{i,k} := H_{i,h}(\bar{\mu}_i)\xi_k,~i=1,2,~ k=1\dt{,}\mathcal{N}_\Gamma.
\end{align}
Als nächstes seien $b_{i,k}^h(\mu_i)\in X_{i,h}^0$ Lösungen von
\begin{align}
b_i(b_{i,k}^h(\mu_i) + \chi_{i,k}, v_{i,h}; \mu_i) = 0 ~\forall v_{i,h}\in X_{i,h}^0,
\end{align}
so dass gilt: 
\[
H_{i,h}(\mu_i) \xi_k = \chi_{i,k} + b_{i,k}^h(\mu_i).
\]
Schließlich definieren wir globale Funktionen
\begin{align}
\Psi_k(\mu) := \left\{\begin{array}{lc} \psi_{1,k} + b_{1,k}^h(\mu_1) & \text{in } \Omega_1,\\ \psi_{2,k} + b_{2,k}^h(\mu_2) & \text{in } \Omega_2. \end{array}\right.
\end{align}
Wegen (7.30) können wir $u_h(\mu)$, definiert in Definition 7.11, dann wie folgt schreiben:
\begin{align}
u_h(\mu) = \sum_{i=1}^2 R_{i,h}^f(\mu_i) + \sum_{k=1}^{\mathcal{N}_\Gamma} U_k(\mu)\Psi_k(\mu),
\end{align}
wobei $U_k(\mu)\in \R$ Lösungen des folgenden linearen Gleichungssystems
\[
b\enbrace{ \sum_{k=1}^{\mathcal{N}_\Gamma} U_k(\mu)\Psi_k(\mu) , \Psi_l(\mu); \mu} = f(\Psi_l(\mu); \mu) - b\enbrace{\sum_{i=1}^2 R_{i,h}^f(\mu_i); \Psi_l(\mu); \mu},
\]
$l=1\dt{,} \mathcal{N}_\Gamma$, wobei wir $R_{i,h}^f(\mu_i)$ auf $\Omega_j$, $j\neq i$ durch Null fortsetzen.

\subsubsection{Static condensation reduced basis element method (SCRBE)}
Seien $X_{i,N}^f,X_{i,k}^N \subset X_{i,h}^0$ RB Räume.
Dann definieren wir RB Lösungen $R_{i,N}^f(\mu_i)\in X_{i,N}^f$ und $b_{i,k}^N(\mu_i)\in X_{i,k}^N$, als Lösungen von 
\begin{align}
b_i(R_{i,N}^f(\mu_i),v_{i,N};\mu_i) &= f_i(v_{i,N};\mu_i) ~\forall v_{i,N}\in X_{i,N}^f,\\
b_i(b_{i,k}^N(\mu_i) + \psi_{i,k}, v_{i,k};\mu_i) &= 0 ~\forall v_{i,k}\in X_{i,k}^N.
\end{align}

\ssect{7.14 Bemerkung}{}
Alternativ kann auch ein RB-Raum $X_{i,N}\subset X_{i,h}^0$ zur Lösung von (7.37) und (7.38) bestimmt werden.\\

Wieder definieren wir globale Funktionen
\begin{align}
\Psi_k^N(\mu) := \left\{\begin{array}{lc} \psi_{1,k} + b_{1,k}^N(\mu_1) & \text{in } \Omega_1,\\ \psi_{2,k} + b_{2,k}^N(\mu_2) & \text{in } \Omega_2, \end{array}\right.
\end{align}
und den SCRBE-Raum
\begin{align}
X^N(\mu) := \spann \enbrace{\Psi_k^N(\mu)~|~1\le k\le \mathcal{N}_\mu}.
\end{align}
Dann erhalten wir eine reduzierte Lösung $\tilde{u}^N(\mu)\in X^N(\mu)$, als Lösung von
\begin{align}
b(\tilde{u}^N(\mu),v;\mu) = f(v;\mu) - b\enbrace{\sum_{i=1}^2 R_{i,N}^f(\mu_i),v;\mu} ~\forall v\in X^N(\mu)
\end{align}
und definieren schließlich
\[
u^N(\mu) = \tilde{u}^N(\mu) + \sum_{i=1}^2 R_{i,N}^f(\mu_i).
\]

\subsubsection{Port reduced SCRBE (PR-SCRBE)}
Als nächstes möchten wir auch reduzierte Räume auf dem Interface oder Port $\Gamma$ betrachten.
Dazu führen wir einen \index{port reduzierten SCRBE} Raum ein
\begin{align}
X_M^N(\mu) := \penbrace{\Psi_k^N(\mu)~|~ 1\le k\le M} \subset X^N(\mu).
\end{align}
Wir betrachten dann das folgende reduzierte Problem:

Finde $\tilde{u}_M^N(\mu) \in X_M^N(\mu)$:
\begin{align}
b(\tilde{u}_M^N(\mu),v;\mu) = f(v;\mu) - b\enbrace{\sum_{i=1}^2 R_{i,N}^f(\mu_i),v;\mu} ~\forall v\in X_M^N(\mu)
\end{align}
und erhalten die reduzierte Lösung
\[
u_M^N(\mu) = \tilde{u}_M^N(\mu) + \sum_{i=1}^2 R_{i,N}^f(\mu_i).
\]

\ssect{7.15 Lemma}{(Wohlgestelltheit von Problem (7.43))}
Das reduzierte Problem (7.43) besitzt unter obigen Annahmen eine eindeutige Lösung $\tilde{u}_M^N(\mu)\in X_N^M(\mu)$ und es gilt die folgende Stabilitätsabschätzung:
\begin{align}
\norm{u_M^N(\mu)}_X \le \frac{1}{\alpha_h(\mu)} \enbrace{2 + \frac{\gamma_h(\mu)}{\alpha_h(\mu)}} \norm{f(.;\mu)}_{X'},
\end{align}
wobei $\alpha_h(\mu)$ und $\gamma_h(\mu)$ Koerzivitäts- und Stetigkeitskonstante zu $b$ bzgl. $X_h$ definiert in Definition 7.4.\\

\bet{Beweis:}\\
Da $b$ nach Annahme koerziv und stetig auf $X_h$, folgt die Wohlgestelltheit der RB-Probleme (7.37) und (7.38).
Aufgrund der Definition der $\Psi_k^N(\mu)$ gilt $X_M^N(\mu)\subset X_h$ und damit folgt die Existenz und Eindeutigkeit einer Lösung von (7.43) mit Lax-Milgram.

Es gilt zunächst $\norm{\sum_{i=1}^2 R_{i,N}^f(\mu_i)}_X \le \frac{1}{\alpha_h(\mu)} \norm{f(.;\mu)}_{X'}$, da $R_{i,N}^f(\mu_i) = 0$ auf $\Omega_j, i\neq j$.

(7.44) folgt dann durch teilen mit $\tilde{u}_M^N(\mu)$ in (7.43).
\hfill $\square$

\ssect{7.16 Bemerkung}{}
Optimale Räume im Sinne von Kolmogorov auf dem Interface $\Gamma$ werden in [Smetana, Patera 15] und ein zuverlässiger und effektiver A posteriori Fehlerschätzer in [Smetana 15] eingeführt.

\ssect{7.17 Bemerkung}{}
Andere Methoden verwenden zum Beispiel nicht-konforme Gebietszerlegungsmethoden [Maday, Ronquist 2002/04] oder Discontinuous Galerkin Methoden [Ohlberger, Schindler 15] zur Kopplung der lokalen reduzierten Lösungen.













\bibliographystyle{siam} 
\bibliography{kathrin}


\cleardoubleoddemptypage

\end{document} 